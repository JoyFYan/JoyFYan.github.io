<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[caffe-python-layer 的自定义]]></title>
      <url>%2F2017%2F02%2F17%2Fcaffe-python-layer%2F</url>
      <content type="text"><![CDATA[还是caffe的自定义层问题。相比于c，python的自定义层更为简单：代码少、外部文件少、方便执行。因此用这种方法实现有利于开发和实验。 准备首先还是要记得在编译的时候加上WITH_PYTHON_LAYER的选项，如果没有加可以先make clean删除编译后的文件，再重新编译。1WITH_PYTHON_LAYER=1 make &amp;&amp; make pycaffe 如果出现1layer_factory.hpp:77] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Python 这样的错误，说明这一步没有成功。 net中的添加在caffe的配置net里我们要输入有关层的信息，下面以EuclideanLossLayer层为例。简单来说我们可以直接使用，因为原本caffe里面已经包括了用c编写的代码，现在我们把它改编为python层。123456789101112131415layer &#123; type: 'Python' name: 'loss' top: 'loss' bottom: 'ipx' bottom: 'ipy' python_param &#123; # 模块名 -- 通常也是文件名 -- 需要放在 $PYTHONPATH 中 module: 'pyloss' # 层名 -- 模块里的类名 layer: 'EuclideanLossLayer' &#125; # set loss weight so Caffe knows this is a loss layer loss_weight: 1&#125; python的层文件需要在$PYTHONPATH目录下。在prototxt中，模块名是pyloss，这意味着你的包括EuclideanLossLayer类名的py文件名也应该是pyloss.py。 layer文件添加123456789101112131415161718192021222324252627282930313233# pyloss.pyimport caffeimport numpy as npclass EuclideanLossLayer(caffe.Layer): def setup(self, bottom, top): # check input pair if len(bottom) != 2: raise Exception("Need two inputs to compute distance.") def reshape(self, bottom, top): # check input dimensions match if bottom[0].count != bottom[1].count: raise Exception("Inputs must have the same dimension.") # difference is shape of inputs self.diff = np.zeros_like(bottom[0].data, dtype=np.float32) # loss output is scalar top[0].reshape(1) def forward(self, bottom, top): self.diff[...] = bottom[0].data - bottom[1].data top[0].data[...] = np.sum(self.diff**2) / bottom[0].num / 2. def backward(self, top, propagate_down, bottom): for i in range(2): if not propagate_down[i]: continue if i == 0: sign = 1 else: sign = -1 bottom[i].diff[...] = sign * self.diff / bottom[i].num 总结经测试文件应该没问题，可以读取运行。不过没有现成使用EuclideanLossLayer的网络，最后计算结果没有验证。 Caffe Python Layer]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GoogleNet :Going deeper with convolutions 论文阅读]]></title>
      <url>%2F2017%2F02%2F13%2Fgoingdeeper%2F</url>
      <content type="text"><![CDATA[这次读旁边拿了纸笔记录，感觉还是方便一些，之后再写篇博客总结一下加深印象。 问题引出Going deeper考虑的问题： 不在于训练数据、模型大小，希望得到新的模型结构； 可以用于移动计算，需要考虑功率、内存使用等问题。 NIN借鉴到的1*1卷积核： 降维（当然也可以升维），减少参数和计算； 增加深度、宽度，而没有明显性能损失。 目前提高深度神经网络性能的方法：加大size→缺点： 容易造成过拟合； 计算复杂。 →解决： 完全连接到稀疏连接（ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions） 如果数据集的概率分布可以由大而稀疏的深层网络表示，则最佳网络拓扑可通过分析最后层的激活的相关统计数据逐层构建。但对于非均匀稀疏数据计算效率低。提出：是否有中间步骤→一个利用额外稀疏性的架构？ Inception国外的命名还真是很有意思的，“Going deeper”就引用了我最开始的图的台词，Inception（《盗梦空间》）就是这台词的出处。好了，接下来就是这篇文章的核心：Inception结构。其实也很简单，利用多个大小卷积核进行提取特征并融合（个人理解）。具体看图更容易理解：之前我们卷积层很简单，是一个nn的滤波器与上一层输入滑动卷积即可。而这里突然又变成了四兄弟，滤波器分别为：11卷积，33卷积，55卷积，33池化，最后汇总输出。为了保证输出大小相同，只要将pad分别设置为0,1,2，stride设置为1即可。这里出来问题在于55计算量较大，特别是在深层情况下，所以说还是naive啊~还好有了NIN的借鉴，利用上文说到的1*1卷积核降维，可以解决这一问题。例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256。结构如下图所示： 总结优势： 每阶段增加了单元数，但也不会过度增加计算复杂； 不同卷积核大小，表示信息在不同尺度上处理，与直观相符合。 感想：之前也有想过不同尺度（也可以说是不同卷积核大小）下进行分别提取特征的，但后面不是很清楚如何能统一得到结果的，其实看完之后也很简单，通过补零即可。更重要的可能还在于1*1卷积层的巧妙使用，从参数量上大大减小了，提出该方法的NIN也需要看一下。 Going Deeper with ConvolutionsGoogLeNet系列解读]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[新年旧聚]]></title>
      <url>%2F2017%2F02%2F03%2Fnewyearparty%2F</url>
      <content type="text"><![CDATA[新年到来，一方面是迎新，另一方面是怀旧。回来这一段，有了两次和同学的聚会，一次是小学，一次是高中。高中的也不能说是很熟的同学，只是一起在外地上学，同校的而已。 首先说一下小学吧。初中之后，我就一直在外地上学，小学同学确实也联系的很少了。还有一点是当时小升初有的同学考上了初高中5年，这样的话还比我们大一级，也就是大学也应该毕业了。这样来看我们的“成分”也是比较复杂的。不过等到同学的提醒，我才发现如今已经离小学毕业过了10年了，真是一转眼。时间越久，也是越难相聚，这次在年前聚会可能也是有点仓促，来的人勉强凑成了一桌。 再次见到阔别的同学，觉得都是成熟了。当年邋遢而调皮捣蛋的同学，如今也可以一身风衣帅气而潇洒。最令我吃惊的是一名同学已经结婚领证了。其实仔细想想也没什么，年龄也到了嘛，以后这消息肯定多着呢。第二点感受是能继续读大家也都不会去轻易工作了，能出去也不会留下了。毕竟是三四线的小城市，况且环境污染也这么严重，以后再聚可能也不止这一个主会场了。 相比于小学，和高中同校不同班的同学相聚，按说应该会有一定的陌生感，但实际上聊的话题更多了，因为就经历来看的确比较像。考上一所还不错的大学，如今不是读研就是出国。特别是刚进门，座位上的四名同学都在讨论毕设的问题，一下有了一种亲切感。之后谈论的不是对大学生活的回顾（比如某校网费按流量计算1G几块钱想想看我所在的地方真是良心），就是对未来读研的期待（恐惧？他们读博的较多）。特别是后来，红酒过了几轮，大家纷纷开始讨论了专业学术问题：从解剖课程到疫苗，从人工智能到机器学习，从清洁能源到可控核聚变（佩服中科大学物理的同学）……感觉如果同行的家长不走我们还能继续聊下去。。吃完了又去玩了我不擅长的狼人，说不定真要多看看网上的节目来长些知识了。 新年易迎，旧友难聚，曾经的同学朋友各奔东西，却反而增加了今后相遇的机会。也希望以后有机会在外地和同学能又一次有共聚的机会。 如果说两次聚会的相同点，那就是——女生太少了(´；ω；`)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[scrapy中遇到的问题与解决]]></title>
      <url>%2F2017%2F01%2F23%2Fscrapy%2F</url>
      <content type="text"><![CDATA[Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。 因为好像这个用的比较多，所以看看用这个框架该怎么写爬虫。其实不难，但是中间出了很多神奇的小问题。 输出不正确、改代码结果不变？其实是因为反复使用命令1scrapy crawl spider -o 1.json 时候，增加的输出数据不会覆盖，而是继续往后面添加。 request不执行找了半天不知道为啥，其中一个比较靠谱的是1Request(url,meta=&#123;'item':item&#125;,callback=self.parse2, dont_filter=True) dont_filter=True让allowed_domains失效了。但是改过了还是不行。最终结果发现改的文件和运行的文件不一样……为什么会这样呢？我中间做了一部分实现了初始功能，就重命名了备份，然而执行命令行竟然一直在执行备份文件。。 输出为utf-8格式（保存中文）更改pipeline文件。123456789101112131415import jsonimport codecsclass WebPipeline(object): # def process_item(self, item, spider): # return item def __init__(self): # self.file = open('data.json', 'wb') self.file = codecs.open( 'scraped_data_utf8.json', 'w', encoding='utf-8') def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(line) return item def spider_closed(self, spider): self.file.close() 123ITEM_PIPELINES = &#123; 'web.pipelines.WebPipeline': 300,&#125; scrapy抓取到中文,保存到json文件为unicode,如何解决 如何在解析函数之间传递值？一种常见的情况：在parse中给item某些字段提取了值，但是另外一些值需要在parse_item中提取，这时候需要将parse中的item传到parse_item方法中处理，显然无法直接给parse_item设置而外参数。 Request对象接受一个meta参数，一个字典对象，同时Response对象有一个meta属性可以取到相应request传过来的meta。所以解决上述问题可以这样做：12345678def parse(self, response): # item = ItemClass() yield Request(url, meta=&#123;'item': item&#125;,callback=self.parse_item) def parse_item(self, response): item = response.meta['item'] item['field'] = value yield item Some Experiences Of Using Scrapy 使用ImagesPipeline下载1234567# setting.pyITEM_PIPELINES = ['demo.pipelines.MyImagesPipeline'] # ImagePipeline的自定义实现类IMAGES_STORE = 'D:\\dev\\python\\scrapy\\demo\\img' # 图片存储路径IMAGES_EXPIRES = 90 # 过期天数IMAGES_MIN_HEIGHT = 100 # 图片的最小高度IMAGES_MIN_WIDTH = 100 # 图片的最小宽度# 图片的尺寸小于IMAGES_MIN_WIDTH*IMAGES_MIN_HEIGHT的图片都会被过滤 ImagePipeline需要在自定义的ImagePipeline类中重载的方法：get_media_requests(item, info)和item_completed(results, items, info)。正如工作流程所示，Pipeline将从item中获取图片的URLs并下载它们，所以必须重载get_media_requests，并返回一个Request对象，这些请求对象将被Pipeline处理，当完成下载后，结果将发送到item_completed方法，这些结果为一个二元组的list，每个元祖的包含(success, image_info_or_failure)。 success: boolean值，true表示成功下载 image_info_or_error：如果success=true，image_info_or_error词典包含以下键值对。失败则包含一些出错信息。 url：原始URL path：本地存储路径 * checksum：校验码。1234567891011121314151617from scrapy.contrib.pipeline.images import ImagesPipelinefrom scrapy.exceptions import DropItemfrom scrapy.http import Requestclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem("Item contains no images") item['image_paths'] = image_paths return item scrapy 下载图片 ImagesPipeline扩展Media Pipeline]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python小爬虫-糗百]]></title>
      <url>%2F2017%2F01%2F20%2Fspider-QSBK%2F</url>
      <content type="text"><![CDATA[序在家没事本来想弄一下pyqt，做一些python下的界面，但是eric装了半天没成功……于是改做爬虫(:3[__]还好网上教程多，参考了一下，大致的框架都比较简单，难的在于针对不同的网页如何写正则表达式。不过这东西写多了应该就掌握方法了。从网上找了一段代码是爬糗百的，由于改版原来的表达式失效了，正好有了一个锻炼的机会。以下是代码： 代码123456789101112131415161718192021222324252627282930313233343536# -*- coding:utf-8 -*-import urllibimport urllib2import repage = 1url = 'http://www.qiushibaike.com/hot/page/' + str(page)user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)'# 需要加上headers才能访问headers=&#123;'User-Agent' : user_agent&#125;try: request = urllib2.Request(url,headers=headers) response = urllib2.urlopen(request)except urllib2.URLError, e: if hasattr(e,"code"): print e.code if hasattr(e,"reason"): print e.reasoncontent = response.read().decode('utf-8')# 正则表达式target = 'div.*?="author clearfix".*?title="(.*?)".*?div.*?="content".*?&lt;span&gt;(.*?)&lt;/span&gt;.*?&lt;/a&gt;.*?&lt;img src="(.*?)".*?&lt;/div&gt;'pattern = re.compile(target,re.S)items = re.findall(pattern,content)print "done"num=1;lenth=len(items);for item in items: # 判断是否存在图片 haveImg = re.search("pic.qiushibaike.com/system/pictures",item[2]) print str(num),'of',str(lenth) print item[0] # 用户名 print item[1] # 内容 if haveImg: print item[2] # 输出图片链接 num+=1; 效果原始网页：代码与结果： 总结这算是比较简单的抓取，不用登陆就可以了，之后再学学困难的。 Python爬虫实战一之爬取糗事百科段子]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[白色·恐怖·游戏 - 返校]]></title>
      <url>%2F2017%2F01%2F18%2Fdetention%2F</url>
      <content type="text"><![CDATA[恐怖类游戏向来不在我的关注范围之内，但近期的《返校》突然成为了大家关注的热点，steam上好评率达到98%，再加上这是台湾的一款国产游戏，让我产生了一定的好奇。不过我这种没有怎么玩过恐怖类游戏的人，可能很难成功通关，因此选择了看录像流程的方式了解了一下，的确没有让人失望。 离校虽然说名字是《返校》，但整个流程大部分都是为了逃离这个学校，一个怪异、恐怖、充满血与鬼的地方有什么可留恋的呢？可是我们有时的确无法控制自己，在这样氛围笼罩的大环境下，一个人的力量往往是微弱的。学校本应是一个追求真理，渴望自由的思想乌托邦，在当时也无法逃脱管制的命运，游戏中频频出现的眼睛似乎无时无刻都在监视着一切的发生，残肢断臂警告着他人触犯条律的下场。离校，只逃避了一时，更何况对于学生，也无处可去。 白色白，像白鹿项链一样纯洁；白，像白纸飞机一样自由。方苪欣是白色的，苍白的脸，洁白的心。无奈社会更白，容不下一丝色彩。如同在荒芜的雪原，孤独地前行，迷失了自我。 恐怖从游戏上来讲，并没有想象中的那么恐怖，或许是在游戏之外过于恐怖了。当魏仲廷交给方苪欣一份禁书书单后，书单的样子变成了一只手枪，我才感到一丝寒意。也正是这只手枪，给老师判了死刑，让同学被捕入狱。之前对台湾的历史了解的不是很多，搜索之后才发现那时社会的恐怖所在。政党的恐惧导致了社会的恐怖，这似乎也是向来的规律，何时何地都逃不过。还有一句话是“知识越多越反动”，有时想想也确实很有道理。 返校十多年过去了，一切似乎又重归平静，魏仲廷出狱，终于能够再一次返校，却无法继续完成曾经的学业，遇到以前的老师同学。在那里的，只有同样等待着的方苪欣，等待着救赎…… 评故事不长，流程大概三个小时可以完成，除去剧情，在追寻碎片化的记忆过程中，也有很多巧妙的设计，特别是通过录音机调频来进行时间的穿越。另外就是游戏风格，的确很有中国特色，从界面人物的剪纸风，到民间传统鬼怪的融入，让玩家都很亲切。游戏体验中的恐怖感适中，总的来说是一款出色的游戏。 赤烛 返校Steam 返校]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Deepdream 实现]]></title>
      <url>%2F2017%2F01%2F11%2FDeepdream%2F</url>
      <content type="text"><![CDATA[Deepdream是一年半前谷歌搞的一个深度学习“艺术品”，最近在cs231n课上看到了，感觉还是很interesting。 环境准备 deepdream还是基于python和caffe深度网络的，因此大概需要以下环境： Standard Python scientific stack: NumPy, SciPy, PIL, IPython. Those libraries can also be installed as a part of one of the scientific packages for Python, such as Anaconda or Canopy. Caffe deep learning framework (installation instructions). Google protobuf library that is used for Caffe model manipulation. 代码 导入库相关的环境配置好了之后，可以先试试库能不能被导入进来：12345678910111213141516171819# imports and basic notebook setupfrom cStringIO import StringIOimport numpy as npimport scipy.ndimage as ndimport PIL.Imagefrom IPython.display import clear_output, Image, displayfrom google.protobuf import text_formatimport caffecaffe.set_mode_gpu();caffe.set_device(2);# 默认GPU 为0# 如果GPU 支持 CUDA 并且 Caffe 编译时添加对 CUDA 支持,可以使用caffe.set_mode_gpu()和caffe.set_device(0);def showarray(a, fmt='jpeg'): a = np.uint8(np.clip(a, 0, 255)) f = StringIO() PIL.Image.fromarray(a).save(f, fmt) display(Image(data=f.getvalue())) 加载模型网络采用了GoogLeNet模型，需要提前下好。1234567891011121314151617181920212223242526model_path = '../caffe/models/bvlc_googlenet/' # 替换为自己模型的目录net_fn = model_path + 'deploy.prototxt'param_fn = model_path + 'bvlc_googlenet.caffemodel'# Patching model to be able to compute gradients.# Note that you can also manually add "force_backward: true" line to "deploy.prototxt".# 以下部分是更改了deploy的参数，增加了"force_backward: true"，然后保存成一个临时文件用于网络。当然也可以自己手动改。model = caffe.io.caffe_pb2.NetParameter()text_format.Merge(open(net_fn).read(), model)model.force_backward = Trueopen('tmp.prototxt', 'w').write(str(model))net = caffe.Classifier('tmp.prototxt', param_fn, mean = np.float32([104.0, 116.0, 122.0]), # ImageNet mean, training set dependent # 均值 channel_swap = (2,1,0)) # the reference model has channels in BGR order instead of RGB # 改RGB通道# a couple of utility functions for converting to and from Caffe's input image layout# 为了caffe 数据处理的功能函数def preprocess(net, img): return np.float32(np.rollaxis(img, 2)[::-1]) - net.transformer.mean['data']def deprocess(net, img): return np.dstack((img + net.transformer.mean['data'])[::-1]) 做梦 Making the “dream” images is very simple. Essentially it is just a gradient ascent process that tries to maximize the L2 norm of activations of a particular DNN layer. Here are a few simple tricks that we found useful for getting good images: offset image by a random jitter normalize the magnitude of gradient ascent steps apply ascent across multiple scales (octaves) 做梦其实很简单，本质上，它只是一个梯度上升过程，试图最大化特定DNN层激活的L2范数。 这里有一些简单的技巧，我们发现有用的获得良好的图像： 由随机抖动偏移图像 规则化梯度上升步长的幅度 在多个尺度上应用上升 首先我们实现一个基本的梯度上升阶跃函数，应用前两个技巧：1234567891011121314151617181920212223242526272829303132# 将输入的数据(data)复制给梯度(diff)def objective_L2(dst): dst.diff[:] = dst.data # 核心函数def make_step(net, step_size=1.5, end='inception_4c/output', jitter=32, clip=True, objective=objective_L2): '''Basic gradient ascent step.''' src = net.blobs['data'] # input image is stored in Net's 'data' blob # 输入图像 dst = net.blobs[end] # 目标层，默认为'inception_4c/output' ox, oy = np.random.randint(-jitter, jitter+1, 2) # 生产抖动 src.data[0] = np.roll(np.roll(src.data[0], ox, -1), oy, -2) # apply jitter shift # 应用抖动 net.forward(end=end) # 向前传播到指定层 objective(dst) # specify the optimization objective # 指定优化目标（默认为objective_L2优化） net.backward(start=end) # 反向传播到优化层 g = src.diff[0] # 输入图像梯度 # apply normalized ascent step to the input image # 对输入图像应用归一化上升步长 src.data[:] += step_size/np.abs(g).mean() * g src.data[0] = np.roll(np.roll(src.data[0], -ox, -1), -oy, -2) # unshift image # 还原抖动 if clip: bias = net.transformer.mean['data'] src.data[:] = np.clip(src.data, -bias, 255-bias) Next we implement an ascent through different scales. We call these scales “octaves”. 接下来，我们通过不同的尺度实现上升。 我们称这些尺度为“octaves”。iter_n是迭代次数，octave_n是尺度缩放次数，octave_scale是尺度缩放比例。1234567891011121314151617181920212223242526272829303132333435363738# 默认参数：迭代10次，缩放4次，缩放比1.4# 即原始图像迭代10次，长宽缩小1.4倍后再次迭代，一共进行4轮（包括原始尺寸）def deepdream(net, base_img, iter_n=10, octave_n=4, octave_scale=1.4, end='inception_4c/output', clip=True, **step_params): # prepare base images for all octaves # 准备数据，生成octave_n个数据 octaves = [preprocess(net, base_img)] for i in xrange(octave_n-1): octaves.append(nd.zoom(octaves[-1], (1, 1.0/octave_scale,1.0/octave_scale), order=1)) src = net.blobs['data'] # np.zeros_like(a): 依据给定数组(a)的形状和类型返回一个新的元素全部为1的数组。 detail = np.zeros_like(octaves[-1]) # allocate image for network-produced details for octave, octave_base in enumerate(octaves[::-1]): h, w = octave_base.shape[-2:] if octave &gt; 0: # upscale details from the previous octave h1, w1 = detail.shape[-2:] detail = nd.zoom(detail, (1, 1.0*h/h1,1.0*w/w1), order=1) src.reshape(1,3,h,w) # resize the network's input image size src.data[0] = octave_base+detail for i in xrange(iter_n): make_step(net, end=end, clip=clip, **step_params) # visualization vis = deprocess(net, src.data[0]) if not clip: # adjust image contrast if clipping is disabled vis = vis*(255.0/np.percentile(vis, 99.98)) showarray(vis) print octave, i, end, vis.shape clear_output(wait=True) # extract details produced on the current octave detail = src.data[0]-octave_base # returning the resulting image return deprocess(net, src.data[0]) 开始做梦123# 打开并显示图片img = np.float32(PIL.Image.open('sky1024px.jpg'))showarray(img) 1_=deepdream(net, img)# 运行 一共会得到4（尺度）*10（迭代）=40张图片。更改结束层会改变结果，如：1_=deepdream(net, img, end='inception_3b/5x5_reduce')# 更改结束层 具体的层可以参考配置文件，googlenet还是比较复杂的。而且结束的越晚，就更能从图像识别出现实物体。如：_=deepdream(net, img, end=&#39;inception_4e/output&#39;)左边的云已经可以看出是一只狗的脸了。 前方高能！上面一步就得到了奇怪的结果，如果把这样的输出再作为输入放到网络里呢？结果比较精神污染： 这是经过大概5轮反复输入得到的结果。官方迭代了100次，丧心病狂啊= = 控制做梦如果调整了我们的优化目标，就可以控制我们想要的结果。比如我们想把原始图像往另一张图像上靠近，可以定义一个指向性的优化目标：12345678def objective_guide(dst): x = dst.data[0].copy() y = guide_features ch = x.shape[0] x = x.reshape(ch,-1) y = y.reshape(ch,-1) A = x.T.dot(y) # compute the matrix of dot-products with guide features dst.diff[0].reshape(ch,-1)[:] = y[:,A.argmax(1)] # select ones that match best guide_features是需要预先提取的目标特征：1234567end = 'inception_3b/output'h, w = guide.shape[:2]src, dst = net.blobs['data'], net.blobs[end]src.reshape(1,3,h,w)src.data[0] = preprocess(net, guide)net.forward(end=end)guide_features = dst.data[0].copy() 好了，运行！1_=deepdream(net, img, end=end, objective=objective_guide) 有了花的感觉啊~ 总结正着传播过来是分类，反着过去是生成，用深度学习产生艺术作品似乎是一个很有意思的方向。而目前也有较为成熟甚至是商业化的项目了，比如deepart。如果你能够承受一定的精神污染，建议挑战一下Nightmare，这是YOLO大神的另一个作品。 deepdream-github]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe-python interface 学习-网络训练、部署、测试]]></title>
      <url>%2F2017%2F01%2F08%2Fpycaffe-interface2%2F</url>
      <content type="text"><![CDATA[继续python接口的学习。剩下还有solver、deploy文件的生成和模型的测试。 网络训练solver文件生成其实我觉得用python生成solver并不如直接写个配置文件，它不像net配置一样有很多重复的东西。对于一下的solver配置文件：12345678910111213141516base_lr: 0.001display: 782gamma: 0.1lr_policy: “step”max_iter: 78200 #训练样本迭代次数=max_iter/782(训练完一次全部样本的迭代数)momentum: 0.9snapshot: 7820snapshot_prefix: "snapshot"solver_mode: GPUsolver_type: SGDstepsize: 26067 test_interval: 782 #test_interval=训练样本数(50000)/batch_size(train:64)test_iter: 313 #test_iter=测试样本数(10000)/batch_size(test:32)test_net: "/home/xxx/data/val.prototxt"train_net: "/home/xxx/data/proto/train.prototxt"weight_decay: 0.0005 可以用以下方式实现生成：1234567891011121314151617181920212223242526from caffe.proto import caffe_pb2s = caffe_pb2.SolverParameter()path='/home/xxx/data/'solver_file=path+'solver1.prototxt's.train_net = path+'train.prototxt's.test_net.append(path+'val.prototxt')s.test_interval = 782 s.test_iter.append(313) #这里用的是append，码风不太一样s.max_iter = 78200 s.base_lr = 0.001 s.momentum = 0.9s.weight_decay = 5e-4s.lr_policy = 'step's.stepsize=26067s.gamma = 0.1s.display = 782s.snapshot = 7820s.snapshot_prefix = 'shapshot's.type = “SGD”s.solver_mode = caffe_pb2.SolverParameter.GPUwith open(solver_file, 'w') as f: f.write(str(s)) 并没有简单多少。需要注意的是有些参数需要计算得到： test_interval：假设我们有50000个训练样本，batch_size为64，即每批次处理64个样本，那么需要迭代50000/64=782次才处理完一次全部的样本。我们把处理完一次所有的样本，称之为一代，即epoch。所以，这里的test_interval设置为782，即处理完一次所有的训练数据后，才去进行测试。如果我们想训练100代，则需要设置max_iter为78200. test_iter：同理，如果有10000个测试样本，batch_size设为32，那么需要迭代10000/32=313次才完整地测试完一次，所以设置test_iter为313. lr_rate：学习率变化规律我们设置为随着迭代次数的增加，慢慢变低。总共迭代78200次，我们将变化lr_rate三次，所以stepsize设置为78200/3=26067，即每迭代26067次，我们就降低一次学习率。 模型训练完整按照定义的网络和solver去训练，就像命令行一样：12solver = caffe.SGDSolver('/home/xxx/solver.prototxt')solver.solve() 不过也可以分得更细一些，比如先加载模型：1solver = caffe.get_solver('models/bvlc_reference_caffenet/solver.prototxt') 这里用的是.get_solver，默认按照SGD方法求解。向前传播一次网络，即从输入层到loss层，计算net.blobs[k].data。1solver.net.forward() # train net 反向传播一次网络，即从loss层到输入层，计算net.blobs[k].diff and net.params[k][j].diff。1solver.net.backward() 如果需要一次完整的计算，正向、反向、更新权重（net.params[k][j].data），可以使用1solver.step(1) 改变数字进行多次计算。 网络部署部署即生成一个deploy文件，用于下面的模型测试。这里既可以用python，也可以直接修改net文件。123456789101112131415161718192021222324252627from caffe import layers as L,params as P,to_protoroot='/home/xxx/'deploy=root+'mnist/deploy.prototxt' #文件保存路径def create_deploy(): #少了第一层，data层 conv1=L.Convolution(bottom='data', kernel_size=5, stride=1,num_output=20, pad=0,weight_filler=dict(type='xavier')) pool1=L.Pooling(conv1, pool=P.Pooling.MAX, kernel_size=2, stride=2) conv2=L.Convolution(pool1, kernel_size=5, stride=1,num_output=50, pad=0,weight_filler=dict(type='xavier')) pool2=L.Pooling(conv2, pool=P.Pooling.MAX, kernel_size=2, stride=2) fc3=L.InnerProduct(pool2, num_output=500,weight_filler=dict(type='xavier')) relu3=L.ReLU(fc3, in_place=True) fc4 = L.InnerProduct(relu3, num_output=10,weight_filler=dict(type='xavier')) #最后没有accuracy层，但有一个Softmax层 prob=L.Softmax(fc4) return to_proto(prob)def write_deploy(): with open(deploy, 'w') as f: f.write('name:"Lenet"\n') f.write('input:"data"\n') f.write('input_dim:1\n') f.write('input_dim:3\n') f.write('input_dim:28\n') f.write('input_dim:28\n') f.write(str(create_deploy()))if __name__ == '__main__': write_deploy() 如果自己修改net，需要修改数据输入：123456layer &#123; name: "data" type: "Input" top: "data" input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 100 dim: 100 &#125; &#125;&#125; 并且增加一个softmax，对于原来的softmaxwithloss直接换掉就行。 网络测试训练好之后得到模型，实际使用是需要用模型进行预测。这时需要用到deploy文件和caffemodel。123456789101112131415161718192021222324252627282930#coding=utf-8import caffeimport numpy as nproot='/home/xxx/' #根目录deploy=root + 'mnist/deploy.prototxt' #deploy文件caffe_model=root + 'mnist/lenet_iter_9380.caffemodel' #训练好的 caffemodelimg=root+'mnist/test/5/00008.png' #随机找的一张待测图片labels_filename = root + 'mnist/test/labels.txt' #类别名称文件，将数字标签转换回类别名称net = caffe.Net(deploy,caffe_model,caffe.TEST) #加载model和network#图片预处理设置transformer = caffe.io.Transformer(&#123;'data': net.blobs['data'].data.shape&#125;) #设定图片的shape格式(1,3,28,28)transformer.set_transpose('data', (2,0,1)) #改变维度的顺序，由原始图片(28,28,3)变为(3,28,28)#transformer.set_mean('data', np.load(mean_file).mean(1).mean(1)) #减去均值，前面训练模型时没有减均值，这儿就不用transformer.set_raw_scale('data', 255) # 缩放到【0，255】之间transformer.set_channel_swap('data', (2,1,0)) #交换通道，将图片由RGB变为BGRim=caffe.io.load_image(img) #加载图片net.blobs['data'].data[...] = transformer.preprocess('data',im) #执行上面设置的图片预处理操作，并将图片载入到blob中#执行测试out = net.forward()labels = np.loadtxt(labels_filename, str, delimiter='\t') #读取类别名称文件prob= net.blobs['Softmax1'].data[0].flatten() #取出最后一层（Softmax）属于某个类别的概率值，并打印print proborder=prob.argsort()[-1] #将概率值排序，取出最大值所在的序号 print 'the class is:',labels[order] #将该序号转换成对应的类别名称，并打印 总结利用python接口，对网络的具体参数能够有更全面的认识和理解。不过也有几点需要注意： 数据格式的转换caffe的数据blob shape是NCHW，通道数在前。而python图像处理时shape是HW*C，通道数在后。因此需要转换一下。 图片显示与保存由于没有图形界面，很方便的jupyter notebook不能使用，只好保存图片查看。 caffe的python接口学习（2）：生成solver文件caffe的python接口学习（5）：生成deploy文件caffe的python接口学习（6）：用训练好的模型（caffemodel）来分类新的图片Deep learning tutorial on Caffe technology : basic commands, Python and C++ code.Multilabel classification on PASCAL using python data-layers]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe-python interface 学习-网络定义详解]]></title>
      <url>%2F2017%2F01%2F06%2Fpycaffe-interface1%2F</url>
      <content type="text"><![CDATA[之前用的都是caffe的命令行接口，单独训练还行，不过看里面层的参数、数据还是很麻烦的。特别是这周实验遇到了比较大的问题，命令行无能为力，还是要好好看看python接口。 python 接口编译这个一般在编译caffe时都会顺带完成，如果遇到ImportError: No module named caffe，可能是没有编译或者没有添加到路径。编译可以在根目录下make pycaffe，目录是/caffe/python。将caffe/python的路径添加到用户环境变量~/.bashrc中：1export PYTHONPATH=/home/xxx/caffe/python 然后输入sudo ldconfig确认。当然我的服务器没有管理员权限，这时可以每次手动添加目录，见下文。编译时要在Makefile.config中修改有关路径，除此之外，1WITH_PYTHON_LAYER := 1 也是需要注意的一点，这在f-rcnn中也提到过。 python 接口调用123456import syssys.path.append('/home/xxx/caffe/python')#手动添加路径import caffeimport numpy as npfrom skimage import ioimport matplotlib.pyplot as plt 以上可以直接复制好，每次都加上。 运行模式小设置123caffe.set_mode_cpu()#设置为cpu模式caffe.set_device(0)#gpu号caffe.set_mode_gpu()#gpu模式 定义网络下面是一个例子：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-"""Spyder Editor"""from caffe import layers as L,params as P,to_protopath='/home/xxx/data/' #保存数据和配置文件的路径train_lmdb=path+'train_db' #训练数据LMDB文件的位置val_lmdb=path+'val_db' #验证数据LMDB文件的位置mean_file=path+'mean.binaryproto' #均值文件的位置train_proto=path+'train.prototxt' #生成的训练配置文件保存的位置val_proto=path+'val.prototxt' #生成的验证配置文件保存的位置#编写一个函数，用于生成网络def create_net(lmdb,batch_size,include_acc=False): #创建第一层：数据层。向上传递两类数据：图片数据和对应的标签 data, label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2, transform_param=dict(crop_size=40,mean_file=mean_file,mirror=True)) #创建第二屋：卷积层 conv1=L.Convolution(data, kernel_size=5, stride=1,num_output=16, pad=2,weight_filler=dict(type='xavier')) #创建激活函数层 relu1=L.ReLU(conv1, in_place=True) #创建池化层 pool1=L.Pooling(relu1, pool=P.Pooling.MAX, kernel_size=3, stride=2) conv2=L.Convolution(pool1, kernel_size=3, stride=1,num_output=32, pad=1,weight_filler=dict(type='xavier')) relu2=L.ReLU(conv2, in_place=True) pool2=L.Pooling(relu2, pool=P.Pooling.MAX, kernel_size=3, stride=2) #创建一个全连接层 fc3=L.InnerProduct(pool2, num_output=1024,weight_filler=dict(type='xavier')) relu3=L.ReLU(fc3, in_place=True) #创建一个dropout层 drop3 = L.Dropout(relu3, in_place=True) fc4 = L.InnerProduct(drop3, num_output=10,weight_filler=dict(type='xavier')) #创建一个softmax层 loss = L.SoftmaxWithLoss(fc4, label) if include_acc: #在训练阶段，不需要accuracy层，但是在验证阶段，是需要的 acc = L.Accuracy(fc4, label) return to_proto(loss, acc) else: return to_proto(loss) def write_net(): #将以上的设置写入到prototxt文件 with open(train_proto, 'w') as f: f.write(str(create_net(train_lmdb,batch_size=64))) #写入配置文件 with open(val_proto, 'w') as f: f.write(str(create_net(val_lmdb,batch_size=32, include_acc=True))) if __name__ == '__main__': write_net() 上面的代码，我们一开始就import了两个包，一个是layers，另一个是params。layers里面包含了Caffe所以内置的层（比如卷积，ReLU等），而params则包含了各种枚举值。网上很少找到函数详解，自己凭着理解总结一下吧： 数据层12345678910data,label=L.Data( source=lmdb, #数据源，训练数据LMDB文件的位置 backend=P.Data.LMDB, #数据类型，本文是lmdb batch_size=batch_size, #batch大小 ntop=2, #输出数量，本文是data和label，所以是2 transform_param=dict(crop_size=40, #crop大小 mean_file=mean_file, #均值文件 mirror=True #镜像操作 ) ) 卷积层12345678conv1=L.Convolution( data, #数据流入（即从数据层得到的data） kernel_size=5, #卷积核大小 stride=1, #步长 num_output=16, #输出 pad=2, #填零 weight_filler=dict(type='xavier') #权重初始化方式'xavier' ) 激活层、dropout层12345678relu1=L.ReLU( conv1, #数据流入（即从卷积层得到的conv1） in_place=True #in_place ，就地运算，节省存储开销 )drop3=L.Dropout( relu3, #数据流入（即从激活层得到的relu3） in_place=True #in_place ，就地运算，节省存储开销 ) 池化层123456pool1=L.Pooling( relu1, #数据流入（即从激活层得到的relu1） pool=P.Pooling.MAX, #池化方式：最大池化 kernel_size=3, #池化核大小 stride=2 #步长 ) 全连接层12345fc3=L.InnerProduct( pool2, #数据流入（即从池化层得到的pool2） num_output=1024, #全连接输出数目 weight_filler=dict(type='xavier') #权重初始化方式'xavier' ) SoftmaxWithLoss层1234loss = L.SoftmaxWithLoss( fc4, #数据流入（即从全连接层得到的fc4） label #数据流入（即从数据层得到的label） ) Accuracy层12345678if include_acc: #在训练阶段，不需要accuracy层，但是在验证阶段，是需要的 acc = L.Accuracy( fc4, label ) return to_proto(loss, acc)else: return to_proto(loss) 总结上面那种是一层一层往上累加的，最后返回了最后一层。当然如果直接建一个caffe.NetSpec()，会有一个整体的把握：123456def mynet(lmdb, batch_size): n = caffe.NetSpec() ################### n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb, transform_param=dict(scale=1./255), ntop=2) ################### return n.to_proto() 方法的参数中的lmdb是指Caffe支持的数据库的一种，叫lmdb，我们传入数据库的路径即可。而n=caffe.NetSpec()是获取Caffe的一个Net，我们只需不断的填充这个n，最后面把n输出到文件。在填充的时候要记得加上n.。各层的具体参数可以参考caffe.proto。总的来说，这种方式生成prototxt要简单一些，代码量比直接写要小，而且层的输入输出清晰：输入是第一个参数，输出是返回值。不过也要注意不同参数的数据格式，如dict(type=&#39;xavier&#39;)。 caffe的python接口学习（1）：生成配置文件Deep learning tutorial on Caffe technology : basic commands, Python and C++ code.Caffe学习4-利用caffe.proto自定义自己的网络]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[2016-2017 新年，新开始]]></title>
      <url>%2F2017%2F01%2F01%2F2017-newyear%2F</url>
      <content type="text"><![CDATA[今天是2016的最后一天，并没有过的轰轰烈烈，似乎也有些对不起这样重要的一年。 一生中重要的事情往往不多，大部分都是处在一种“量变”的状态，“质变”不是说来就来的，而是需要有这样的一个机会。机会如何把握呢？有句话是“机会总是给有准备的人”，似乎就是在说“量变”的重要性。但是我可能还是理解错了。 首先，重要的事情之所以重要，不仅仅体现在这一件事情上，而是会接连影响到之后的事情，决定着事物发展方向。在学业的这段时间里，中考决定了高中生活，高考决定了大学生活，找工作决定了工作生涯……几年的生活工作往往就决定于一两天的选择。 第二，“量变”到底决定的是什么？每天好好学习，学习成绩就会有所提升，反映到升学上，就是选择的更多，即有更多更好的资源和前景，未来的生活状况的平均水平会有所提高。再好的学校也会有成绩、能力不足的人，再差的学校也会有大神级别的人物，但不同学校的分布水平不同。所以能去好一点的学校肯定不回去差的。 第三， 机会是什么？机会等于有更多的选择空间吗？假如我足够优秀，全世界的学校专业我都可以挑选，我就是成功的吗？或许只说对了一半。这只代表我有能力上学，却没有回答为什么学的问题。这个最重要的问题常常被忽视掉。因为这个问题的答案只有每个人自己知道，别人是不会关心的，或者说别人只是站在别人的角度去考虑。当别人一次次展示自己的成功时，往往会淡忘掉自己的目标和理想。每个人还是不一样的。 实际上有很多东西没有考虑清楚就上路了，却发现已经没法调头。2016就是这样一个重要的一年，重要的不在于成功地拥有了很多选择，而在于应该去如何选择。 二十多年来，经历了无数事情，唯独选择做得少，这也导致我在这方面的能力不足，特别是选择之后才明白一些事情，此时就晚了。因此信息是很重要的，特别是在信息不对称时，很容易处于被动。而当自己不知所措时，不能听从他人，而要问问自己，问一问过去的自己，究竟所要的是什么。 还有一点就是争取，办不成的事要再试试，不要总把自己的需求和愿望当作是在麻烦别人。同样的，别人拜托自己所不想做的事情，也不要勉强去做，要学会拒绝。 走得越远，越发现自己曾经的思维、方法是跟不上大环境的，还好离真正的社会也有一段距离。希望能慢慢提升自己，成为一个成熟的人。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[12月观影]]></title>
      <url>%2F2016%2F12%2F28%2F12%E6%9C%88%E8%A7%82%E5%BD%B1%2F</url>
      <content type="text"><![CDATA[在12月看了三场电影，一次在成都，两次在北京，算是频率比较高的了。分别是：《你的名字。》《三少爷的剑》《长城》。其实我很少看国产电影，第二部是师兄请客，第三部是圣诞节准备看电影但是实在没有什么评价很好的电影了才看的。印象中上次的国产电影是去年的跨年，看的《老炮儿》。可惜这两次观影又让我对国产失望了一分。 先说说三少爷吧。不吐槽人物装扮、演技的话，从剧情上，燕十三可能是最“正常”的一个人了。全剧围绕一个大备胎，备胎还被妹子误杀，备胎又把妹子杀了，男主一脸懵逼只好把仰慕自己基友杀了，最后和青楼女子生活在了一起（误……燕十三一心想打败三少爷，却在即将离世的前几周教伪装成平民（退隐江湖）的三少爷剑法，这好像是最大的幽默了。除了教剑法，还教做人处事，让三少爷明白了“哦原来不杀人也能做成事啊”。三少爷最无语的地方（也是同学原著党跑出电影院的地方），就是直到后来杀了苗子全村的慕容秋荻问谢晓峰“你还愿意再娶我一次吗？”谢晓峰说“嗯。”男主真是让人喜欢不起来啊~ 《长城》比三少爷还好一点，毕竟是做了尝试。 《长城》是由中国电影股份有限公司、乐视影业、传奇影业、环球影业联合出品，由中国导演张艺谋执导，马特·达蒙、景甜、佩德罗·帕斯卡、威廉·达福、刘德华、张涵予等联合主演的奇幻动作片。 这阵容真是中西融合，得知主题是抵抗饕餮，联想到了《独立日》和《星河战队》，这就是把传统的在太空打外星人改到了在古代打怪兽嘛。是把西方个人英雄主义和爱国主义思想（？）的一次结合。其实这两者在这类电影里面也是相通的，最后的任务往往只需要几个主角，他们成功了国家、世界才有救。具体这部电影，的确缺乏了亮点，也不知道是否是因为面对国际市场，做出来有点四不像，我也不想过多评价，3分/5分也差不多了。 神奇的是，这两天关于电影有了这样的新闻： 《摆渡人》《长城》等电影票房不如人意，豆瓣、猫眼要不要背锅？“张艺谋已死。”看完《长城》后，“亵渎电影”12月15日发了这么一条微博，他当时没有预料的是这条评论引发的后续效应将远远超过其控制范围。当天乐视影业 CEO 张昭就毫不客气地转发对骂，并以乐视影业官方名义发出了警告函，称将采取法律手段。今天人民日报客户端发文，以此为例批判“蓄意恶评伤害电影产业”。 相比之下是今年中国电影市场惨淡。如果只是这样，失去的可能不只是票房了，还有信心。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[cs231n笔记1]]></title>
      <url>%2F2016%2F12%2F21%2Fcs231n%E7%AC%94%E8%AE%B01%2F</url>
      <content type="text"><![CDATA[斯坦福深度学习与机器视觉课程cs231n，感觉挺不错的，顺便记下来一些零碎的点，不过具体内容还是要参考笔记、视频。不过网易云课堂的视频还是有些问题的。 研究历史 一开始是对猫的视觉进行研究，发现有如下神奇的特点： 对于整个图像，猫的视觉基础神经元没有被激活。 在切换图像时，神经元被激活。 因此研究人员认为神经元对简单形状、边缘有反应。 David Marr提出视觉是分层的。 1234567st=&gt;start: 输入图像e=&gt;end: 结束op1=&gt;operation: 边缘信息op2=&gt;operation: 2.5D图像op3=&gt;operation: 3D图像st-&gt;op1-&gt;op2-&gt;op3 90年代感知分组，将图像分为不同区域。1999，SIFT特征，图像识别、匹配。2001，Face Detection，没有进行3D建模。 数据集： PASCAL 20类 IMAGENET 22K类，14M图 2011年及之前图像分类识别基本靠SIFT+SVM。其竞赛2012年出现CNN，AlexNet，7层layer夺得冠军。之后2014：GoogleNet，VGG；2015：MSRA深度残差，均基于CNN。 kNN分类器两图像距离的定义。 L1距离：像素间绝对值再求和 L2距离：像素间差的平方求和再开方 超参数：无法在训练中优化，如距离定义、k值。采用单一验证、交叉验证方法。 kNN流程： 计算已知类别数据中图像（train set）与需要预测点图像（test set）距离； 按照距离依次排序； 选取与需要预测图像距离最小的前k个； 确定前k个中每种类别出现的次数； 将出现次数最多的类别作为预测类别。 损失 SVM损失公式：$j\neq y_i$是指对于第$i$类，只对其他类进行求和运算；sj是第j类得分结果；s{y_i}是本类的得分结果；$\Delta$是常数，一般取1。这是某一类的损失，总损失可以求平均。 image cat score car score frog score losses CAT 3.2 5.1 -1.7 2.9 CAR 1.3 4.9 2.0 0 FROG 2.2 2.5 -3.1 10.9 正则化 正则化项避免过拟合，考虑更多输入。 L1正则：w权重绝对值之和 L2正则：w权重平方和 Softmax损失公式：计算过程： item cat car frog score 3.2 5.1 -1.7 exp 24.5 164.0 0.18 normalize 0.13 0.87 0 $L_{cat}=-log(0.13)=0.89$当初始时（w很小），L=-log(1/N)=log(N)。 CS231n Convolutional Neural Networks for Visual RecognitionCS231n官方笔记授权翻译总集篇发布斯坦福CS231n—深度学习与计算机视觉]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文阅读：Hyper-class Augmented and Regularized Deep Learning for Fine-grained Image Classification]]></title>
      <url>%2F2016%2F12%2F20%2FHyper-class-Augmented%2F</url>
      <content type="text"><![CDATA[介绍 难点： 精细粒度的标记数据的获取要昂贵得多（通常需要专业领域）; 存在大的类内（intra-class）和小类间（inter-class）方差。 目前训练策略：在已有模型上预训练CNN并在小规模数据集上fine-tune。 本文工作： 使用hyper-class来增强数据，从网络上搜索hyper-class-labeled的数据，形成多类学习任务。 公式化精细识别模型和hyper-class识别模型，通过挖掘二者的关系提升识别率。 增强数据： super-class，包括一系列的精细类别。 factor-type hyper-classes 因子类型。提出超类增强和正则化深度学习（hyper-class augmented and regularized deep learning）。学习框架与神经网络与多任务学习密切相关。这个想法是通过允许他们共享神经网络的相同特征层来联合训练多个相关任务。 超类增强和正则化深度学习 解决第一个难点，用由一些超类标记的大量辅助图像来增强细粒度数据。第二个难点，提出新的CNN模型。 超类数据增强 super-class，包括一系列的精细类别。 factor-type hyper-classes 因子类型。 对于给定的细粒度（fine-grained class）类，图像可能有不同的视角视图，即因子类型超类（factor-type hyper-classes）。这和super-type hyper-class 与 fine-grained classes完全不同。一个fine-grained class只能归属于一个单一的super-type class。比如fine-grained class “吉娃娃”属于super-type class“dog”，并不属于“cat”。然而对于汽车数据，其 fine-grained class可以有不同的视角，因此不必归为同一个单独的hyper-class。从生成的角度来看，可以通过首先生成其视图（hyper-class）然后生成给定的视图来生成汽车图像的fine-grained class。这也是我们下一小节描述的模型的概率基础。由于这种类型的超类可以被认为是图像的隐藏因素，这种类型称为factor-type hyper-class。super-type和factor-type 两种hyper-class之间的关键区别是：super-type是由fine-grained类隐含的，而factor-type对于给定的fine-grained类是未知的。factor-type另一个示例为人脸的不同表情（愉快，愤怒，微笑等），每个人都有不同表情的多张图像。 正则化学习模型 给定细粒度训练图像（fine-grainedtraining images）和辅助超类标记图像（auxiliary hyper-class labeled images），一个直接的策略是通过共享共同的特征和学习分类器来训练多任务CNN。在多任务学习中，hyper-class类和fine-grained类的标签集是不相交的，我们不用hyper-class标签来标记fine-grained的数据。 Factor-type Hyper-class Regularized Learning 给定图像x，识别结果为y的概率为：Pr(v|x)是任何 factor-type hyper-class v的概率，并且Pr(y|v,x)指定给定factor-type hyper-class和输入图像x的任何fine-grained class的概率。使用softmax函数对factor-type hyper-class概率进行建模，h(x)表示x的高层特征，即：其中uv表示hyper-class分类模型的权值，Pr(y=c|v,x)如下计算：其中w{v,c}表示factor-specific精细分类模型的权值，此时预测公式为：虽然我们的模型在混合模型中有其根，但是值得注意的是，不像大多数以前的混合模型，处理Pr(v|x)作为自由参数，我们将其制定为一个鉴别模型。它是 hyper-class增强图像，允许我们准确地学习{uv}。 然后我们可以记录Dt中的细粒度识别数据的负对数似然性和Da中的数据的hyper-class识别：为了激励非平凡正则化，我们注意到factor-specific权重w_{v,c}应该捕获与对应的factor-type hyper-class分类器uv类似的高水平factor-related特征。为此在w{v,c}和uv间引入正则化：为了理解正则化，在我们的汽车识别示例中，原始的fine-grained 数据不能够学习每视角类别分类器w{v,c}，因为没有办法推断视角hyper-class。但现在我们可以在hyper-class增强数据上训练视点分类器uv，因此正则化负责将知识传递到每个视角类别分类器，从而帮助模糊fine-grained任务中的类内方差。引入w’{v,c}=w_{v,c}-u_v，则上式化简为：$Pr(y=c|x)$由下式给出：可以看出，fine-grained 分类器与factor-type hyper-class分类器共享相同的分量$u_v$。 因此，它将所提出的模型连接到传统的浅层多任务学习中使用的权重共享。 Super-type hyper-class regularized learningsuper-type hyper-class正则化深度学习的唯一区别在于$Pr(y|v,x)$，它可以简单地建模因为super-type hyper-class $v_c$由fine-grained标签c隐含地表示。正则化为： 统一的深度CNN 使用hyper-class增强数据和多任务正则化学习技术，我们达到统一的深CNN框架，如图所示：其示了优化问题： Xie S, Yang T, Wang X, et al. Hyper-class augmented and regularized deep learning for fine-grained image classification[C]// IEEE Conference on Computer Vision and Pattern Recognition. 2015.车型识别“Hyper-class Augmented and Regularized Deep Learning for Fine-grained Image Classification”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe自定义层]]></title>
      <url>%2F2016%2F12%2F19%2Fcaffe%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82%2F</url>
      <content type="text"><![CDATA[以后可能要在caffe内做一个判断的层，先学一下怎么添加新层。 developing new layer 开发一个新层 添加一个层的类声明到：include/caffe/layers/your_layer.hpp。 包括type的内联实现方法覆盖virtual inline const char* type() const { return &quot;YourLayerName&quot;; }，将YourLayerName替换为你的层名称。 实现{*} Blobs()方法来指定blob数量要求； 参阅/caffe/include/caffe/layers.hpp以使用内联{*} Blobs()方法强制执行top和bottom Blob计数。 如果你只实现CPU代码，则省略* _gpu声明。 在src/caffe/layers/your_layer.cpp中实现层。 （可选）用于一次性初始化的LayerSetUp：读取参数，固定大小的分配等。 Reshape用于计算top blob的大小，分配缓冲区以及取决于bottom blob的形状的任何其它工作。 Forward_cpu用于层的计算 Backward_cpu用于其反向梯度传播（可选 - 图层可以是仅向前传播） （可选）在layers/your_layer.cu中实现GPU版本Forward_gpu和Backward_gpu。 如果需要，在proto/caffe.proto中声明参数，使用（然后增加）”next available layer-specific ID”(“下一个可用的特定于层的ID”)在需要的message LayerParameter之上。 使用layer_factory.hpp中提供的宏在cpp文件中实例化并注册层。 假设有一个新层MyAwesomeLayer，可以使用以下命令实现它： 12INSTANTIATE_CLASS(MyAwesomeLayer);REGISTER_LAYER_CLASS(MyAwesome); 注意，应该将注册代码放在自己的cpp文件中，因此实现层是自包含的。 或者，如果您的图层有多个engines，也可以注册Creator。 示例见：caffe/layer_factory.cpp中的GetConvolutionLayer。 在test/test_your_layer.cpp中写入测试。 使用test/test_gradient_check_util.hpp来检查Forward和Backward。 仅向前传播层 如果写一个只包含在测试网络中的层，可不必编写反向传递。 可以在include/caffe/your_layer.hpp中编写一个Backward_cpu（或Backward_gpu）的内联实现以及您的图层的定义，如下所示：123virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123; NOT_IMPLEMENTED;&#125; NOT_IMPLEMENTED宏（在common.hpp中定义）会抛出一个错误日志“尚未实现”。 例如，查看准确度层（accuracy_layer.hpp）和阈值层（threshold_layer.hpp）定义。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[python文件命名小脚本]]></title>
      <url>%2F2016%2F12%2F12%2Fpython%E6%96%87%E4%BB%B6%E5%91%BD%E5%90%8D%E5%B0%8F%E8%84%9A%E6%9C%AC%2F</url>
      <content type="text"><![CDATA[写个文件命名的python程序，复（yu）习一下python。程序写得应该不是很好T T。。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import os;import shutil;from PIL import Image;##输入为图片路径##命名方式为统一位数数字递增##单一文件格式查找##输出为：## result.txt 文件名，是否为完整图片## \full 完整图片库## \part 不完整图片库path="D:\mydatabase";#设置工作目录os.chdir(path)#更改工作目录f=open("result.txt","w");#打开输出到的txt文件def rename(begin,weishu):#批量重命名，begin：起始数。weishu：数字位数 count=begin; filelist=os.listdir(path)#该文件夹下所有的文件（包括文件夹） for files in filelist:#遍历所有文件 Olddir=os.path.join(path,files);#原来的文件路径 addstr="";#补增0 if os.path.isdir(Olddir):#如果是文件夹则跳过 continue; addwei=weishu-len(str(count));#计算需要填多少0 for i in range(addwei): addstr+="0";#增加到所需位数 filename=os.path.splitext(files)[0];#文件名 filetype=os.path.splitext(files)[1];#文件扩展名 if filetype!=".jpg":#如果不为jpg文件则跳过 continue; string=str(count);#将计数值转为字符 newfilename=addstr+string+filetype;#合成最终文件名 f.write(newfilename+" ");#输出文件名到文件 Newdir=os.path.join(path,newfilename);#新的文件路径 os.rename(Olddir,Newdir);#重命名 classify(newfilename)#进行分类操作 count+=1;#计数加一 print(newfilename);#打印当前处理文件名 f.close(); def classify(filename):#分类函数，filename：所需分类文件名 img = Image.open(filename)#打开所要分类的图片 imgsize=img.size;#图像大小 full=1;#是否完整，1为完整 w=imgsize[0]; h=imgsize[1]; rate=float(w)/float(h);#长宽比 if rate&lt;0.67 or rate&gt;1.5 or w&lt;200 or h&lt;200: full=0; if full: if not os.path.isdir("full"):#是否存在full目录 os.mkdir("full");#新建full目录 Newdir0=os.path.join(path+"\\full",filename); shutil.copyfile(filename,Newdir0);#复制文件 f.write("1"+"\n");#输出是否完整标记 else: if not os.path.isdir("part"):#是否存在part目录 os.mkdir("part");#新建part目录 Newdir0=os.path.join(path+"\\part",filename); shutil.copyfile(filename,Newdir0);#复制文件 f.write("0"+"\n");#输出是否完整标记 rename(0,6);#运行]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO：You Only Look Once 论文阅读]]></title>
      <url>%2F2016%2F12%2F11%2FYOLO%EF%BC%9AYou-Only-Look-Once-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%2F</url>
      <content type="text"><![CDATA[You Only Look Once: Unified, Real-Time Object Detection YOLO官网论文阅读笔记：You Only Look Once: Unified, Real-Time Object Detection 简介与特点YOLO是今年CVPR上提出的一种目标检测方法，其速度达到了45fps（YOLO v2 达到了67fps），完全可以处理视频。其框架是直接利用CNN的全局特征预测每个位置可能的目标，比RCNN先检测多个RP再CNN快了很多。 实际测试中，检测提供的dog.jpg图片（768*576）使用了0.043s，GPU是K40，也到了20fps以上。其大致流程为： 缩放图片到448x448； 运行单个CNN； 通过模型的置信度阈值得到检测结果。 除了速度快这一特点，还有： 背景误差小YOLO在预测时考虑全局图像，与滑动或region proposal只提出图像中一块区域不同。因此更能得到分类目标附近区域信息， 与快速R-CNN相比，YOLO的背景误差数量少于一半。 学习对象的可概括表示当训练对象为自然图像，而测试图像为艺术作品时，有很好的检测率。 不足精度相比于最先进的检测系统，但YOLO v2 已经达到了76.8的mAP。 统一检测YOLO将分离的组件检测器统一到一个神经网络中。网络使用整幅图的特征来预测bounding box，同时进行分类，网络全局地考虑整个图像和图像中的所有对象。 首先把图像分成$S \times S$个网格，如果对象的中心落入网格单元中，则该网格单元负责检测该对象。每个网格单元预测这些框的$B$个bounding box和置信度得分。 这些置信度分数反映了box包含某种对象的自信程度，以及它对框预测的box的准确程度。定义置信度为： $$Pr(Object)*IOU_{pred}^{truth}$$ 如果该网格中没有目标，则Pr项值应该为0，整体为0，否则为1。每个bounding box包括5个预测值：x，y，w，h和置信度。$(x,y)$坐标表示相对于网格单元的边界的框的中心。每一块需要预测的值是bounding box相对于该块中心的偏移，以及相对长宽，类别。每个网格单元还预测$C$条件类概率$Pr(Class_i|Object)$。 这些概率以网格单元包含对象为条件，即 最终预测结果为$S\times S\times (B * 5+C)$的张量。 网络设计模型以CNN实现，网络的初始卷积层从图像中提取特征，而完全连接的层预测输出概率和坐标。网络有24卷积层，其次是2完全连接的层。如图所示：输出为$7 \times 7 \times 30$的张量。每个grid有30维，这30维中，8维是回归box的坐标，2维是box的confidence，还有20维是类别。 其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间。 训练 最终层预测类概率和边界框坐标。 通过图像宽度和高度将边界框宽度和高度归一化，使得它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的偏移，使得它们也在0和1之间 。 最后一层使用线性激活函数（linear activation function），其他层使用leaky rectified linear activation： $$\phi (x)=\begin{cases}x, &amp; \text{if $x&gt;0$}\\ 0.1x, &amp; \text{otherwise}\end{cases}$$ 优化了模型输出中的平方误差。使用平方误差，因为它很容易优化，但它不完全符合我们的最大化平均精度的目标。 It weights localization error equally with classification error，即8维的localization error和20维的classification error 同样重要，这可能不是理想的。 此外，在每个图像中，许多网格单元不包含任何对象。 这将这些网格单元的“置信度”为零，通常压倒包含对象的网格的梯度。 这可能导致模型不稳定，导致训练早期发散。为了弥补这一点，增加了边界框坐标预测的损失，并减少了对不包含对象的框的置信预测的损失。 使用两个参数： $$\lambda_{coord}=5$$ $$\lambda_{noobj}=0.5$$ 来完成这个。通过这样的因子设置，更重视8维的坐标预测，即给这些localization error损失前面赋予更大的loss weight, 更小的classification error 。对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的。而sum-square error loss中对同样的偏移loss是一样。 预测边界框宽度和高度的平方根，而不是宽度和高度。在训练期间，我们只需要一个bounding box预测器负责每个对象（原本每个网格单元会有B个预测）。 我们将一个与ground truth相比的最高IOU的预测器指定为“负责”。loss function：其中$1i^{obj}$表示如果对象出现在单元格i中，$1{ij}^{obj}$表示网络单元格i中的第j个bounding box预测器对于该预测是“负责”的。如果对象存在于该网格单元中，则损失函数仅惩罚分类误差（因此前面讨论的条件类概率）。 它也只惩罚边界框坐标误差，如果该预测器是对ground truth “负责”的。即： 只有当某个网格中有object的时候才对classification error进行惩罚。 只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。 测试结果如表：特别的，作者还进行了艺术作品中的检测识别，以测试其泛化性能。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[愿饮一盏口嚼酒]]></title>
      <url>%2F2016%2F12%2F05%2F%E6%84%BF%E9%A5%AE%E4%B8%80%E7%9B%8F%E5%8F%A3%E5%9A%BC%E9%85%92%2F</url>
      <content type="text"><![CDATA[0 愿饮一盏口嚼酒，可唤三载牵丝梦。 1单单就这部电影来说，并不是一个悲剧，当时我也并没有哭，这样的结局，真好。 重要的人，不能忘记的人，不想忘记的人 是的，这是人，而并不是一个名字。用笔写在书上，记在日记里，写在你我手上，名字是毫无意义的。就像它出现在遇难者名单中，只是一个冰冷的名字罢了。幸好在你手上写的是“喜欢你”啊。这样才会有相互吸引的动力吧。三叶为什么能够割去头发赶赴东京，泷又如何会独自跑到荒凉的山谷，三年前已经有了答案。 2一条短短的绳结系住了二人。 聚拢，成形，捻转，回绕，时而返回，暂歇，再联结。这就是组纽。这就是时间。这就是产灵（musubi） “产灵”可能没有很好的表达出它应有的含义，但我想每个人都知道这是一种什么样的东西。这是祖母所说的，而祖母与三叶四叶经常在做一些绳结。不同颜色的丝线纵横交错，一旦成节，就难以分开。我们文化中类似的正如中国结，比这样一个绳带要复杂多了。 等待就是它本身的目的。不一定等到甚么，只要等，连系就在。——梁文道 名字忘了一次又一次，而结不散。 3我最感兴趣的是口嚼酒，从一开始制作的过程我就被吸引了：少女嚼下米饭再吐出，经过自然发酵形成酒。而承装米的盒子，同样是用红色的绳线所系住。不知道为什么我觉得这一过程很美。这代表了三叶最重要的东西，是她的“一半”（灵魂的一部分，或者说是神明与现实的纽带）。或许这是陨石撞击后三叶唯一能够留下来的东西吧，而泷喝下了它才得以再次回到过去。 水也好，米也罢，还有酒，什么东西进入身体的过程，也叫作产灵（musubi）。进入身体的东西，会和魂相结合。所以今天的奉纳，是宫水一脉传承百年，让人和神灵相联结的重要的传统。 喝下口嚼酒是进入身体的结合过程，那么制作口嚼酒，便是如同系结一般。三年前三叶除了给了泷自己头上发带的绳结，能够让二人相遇相识，更是“准备”了灵魂的绳结。泷和三叶能以这样的一种方式结合，真的很好。 4喜欢只有相互才是美好的，很羡慕这样的人，也为他们祝福。可是如何才能满足这个条件呢，交换身体是一个好的选择吗？能够喜欢别人并去追寻是一个很重要的技能，但另一方面，知道别人喜欢自己，自己也能喜欢对方却是可遇不可求的。就像三叶能从乡下来到东京，就可以正好碰到了列车上的泷，这种可能性有多少呢？而从嘈杂的列车上，泷又能记得住一个只有一面、一带之缘的人的名字吗？或许很多人都觉得这只是电影中剧情的需要吧。 可我确相信，的确会有这样的事情发生，但是你永远不可能提前预知，因为真正能够触动内心，变成不想忘记的人还是少数，大多数只是匆匆的过客。或许等到一两年后，甚至七八年后，对往事重新回忆，才发现原来结早已牢牢系好。 祖母说她们家族一直都有这种“交换身体”的梦，三叶父亲母亲是否也交换过呢？我感觉可能没有，因为他之前很少理解三叶的变化，虽然说最后父亲还是相信了女儿进行了演习，这可能只是一种信任。如此看来交换身体也并非是一个达到相互喜欢好的选择啊。三叶帮泷去和前辈约会，自己却不知为何在镜子前落下了眼泪；泷与前辈身在一起，心却不在：电梯里努力不让自己碰到前辈，在展览馆看到糸守町的图片，伫立许久……因为他们已经知道了，谁是 重要的人，不能忘记的人，不想忘记的人。 5君の名は.这句话没有用问号，心中已经有了答案，看似是最重要的元素，却又是无足轻重的代号。五年后，我们知道了泷已经毕业，希望成为一个即使城市突然消失，也能能够让城市留下些什么的建筑师。而更重要的是三叶的故事，从乡下来到城市，应该比泷更辛苦一些。最后泷从阶梯又下到上，三叶由上至下，相遇无言，却又猛然回头相问，可能是对他们最好的结局吧。 End?仔细想想，哭和笑一样只是感情的表达而已，即使是喜剧，我们被泷和三叶的追寻过程所感动，为最终的相遇而喝彩，都是可以用这种方式表达的。那么我，或许是没有到最后哭或笑的时候吧。 写于2016.12.3 17:00，成都]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[第一次选举]]></title>
      <url>%2F2016%2F11%2F29%2F%E7%AC%AC%E4%B8%80%E6%AC%A1%E9%80%89%E4%B8%BE%2F</url>
      <content type="text"><![CDATA[好像从初中课本上就已经知道每个人拥有选举权和被选举权，每个成年人都有选举的权利。可是至今没有见过选票是什么样的，似乎枉做了一位中国公民。但昨天突如其来的一份选举通知让我有了这样一个机会，我也可以当家作主一次了。一同发来的还有一份候选人资料，上面列了四名候选人以及其工作经历，没有像班级里面的选举，至少应该有个竞选宣言吧。看了一遍自己毫无感觉，完全不知道应该投给谁。如果说上面附上每名候选人的照片，我可能就会以貌取人选择长得不错的，可惜别人肯定看透了这个小心思，四名女候选人都没有照片，那么该如何评选呢？ 投票时间是上午9:30到10:40，为了减少绕路，出一次门干完所有事，我选择临近结束投票，之后正好赶上午饭。投票地点在学院的小房间，做个小活动开个小会都在这里。两名工作人员，一个还拿着手机应该是在拍照录像。这突然就显得正式严肃了很多。首先要填一个选民证，是自己的姓名年龄之类的。可是工作人员也没有查验我的身份，仅凭自己填就可以投票，似乎不是很合理，万一有敌对势力破坏选举，或者选票造假呢？班上其他需要选举的人好像有的没有来，完全可以顶替投票嘛。在这之后终于拿到了庄严的一票：之后呢，也要投好庄严一票。……可面前只有四个名字，从昨天见到到今天投票还不过一天，班干部竞选也要经过几个月呢。没办法，既然来了还是要投的，看哪个顺眼选哪个吧。首先选了一个名字有三个字的（其他都是两个字），之后又把第一个和最后一个选了，有始有终。那么第二个就无法再被选，也只好这样。打上圆圈，把选票交还给工作人员，我也就离开了。 吃饭的路上，发现出太阳了真是美好的一天——也是第一次行使自己选举权的一天。然而我突然感到有些对不起第二个候选者，万一她才是有政见、有能力的人呢？这种感觉很奇怪，明明行使了自己的权利，却感觉和没有一样，唯一值得纪念的或许只是见到了选票？在没有了解候选人的情况下投了票，或许还不如弃权呢。同学没有去，说结果肯定是内定的，我却不是很赞同。我相信投出的选票会真正被统计并选出得票较多的人，不过不同的选举结果会为社会带来多大的改变，却很难得知。但是既然作为了选民，经历了选举，我可以对别人说中国是有民主的，因为是我选出了人大代表。不过如果有人继续问下去，我可能就无法回答了，或许需要等到我成为候选人的那一天才会有答案吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Machine learning- Trends, perspectives, and prospects]]></title>
      <url>%2F2016%2F11%2F26%2FMachine-learning-Trends-perspectives-and-prospects%2F</url>
      <content type="text"><![CDATA[论文阅读：Machine learning: Trends, perspectives, and prospects绪论 机器学习强调让电脑通过学习自动地提高自己。其发展基于： 新的学习算法和理论 可用数据增多和计算成本变低 可以看到，机器学习的这几年的流行是多方面的因素，第二点的作用甚至更大一些：作为机器学习“燃料”的数据不仅提高了准确性，更避免了过拟合，提升泛化性能；计算速度的大幅提高，才让九十年代末进入瓶颈的算法得以真正运行，并且应用到较为复杂的图像领域，使深层网络成为可能。 其关注问题在于： 如何构建一个让机器通过经验（experience）学习从而得到提升？ 什么是掌控所有学习系统基本统计计算理论？(What are the fundamental statistical-computational-information-theoretic laws that govern all learning systems, including computers,humans, and organizations?) 第一个问题较为容易理解，这是一个很实际的问题。比如我们可以定义损失函数来评价模型目前的偏差，并想方设法减小这些偏差，我们的模型也就得到了提升。这种反馈的思想也早已在电子、控制行业中采用，只不过用途不尽相同：机器学习的目的是预测甚至是生成，即输入未知数据来得到分析之后的属性，或通过分析产生新的数据；而电子控制中的目的是稳定，在干扰的条件下将所控制的值控制到我们想要的值上。第二个问题很难回答。目前在人脑的机理尚未完全了解的情况下，我们已经开始尝试通过纯计算来达到学习的效果。可能我们做的只是不断地优化，但效果已经显现。 通过二十年的发展，机器学习已经在很多方面得到了应用，原文也举了很多例子。 学习问题可以被定义为当通过某种类型的训练经验来执行某个任务时改善一些性能度量的问题。 增强性能减小误差是学习的目标，改变误差的定义往往会达到不同的效果，比如对label true 而output false 的结果（未检出）加大惩罚，就会达到“宁可错杀一千不可放过一个”的效果。当然这只是有客观结果的学习，对于无客观结果，如融合两幅画的风格生成新的一幅画，很难用人的判断去决定误差大小。 许多算法关注于函数的近似问题。 传统回归问题也是一种函数的近似，而当我们我们把图片看作是函数输入$x$，分类结果看作是输出$y$，那么网络可以看作一种较为复杂的函数$f(x)$（简单说是经过了很多层的矩阵运算）。传统我们可以根据公式，使用不同模型（多项式、高斯函数）对输入输出进行回归拟合，达到预测效果。而如今数据量太大，很难得到解析解，这就需要不断地改变参数从而使误差减小。 无论什么学习算法，一个关键的科学和实践目标是在理论上描述特定学习算法的能力和任何给定学习问题的固有困难：算法如何准确地从特定类型和大小的训练数据学习？算法对于其建模假设中的误差或训练数据中的误差有多鲁棒性？给定一个给定量的训练数据的学习问题，是否有可能设计一个成功的算法或这个学习问题根本难以处理？ 也正是因为采用不断迭代更改参数提高性能的原因，一旦模型确定，我们很难再去更改学习后模型内的参数。输入的数据也不同，得到的模型结果很难是相同的。这样一切得到的结果就是数学上的概率。 Drivers of machine-learning progress 过去十年，网络和移动计算系统收集和传输大量数据的能力迅速增长，这种现象通常被称为“大数据”。收集这些数据的科学家和工程师经常转向机器学习，并从这些数据集获得有用的预测和决策。事实上，数据的绝对尺寸（sheer size）使得必须开发出可扩展的程序，同时考虑计算和统计，但问题不仅仅是现代数据集的大小；它是许多这些数据的粒状（granular），个性化的性质。 大数据差不多也是和机器学习同时兴起的，这是十年中数据指数型上升的必然结果。同样数据也从集约化的数据库（由工程师指定数据的项目、输入的格式），变成了发散的个性化的内容，每个人都可以是数据的制造者。这对数据的处理和分析带来了一定的困难。 Core methods and recent progress下面是核心方法和目前进展 The most widely used machine-learning methods are supervised learning methods 目前看来还是监督学习是使用最广泛的。这也就是上面说的函数拟合问题了：通过学习映射f（x）形成它们的预测，其为每个输入x（或给定x的y上的概率分布）产生输出y。当然存在许多不同形式的映射f，包括决策树，决策树，逻辑回归，支持向量机，神经网络，内核机器和贝叶斯分类器。不过这些不是重点，重点在于深度学习。 深度网络是阈值单元（threshold units）的多层网络，每个网络计算其输入的一些简单的参数化函数。 目前的深度网络在利用GPU的并行计算下，参数已经达到了数十亿，真是一个庞大的数字。。不过效果是越来越智能了，下图是两个例子： 尽管深度学习的大量实际成功来自用于发现这种表示的监督学习方法，但是还努力开发不需要标记训练数据的深度学习算法。一般问题被称为无监督学习，机器学习研究的第二范例。广泛地，无监督学习通常涉及在关于数据的结构性质（例如，代数，组合或概率）的假设下对未标记的数据的分析。 非监督学习是接下来的一个研究重点，Yann LeCun 在CMU的演讲上也特别提到了这一点（演讲内容）。未来的机器学习不仅要能学会，还要能通过学习到的进行创造，如生成对抗网络，视频预测等。 回归到论文中，第三个主要的机器学习例子是强化学习（ reinforcement learning ）。这里训练对象与外部环境有很多的互动。基本原理为： 如果Agent的某个行为策略导致环境正的奖赏(强化信号)，那么Agent以后产生这个行为策略的趋势便会加强。Agent的目标是在每个离散状态发现最优策略以使期望的折扣奖赏和最大。 Emerging trends 一个主要趋势是对机器学习算法操作的环境的日益关注。词语“环境”在这里部分地指计算架构；而经典的机器学习系统涉及在单个机器上运行的单个程序，现在通常将机器学习系统部署在包括数千或十万个处理器。 文中的例子是购物推荐系统，这是一个很实用并且应用范围很广的系统。对于每个人可以独立地学习其购物习惯，而对于一类人也可以有一个整体的学习和分类。大小不同层次和类别，可以让机器学习更有针对性。小的可以部署在个人客户端，大的可以在服务器，这样也充分利用了资源。 词语“环境”还指数据源，其范围从可能具有隐私或所有权关注的一组人，到可能对机器学习系统具有某些要求的分析员或决策者（例如 ，其输出是可视化的），以及围绕系统部署的社会，法律或政治框架。 环境还可以包括其他机器学习系统或其他代理，并且系统的整体集合可以是合作的或敌对的。 广义地说，环境向学习算法提供各种资源并对这些资源施加约束。 机器学习研究人员越来越正式化这些关系，旨在设计在各种环境中可证明有效的算法，并明确允许用户表达和控制资源之间的权衡。 这里的环境是更大的环境，甚至超出了算法的范围。 之后是分布式的学习系统。学习是一项很耗费资源的计算活动，单个区域的计算资源不足时，采用分布式计算会大大提高效率。这里需要解决的是通信的问题了。 最终目标是除了精度要求之外还能够向机器学习系统提供时间和空间预算，系统找到允许实现这种要求的操作点。 Opportunities and challenges目前的机器学习方法，是我们在如人类和其他动物，组织，经济和生物进化中观察到的学习类型。机器学习由简到难，这导致一些研究人员开始探索如何构建计算机终身学习者或永无止境的学习的问题。或者说通过学习能否找到一种最佳的学习方法从而自我学习。这样的话计算机是否就拥有了自我进化的能力呢。与自然学习系统类比的另一个方面提出了基于团队，混合主动学习的想法。 当然目前很大的问题在于数据来源与隐私。真正的大数据往往只存在于少数的几家大型互联网公司，而这些数据的来源就是网上每名用户。或许你正在使用免费的网盘服务，随时同步的照片为你带来了方便。但这些照片对于机器学习来说是最好的原料，而这些公司是否会在用户不知情的情况下拿来使用分析呢？从用户个人来看可能很难接受，但每一张单独的照片对于机器学习无关紧要，只有大量整体的数据才能达到最好的效果，比如非监督学习。由此带来的社会问题也是值得我们思考。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[from wordpress to hexo 搬迁成功]]></title>
      <url>%2F2016%2F11%2F18%2Ffrom-wordpress-to-hexo-%E6%90%AC%E8%BF%81%E6%88%90%E5%8A%9F%2F</url>
      <content type="text"><![CDATA[搬迁成功！之前网站建立在树莓派服务器上，很大的问题在于没有固定的IP，经常找不到服务器。。 经过一段时间的探索，发现github.io真是建站神器，免费的空间还没有什么限制。唯一一点可能是在PKU更新内网无法访问……这也导致一开始的尝试一直处于失败状态。（另外git clone coding也失败，这个不是内网吗？） 注意的问题当然在部署时还是有一定的问题出现： Deployer not found 当键入hexo d时会出现 1RROR Deployer not found: 这是因为git的部署器没有安装，需要安装后： 1npm install hexo-deployer-git --save 再执行。 当然遗憾的是还是出错： 123456789101112131415 NFO Deploying: gitINFO Setting up Git deployment...'git' FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: spawn git ENOENT at notFoundError (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:11:11) at verifyENOENT (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:46:16) at ChildProcess.cp.emit (F:\myhexo\node_modules\cross-spawn\lib \enoent.js:33:19) at Process.ChildProcess._handle.onexit (internal/child_process.js:215:12)FATAL spawn git ENOENTError: spawn git ENOENT at notFoundError (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:11:11) at verifyENOENT (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:46:16) at ChildProcess.cp.emit (F:\myhexo\node_modules\cross-spawn\lib \enoent.js:33:19) at Process.ChildProcess._handle.onexit (internal/child_process.js:215:12) 跟当初wordpress一样总会留个问题啊。。 注意站点设置文件与主题设置文件的区别 这个在Next的设置中其实也有提示，然而我并没有很注意= =||一开始多说的评论设置在站点设置文件更改没有问题，但菜单始终失败，最后发现是应该在主题设置文件更改。 下一步之前wordpress的文章基本上都迁移过来了，但是图片就比较麻烦了，需要重新整理，抽空弄一下吧。Hexo生成的是静态网页，有一点不好是非常依赖hexo程序的生成，这样就不能随时随地发了（当是发微博吗？）……总之小站重新复活，好好干~ 感谢Hexo，感谢Next主题对本博客的大力支持。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe-faster-rcnn demo测试]]></title>
      <url>%2F2016%2F11%2F12%2FCaffe-faster-rcnn%20demo%E6%B5%8B%E8%AF%95%2F</url>
      <content type="text"><![CDATA[RCNN是目前detection中较新且准确度较高的方法，充分发挥了CNN分类的优势，但速度并不快，从而产生了fast rcnn和faster rcnn来解决这个问题。本文使用py-faster-rcnn对该方法做一初步测试。 rbgirshick/py-faster-rcnn 环境准备软件环境 Caffe Python 一般来说这些我们都已经有所接触，但仍有一些需要注意的地方： 要使用rbgirshick/py-faster-rcnn中的caffe编译一次，其caffe在rbgirshick/py-faster-rcnn/caffe-fast-rcnn @ 0dcd397中。因为这里面有一些专门为f-rcnn写的层，具体区别可以在caffe.proto中查看，如增加了ROIPoolingParameter、SmoothL1LossParameter等参数。 编译时一定要增加对Python层（Python layers）的支持。具体需要打开Makefile.config，找到： 12# In your Makefile.config, make sure to have this line uncommentedWITH_PYTHON_LAYER := 1 将其改为1，否则运行时会出错，提示没有对应的layer。 硬件要求小的网络用Titan, K20, K40这些就可以，显存3G以上。大的可能需要K40,11G以上显存，当然这些往往个人无法搭建起来。 安装（DEMO） 编译Cython模块 12 cd $FRCN_ROOT/libmake $FRCN_ROOT为你的FRCNN根目录，下同。 编译Caffe 和 pycaffe 123456cd $FRCN_ROOT/caffe-fast-rcnn # Now follow the Caffe installation instructions here: # http://caffe.berkeleyvision.org/installation.html # If you&apos;re experienced with Caffe and have all of the requirements installed # and your Makefile.config in place, then simply do:make -j8 &amp;&amp; make pycaffe -j8是指8核编译，更快一些。 下载预计算的R-CNN检测器12cd $FRCN_ROOT./data/scripts/fetch_faster_rcnn_models.sh 这个模型解压出来750M，下载的话大概695M，而且很慢。。为了方便大家，我把模型上传到了百度云，faster_rcnn_models， 密码：gbpo。 运行这一步就很简单了，12cd $FRCN_ROOT./tools/demo.py 当然权限不足直接运行py也可以。这个运行是需要在图像界面下进行的，否则会报错。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（11）python的数据可视化]]></title>
      <url>%2F2016%2F11%2F09%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%8811%EF%BC%89python%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
      <content type="text"><![CDATA[caffe本身没有可视化的工具，一般需要配合python或matlab实现数据的可视化，在实践本文之前要先把caffe python编译好。另外有的服务器只有shell，没有可视化的界面，只好先把每一层的数据先保存成图片格式，再进行显示。 Caffe学习系列(14)：初识数据可视化Python and/or MATLAB Caffe (optional) 载入数据12345678910import numpy as npimport caffefrom skimage import iocaffe_root='/home/XXX/caffe/'import os,sysos.chdir(caffe_root)sys.path.insert(0,caffe_root+'python')im = caffe.io.load_image('examples/images/cat.jpg')io.imsave('cat.jpg',im)print im.shape 这里用到了skimage 这个库，caffe用于读取图像的函数caffe.io.load_image也是用的这个，具体可以在python/caffe/io.py中查看。之后我们也用这个库进行图像的保存。以上的程序较为简单，读取了示例图片，为了验证是否正确又另存为了副本。最后输出的shape为：（360,480,3）。 载入卷积模型1net = caffe.Net('examples/net_surgery/conv.prototxt', caffe.TEST) 123456789101112131415161718192021222324252627# Simple single-layer network to showcase editing model parameters.name: "convolution"layer &#123; name: "data" type: "Input" top: "data" input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 100 dim: 100 &#125; &#125;&#125;layer &#123; name: "conv" type: "Convolution" bottom: "data" top: "conv" convolution_param &#123; num_output: 16 kernel_size: 5 stride: 1 weight_filler &#123; type: "gaussian" std: 0.01 &#125; bias_filler &#123; type: "constant" value: 0 &#125; &#125;&#125; 载入的是示例中简单的卷积模型，但在shape上有所修改：第二个dim由1改为3，代表三通道输入；同时num_output改为了16，增加了滤波器的个数。 数据格式处理12345im_input=im[np.newaxis,:,:,:].transpose(0,3,1,2)print "data-blobs:",im_input.shape#print "datashape:",net.blobs['data'].data.shapenet.blobs['data'].reshape(*im_input.shape)net.blobs['data'].data[...] = im_input 图片的输入规格和caffe的blob规格并不相同。图片的维度为（360,480,3），而blob的4维数组要求通道数在前，因此需要改变顺序，并且由于仅有一张图片，需要增加一维代表图片序号，该维值为0即可。因此im_input=im[np.newaxis,:,:,:].transpose(0,3,1,2)先增加了一个维度，后改变了维的顺序，使其与输入要求相同。之后改变blobs数据层的维度，使之与图像大小相同（这一步感觉会让网络配置文件中input_param：shape的维度改变，可能是为了方便程序的扩展，没有直接改配置文件）。最后把图像数据输入到blob。 保存图像为了方便调用，可以写一个保存图片的函数：1234567891011def save_data(data,name,padsize=1, padval=0): data -= data.min() data /= data.max() # force the number of filters to be square n = int(np.ceil(np.sqrt(data.shape[0]))) padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3) data = np.pad(data, padding, mode='constant', constant_values=(padval, padval)) # tile the filters into an image data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1))) data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:]) io.imsave(name,data) 保存图片首先进行了归一化操作，之后为了美观生成一个方形模板，再把图片依次放上去。测试这一段可以看一下原图多通道的每一通道分量：1save_data(net.blobs['data'].data[0],'origin images.jpg') 卷积层输出123456net.forward()print "data-blobs:",net.blobs['data'].data.shapeprint "conv-blobs:",net.blobs['conv'].data.shapeprint "weight-blobs:",net.params['conv'][0].data.shapesave_data(net.params['conv'][0].data[:,0],'conv weights(filter).jpg')save_data(net.blobs['conv'].data[0],'post-conv images.jpg') 经过一次向前计算，得到了卷积后的结果和初始的卷积核值，打印他们的大小分别为： data-blobs: (1, 3, 360, 480) conv-blobs: (1, 16, 356, 476) weight-blobs: (16, 3, 5, 5) 最后保存成了两个图片文件：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（10）数据转换img2db]]></title>
      <url>%2F2016%2F11%2F08%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%8810%EF%BC%89%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2img2db%2F</url>
      <content type="text"><![CDATA[在处理图像时，我们已拥有的图像往往是常用的jpg、png格式，但在caffe中，输入的数据类型常是lmdb或leveldb，因此我们需要对原始数据进行转换。 Caffe学习系列(11)：图像数据转换成db（leveldb/lmdb)文件 convert_imageset在caffe中，提供了一个用于格式转换的文件：convert_imageset.cpp，存放在根目录下的tools文件夹下。编译之后，生成对应的可执行文件放在 build/tools/ 下面，这个文件的作用就是用于将图片文件转换成caffe框架中能直接使用的db文件。而windows平台下，如果用vs编译会在build/x64/debug中生成convert_imageset.exe。 使用方法该文件的使用格式： convert_imageset [FLAGS] ROOTFOLDER/ LISTFILE DB_NAME 而数据集推荐的是imagenet。对于具体的参数包含有： FLAGS：图片转换参数。 ROOTFOLDER/：图片的绝对路径，从系统根目录开始。 LISTFILE：图片文件清单文件，为txt格式，一行有一张图片。 DB_NAME：生成db文件的存放目录。 其中图片文件清单比较麻烦，因此可以使用脚本文件读取图片并保存为txt。 图片文件清单的生成本文以caffe程序中自带的图片为例，进行讲解，图片目录是 example/images/, 两张图片，一张为cat.jpg, 另一张为fish_bike.jpg，表示两个类别。我们创建一个sh脚本文件，调用linux命令来生成图片清单： # sudo vi examples/images/create_filelist.sh 编辑这个文件,输入下面的代码并保存：123456789# /usr/bin/env shDATA=examples/imagesecho "Create train.txt..."rm -rf $DATA/train.txtfind $DATA -name *cat.jpg | cut -d '/' -f3 | sed "s/$/ 1/"&gt;&gt;$DATA/train.txtfind $DATA -name *bike.jpg | cut -d '/' -f3 | sed "s/$/ 2/"&gt;&gt;$DATA/tmp.txtcat $DATA/tmp.txt&gt;&gt;$DATA/train.txtrm -rf $DATA/tmp.txtecho "Done.." 这个脚本文件中，用到了rm,find, cut, sed,cat等linux命令。 rm: 删除文件 find: 寻找文件 cut: 截取路径 sed: 在每行的最后面加上标注。本例中将找到的cat.jpg文件加入标注为1，找到的bike.jpg文件加入标注为2 cat: 将两个类别合并在一个文件里。 最终生成如下的一个train.txt文件： cat.jpg 1 fish-bike.jpg 2 当然，图片很少的时候，手动编写这个列表清单文件就行了。但图片很多的情况，就需要用脚本文件来自动生成了。在以后的实际应用中，还需要生成相应的val.txt和test.txt文件，方法是一样的。生成的这个train.txt文件，就可以作为第三个参数，直接使用了。 FLAGS参数设置接下来，我们来了解一下FLAGS这个参数组，有些什么内容： gray: 是否以灰度图的方式打开图片。程序调用opencv库中的imread()函数来打开图片，默认为false shuffle: 是否随机打乱图片顺序。默认为false backend:需要转换成的db文件格式，可选为leveldb或lmdb,默认为lmdb resize_width/resize_height: 改变图片的大小。在运行中，要求所有图片的尺寸一致，因此需要改变图片大小。 程序调用opencv库的resize（）函数来对图片放大缩小，默认为0，不改变 check_size: 检查所有的数据是否有相同的尺寸。默认为false,不检查 encoded: 是否将原图片编码放入最终的数据中，默认为false encode_type: 与前一个参数对应，将图片编码为哪一个格式：‘png’,’jpg’…… 好了，知道这些参数后，我们就可以调用命令来生成最终的lmdb格式数据了。 转换脚本的编写由于参数比较多，因此我们可以编写一个sh脚本来执行命令。首先，创建sh脚本文件： # sudo vi examples/images/create_lmdb.sh 编辑，输入下面的代码并保存：123456#!/usr/bin/en shDATA=examples/imagesrm -rf $DATA/img_train_lmdbbuild/tools/convert_imageset --shuffle \--resize_height=256 --resize_width=256 \/home/xxx/caffe/examples/images/ $DATA/train.txt $DATA/img_train_lmdb 设置参数-shuffle,打乱图片顺序。设置参数-resize_height和-resize_width将所有图片尺寸都变为256*256。/home/xxx/caffe/examples/images/ 为图片保存的绝对路径。最后，运行这个脚本文件： # sudo sh examples/images/create_lmdb.sh 就会在examples/images/ 目录下生成一个名为 img_train_lmdb的文件夹，里面的文件就是我们需要的db文件了。 针对于windows环境，可以使用bat快速运行： D:/caffe-master/Build/x64/Release/convert_imageset --shuffle --resize_height=256 --resize_width=256 D:/caffe-master/examples/images/ D:/caffe-master/examples/images/train.txt D:/caffe-master/examples/images/img_train_lmdb]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（9）LeNet在Caffe上的使用]]></title>
      <url>%2F2016%2F11%2F08%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%889%EF%BC%89LeNet%E5%9C%A8Caffe%E4%B8%8A%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
      <content type="text"><![CDATA[使用官网例程训练LeNet。 Training LeNet on MNIST with Caffe 准备数据Caffe程序的运行要注意需命令行要在Caffe的根目录下。 cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh 依次运行，会在caffe\examples\mnist下得到两个目录mnist_train_lmdb, 和 mnist_test_lmdb，作为训练和测试集。 定义MNIST网络Caffe上的LeNet并不是传统的LeNet-5，在参数上还是有所不同的。以\caffe\examples\mnist\lenet_train_test.prototxt 为例（本地文件与官网上的教程也有所区别），介绍一下如何定义网络。首先是定义网络名称：1name: "LeNet" 数据层利用我们已经生成的MNIST数据，把数据输入到网络：12345678910111213141516171819202122232425262728293031323334layer &#123; name: "mnist" type: "Data" top: "data" top: "label" include &#123; phase: TRAIN &#125; transform_param &#123; scale: 0.00390625 &#125; data_param &#123; source: "examples/mnist/mnist_train_lmdb" batch_size: 64 backend: LMDB &#125;&#125;layer &#123; name: "mnist" type: "Data" top: "data" top: "label" include &#123; phase: TEST &#125; transform_param &#123; scale: 0.00390625 &#125; data_param &#123; source: "examples/mnist/mnist_test_lmdb" batch_size: 100 backend: LMDB &#125;&#125; 具体来说，本层名为：”mnist”，类型为”data”，输出到两个blob：”data””label”。下面用到了之前所说的include，包含TRAIN与TEST，表示该层是在训练还是测试时调用，其区别在于输入的不同数据集（见data_param）。transform_param用于输入数据的缩放，使之在$[0,1]$内，其中$0.00390625=1/256$。 卷积层本网络中，有两个卷积层，第一层为：1234567891011121314151617181920212223layer &#123; name: "conv1" type: "Convolution" bottom: "data" top: "conv1" param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; convolution_param &#123; num_output: 20 kernel_size: 5 stride: 1 weight_filler &#123; type: "xavier" &#125; bias_filler &#123; type: "constant" &#125; &#125;&#125; 这一层使用数据层的数据作为输入，生成”conv1”层，具体产生20个通道的输出，卷积核大小为5，卷积步长为1。先后两个lr_mults是对本层可学习参数的速率调整，权重学习率与solver中的学习率相同，而偏置学习率为其两倍，这往往导致更好的手链率。权重初始化使用”xavier”方式，偏置初始化为0。第二个卷积层在池化层1后，输出到池化层2，参数除了输出个数（num_output）改为50，其余的相同。 池化层1234567891011layer &#123; name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125; 第一个池化层表示采用最大池化的方法，进行大小为2，步长为2的非重叠池化。第二个池化层与第一个完全相同，其输入为卷积层2，输出到全连接层1. 全连接层123456789101112131415161718192021layer &#123; name: "ip1" type: "InnerProduct" bottom: "pool2" top: "ip1" param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; inner_product_param &#123; num_output: 500 weight_filler &#123; type: "xavier" &#125; bias_filler &#123; type: "constant" &#125; &#125;&#125; 全连接层与卷积层的写法非常相似，ip1层产生500个输出。在激活层后，还有一个全连接层，用于最后的输出分类，因此有10个输出。 激活层123456layer &#123; name: "relu1" type: "ReLU" bottom: "ip1" top: "ip1"&#125; ReLU是一个元素操作，因此可以使用原地操作（in-place operations）用于节省空间。其实就是top与bottom的名字相同。当然其他的层不能使用重复的blob名称。 损失层最后是损失层：1234567layer &#123; name: "loss" type: "SoftmaxWithLoss" bottom: "ip2" bottom: "label" top: "loss"&#125; softmax_loss层同时实现softmax和多项对数损失（这可以节省时间并提高数值稳定性）。输入为预测的输出和label，并且没有输出（向后的输出）。它计算损失函数，并且反向传播相对于ip2的梯度。 Accuracy准确率层这一层是用于在测试中返回准确率使用的：12345678910layer &#123; name: "accuracy" type: "Accuracy" bottom: "ip2" bottom: "label" top: "accuracy" include &#123; phase: TEST &#125;&#125; 与loss相似，但要注明phase: TEST。 定义MNIST 求解器求解器文件路径为： $CAFFE_ROOT/examples/mnist/lenet_solver.prototxt: 12345678910111213141516171819202122232425# The train/test net protocol buffer definitionnet: "examples/mnist/lenet_train_test.prototxt"# test_iter specifies how many forward passes the test should carry out.# In the case of MNIST, we have test batch size 100 and 100 test iterations,# covering the full 10,000 testing images.test_iter: 100# Carry out testing every 500 training iterations.test_interval: 500# The base learning rate, momentum and the weight decay of the network.base_lr: 0.01momentum: 0.9weight_decay: 0.0005# The learning rate policylr_policy: "inv"gamma: 0.0001power: 0.75# Display every 100 iterationsdisplay: 100# The maximum number of iterationsmax_iter: 10000# snapshot intermediate resultssnapshot: 5000snapshot_prefix: "examples/mnist/lenet"# solver mode: CPU or GPUsolver_mode: GPU 这些参数见上一篇：caffe学习（8）Solver 配置详解。 训练并测试模型简单的话可以直接运行： cd $CAFFE_ROOT ./examples/mnist/train_lenet.sh 即运行： ./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt 也就是我们上面的求解器配置文件。首先出现的是我们打开的solver文件，之后打开网络模型：lenet_train_test.prototxt，初始化网络参数。 I1108 16:08:29.103813 46285 layer_factory.hpp:77] Creating layer mnist I1108 16:08:29.104310 46285 net.cpp:100] Creating Layer mnist I1108 16:08:29.104336 46285 net.cpp:408] mnist -&gt; data I1108 16:08:29.104374 46285 net.cpp:408] mnist -&gt; label I1108 16:08:29.107558 46328 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb 不过仔细看的话会发现初始化了两遍网络，其实是因为我们同时在训练和测试，这两个网络的区别就是测试有”accuracy”层，训练没有。这些信息告诉了层之间的连接、输入、输出关系。结束后正式开始训练： I1108 16:08:29.156116 46285 net.cpp:283] Network initialization done. I1108 16:08:29.156206 46285 solver.cpp:60] Solver scaffolding done. I1108 16:08:29.156466 46285 caffe.cpp:251] Starting Optimization I1108 16:08:29.156500 46285 solver.cpp:279] Solving LeNet I1108 16:08:29.156512 46285 solver.cpp:280] Learning Rate Policy: inv I1108 16:08:29.158172 46285 solver.cpp:337] Iteration 0, Testing net (#0) I1108 16:08:31.021287 46285 solver.cpp:404] Test net output #0: accuracy = 0.0933 I1108 16:08:31.021385 46285 solver.cpp:404] Test net output #1: loss = 2.36349 (* 1 = 2.36349 loss) 可以看到，初始化参数后测试模型，准确率有9.33%，比10%还低一些。基于参数设置，我们每迭代100次输出loss 信息，每迭代500次测试模型，输出accuracy 信息： I1108 16:08:46.974346 46285 solver.cpp:337] Iteration 500, Testing net (#0) I1108 16:08:48.808943 46285 solver.cpp:404] Test net output #0: accuracy = 0.9767 I1108 16:08:48.809048 46285 solver.cpp:404] Test net output #1: loss = 0.068445 (* 1 = 0.068445 loss) I1108 16:08:48.823623 46285 solver.cpp:228] Iteration 500, loss = 0.0609579 I1108 16:08:48.823714 46285 solver.cpp:244] Train net output #0: loss = 0.0609579 (* 1 = 0.0609579 loss) I1108 16:08:48.823740 46285 sgd_solver.cpp:106] Iteration 500, lr = 0.0192814 可以发现输出500次后准确率已经达到了97.67%。达到训练次数后（这里减少了训练次数），得到最终结果： I1108 16:09:04.727638 46285 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel I1108 16:09:04.754024 46285 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate I1108 16:09:04.770093 46285 solver.cpp:317] Iteration 1000, loss = 0.0819535 I1108 16:09:04.770177 46285 solver.cpp:337] Iteration 1000, Testing net (#0) I1108 16:09:06.607952 46285 solver.cpp:404] Test net output #0: accuracy = 0.9844 I1108 16:09:06.608042 46285 solver.cpp:404] Test net output #1: loss = 0.0491373 (* 1 = 0.0491373 loss) I1108 16:09:06.608055 46285 solver.cpp:322] Optimization Done. I1108 16:09:06.608064 46285 caffe.cpp:254] Optimization Done. 得到了两个文件：lenet_iter_1000.caffemodel和lenet_iter_1000.solverstate。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（8）Solver 配置详解]]></title>
      <url>%2F2016%2F11%2F08%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%888%EF%BC%89Solver%20%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[Solver是求解学习模型的核心配置文件，网络确定后，solver就决定了学习的效果。本文结合caffe.proto和网上资料，对solver配置进行学习。 SolverCaffe学习系列(7)：solver及其配置，denny402 Solver在caffe中的定义通常的solver文件与net文件相互关联，同样的net我们往往使用不同的solver尝试得到最好的效果，其运行代码为： caffe train --solver=*_slover.prototxt 关于solver的一切，都在caffe.proto文件中message SolverParameter 这一部分。 网络文件源12345678910// Proto filename for the train net, possibly combined with one or more// test nets.optional string net = 24;// Inline train net param, possibly combined with one or more test nets.optional NetParameter net_param = 25;optional string train_net = 1; // Proto filename for the train net.repeated string test_net = 2; // Proto filenames for the test nets.optional NetParameter train_net_param = 21; // Inline train net params.repeated NetParameter test_net_param = 22; // Inline test net params. 这是最开始的部分，需要说明net文件的位置。在这四个train_net_param, train_net, net_param, net字段中至少需要出现一个，当出现多个时，就会按着(1) test_net_param, (2) test_net, (3) net_param/net 的顺序依次求解。必须为每个test_net指定一个test_iter。还可以为每个test_net指定test_level和/或test_stage。注意的是：文件的路径要从caffe的根目录开始，其它的所有配置都是这样。可以看到这几行的标签序号并不是顺序的，也说明caffe在不断地修改，下一个可用的序号是41。 网络状态123456789// The states for the train/test nets. Must be unspecified or// specified once per net.//// By default, all states will have solver = true;// train_state will have phase = TRAIN,// and all test_state's will have phase = TEST.// Other defaults are set according to the NetState defaults.optional NetState train_state = 26;repeated NetState test_state = 27; 网络状态必须是未指定的或者只能在一个网络中指定一次。关于NetState，其定义为：123456789message NetState &#123; optional Phase phase = 1 [default = TEST]; optional int32 level = 2 [default = 0]; repeated string stage = 3;&#125;enum Phase &#123; TRAIN = 0; TEST = 1;&#125; 迭代器12// The number of iterations for each test net.repeated int32 test_iter = 3; 首先是test_iter，这需要与test layer中的batch_size结合起来理解。mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。因此test_iter设置为100。执行完一次全部数据，称之为一个epoch。123456// The number of iterations between two testing phases.optional int32 test_interval = 4 [default = 0];optional bool test_compute_loss = 19 [default = false];// If true, run an initial test pass before the first iteration,// ensuring memory availability and printing the starting value of the loss.optional bool test_initialization = 32 [default = true]; test_interval是指测试间隔，每训练test_interval次，进行一次测试。同时test_compute_loss可以选择是否计算loss。test_initialization是指在第一次迭代前，计算初始的loss以确保内存可用。 123456789optional float base_lr = 5; // The base learning rate// the number of iterations between displaying info. If display = 0, no info// will be displayed.optional int32 display = 6;// Display the loss averaged over the last average_loss iterationsoptional int32 average_loss = 33 [default = 1];optional int32 max_iter = 7; // the maximum number of iterations// accumulate gradients over `iter_size` x `batch_size` instancesoptional int32 iter_size = 36 [default = 1]; base_lr指基础的学习率；display是信息显示间隔，迭代一定次数显示一次信息。average_loss用于显示在上次average_loss迭代中的平均损失。max_iter是最大迭代次数，需要合适设置达到精度、震荡的平衡。iter_size是迭代器大小，梯度的计算是通过iter_size x batch_size决定的。 学习策略 123456789101112optional string lr_policy = 8;optional float gamma = 9; // The parameter to compute the learning rate.optional float power = 10; // The parameter to compute the learning rate.optional float momentum = 11; // The momentum value.optional float weight_decay = 12; // The weight decay.// regularization types supported: L1 and L2// controlled by weight_decayoptional string regularization_type = 29 [default = "L2"];// the stepsize for learning rate policy "step"optional int32 stepsize = 13;// the stepsize for learning rate policy "multistep"repeated int32 stepvalue = 34; 只要是梯度下降法来求解优化，都会有一个学习率，也叫步长。base_lr用于设置基础学习率，在迭代的过程中，可以对基础学习率进行调整。怎么样进行调整，就是调整的策略，由lr_policy来设置。caffe提供了多种policy： fixed: 总是返回base_lr（学习率不变） step: 返回 base_lr * gamma ^ (floor(iter / step))还需要设置stepsize参数以确定step，iter表示当前迭代次数。 exp: 返回base_lr * gamma ^ iter， iter为当前迭代次数 inv: 如果设置为inv,还需要设置一个power, 返回base_lr (1 + gamma iter) ^ (- power) multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据stepvalue值变化。 poly: 学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power) sigmoid: 学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))。 multistep示例：1234567891011base_lr: 0.01momentum: 0.9weight_decay: 0.0005# The learning rate policylr_policy: "multistep"gamma: 0.9stepvalue: 5000stepvalue: 7000stepvalue: 8000stepvalue: 9000stepvalue: 9500 之后有momentum，上次梯度更新的权重；weight_decay权重衰减，防止过拟合；regularization_type正则化方式。 clip_gradients1optional float clip_gradients = 35 [default = -1]; 参数梯度的实际L2范数较大时，将clip_gradients设置为&gt; = 0，以将参数梯度剪切到该L2范数。具体作用还不是很理解。 snapshot快照12345678910optional int32 snapshot = 14 [default = 0]; // The snapshot intervaloptional string snapshot_prefix = 15; // The prefix for the snapshot.// whether to snapshot diff in the results or not. Snapshotting diff will help// debugging but the final protocol buffer size will be much larger.optional bool snapshot_diff = 16 [default = false];enum SnapshotFormat &#123; HDF5 = 0; BINARYPROTO = 1;&#125;optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO]; 快照可以将训练出来的model和solver状态进行保存，snapshot用于设置训练多少次后进行保存，默认为0，不保存。snapshot_prefix设置保存路径。还可以设置snapshot_diff，是否保存梯度值，保存有利于调试，但需要较大空间存储，默认为false，不保存。也可以设置snapshot_format，保存的类型。有两种选择：HDF5 和BINARYPROTO ，默认为BINARYPROTO。 运行模式 1234567891011enum SolverMode &#123; CPU = 0; GPU = 1;&#125;optional SolverMode solver_mode = 17 [default = GPU];// the device_id will that be used in GPU mode. Use device_id = 0 in default.optional int32 device_id = 18 [default = 0];// If non-negative, the seed with which the Solver will initialize the Caffe// random number generator -- useful for reproducible results. Otherwise,// (and by default) initialize using a seed derived from the system clock.optional int64 random_seed = 20 [default = -1]; 设置CPU或GPU模式，在GPU下还可以指定使用哪一块GPU运行。random_seed用于初始生成随机数种子。 Solver类型 1234567891011// type of the solveroptional string type = 40 [default = "SGD"];// numerical stability for RMSProp, AdaGrad and AdaDelta and Adamoptional float delta = 31 [default = 1e-8];// parameters for the Adam solveroptional float momentum2 = 39 [default = 0.999];// RMSProp decay value// MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)optional float rms_decay = 38; type是solver的类型，目前有SGD、NESTEROV、ADAGRAD、RMSPROP、ADADELTA、ADAM = 5这六类。之后的一些是这些类型的特有参数，根据需要设置。 杂项 123456// If true, print information about the state of the net that may help with// debugging learning problems.optional bool debug_info = 23 [default = false];// If false, don't save a snapshot after training finishes.optional bool snapshot_after_train = 28 [default = true]; debug_info用于输出调试信息。snapshot_after_train用于训练后是否输出快照。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Google Protocol Buffer 学习]]></title>
      <url>%2F2016%2F11%2F07%2FGoogle%20Protocol%20Buffer%20%E5%AD%A6%E4%B9%A0%2F</url>
      <content type="text"><![CDATA[Caffe上有很多使用了Google Protocol Buffer的东西，从网上来看，这“是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，很适合做数据存储或 RPC 数据交换格式。它可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式”。作为caffe模型定义的数据格式，看懂caffe.proto对caffe的理解会有很大帮助。 Google ProtobufGoogle Protocol Buffer 的使用和原理，刘 明 小例子我们首先要在.proto文件中定义协议缓冲区消息类型（protocol buffer message types），来指定要序列化的信息的结构。下面是官网的一个小例子，定义了一个人的信息： message Person { required string name = 1; required int32 id = 2; optional string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { required string number = 1; optional PhoneType type = 2 [default = HOME]; } repeated PhoneNumber phone = 4; } 在 protobuf 的术语中，结构化数据被称为 Message，message中有不同成员。proto 文件非常类似 java 或者 C 语言的数据定义，string、int32这种类型我们已经见得多了。支持类型包括数字（整数或浮点）, 布尔值,，字符串，原始字节（raw bytes），或者是其他的message类型 (如上例) 。除了这些类型，其前后多了一些“修饰”。类型前是field rules，有可选（optional）、必填（required）、重复（repeated）三种。后面是message中的ID，同一message下ID随成员递增。 定义一个消息类型上面的例子其实已经定义了一个较为复杂的message。而对于一个完整的proto文件，在文件前还应该加上 syntax = &quot;proto2&quot;;//说明语法是2还是3 package caffe; //包名，通常与文件名相同 首先是field rules， Specifying Field Rules required：本字段一个massage必须只有一个。 optional：本字段可以有0个或1个。 repeated：本字段可以重复任何次，并保留顺序。 其中必填字段在使用中需要小心，特别是从必填改到可选时，读取时可能认为这个字段是不完整的。 Assigning Tags 定义消息中的每一个字段都有唯一的编号标签ID，用于消息二进制标识，并且在使用后不应该变。由于编码的原因，值在1到15范围的编号需要一个字节编码，包括标识号与字段类型。在16到2047范围的标签用两个字节。这对于数据储存大小有很大的联系，因此频繁出现的标签成员应该使编号尽可能小。其原因在于一种Varint的编码。其标签范围是1到$2^{29}-1$（536,870,911），同时除了中间的19000到19999，这是为协议缓冲区（Protocol Buffers）实现保留的。 Reserved Fields 保留字段，是对于被删除或者注释的字段进行保留， 如果以后加载相同.proto的旧版本，这可能会导致严重的问题。确保不会发生这种情况的一种方法是指定保留已删除字段的字段标签。协议缓冲区编译器将报告任何未来的用户是否尝试使用这些字段标识符。 message Foo { reserved 2, 15, 9 to 11; reserved &quot;foo&quot;, &quot;bar&quot;; } 不能在同一保留语句中混合字段名和标识号。 编译.proto文件使用写好的proto就可以用编译器将文件编译为目标语言了。在protobuf V3.0网站上可以下载 Protobuf V3.0的源代码，V2.6版本在网页上比较靠后，更新到2.6.1。然后解压编译安装便可以使用它了。从caffe文件上看用的还是2的语法。2和3的区别可以从网上搜到，如Google Protobuf 3版本介绍。其实变化也是不少，比如只保留repeated标记数组类型，optional和required都被去掉了；字段default标记不能使用了。 坑爹的是服务器没有权限装不了，于是只好在自家电脑上用windows版的。主要是用于验证， package lm; message helloworld { required int32 id = 1; // ID required string str = 2; // str optional int32 opt = 3; //optional field } 用命令行执行 protoc -I=. --cpp_out=. lm.helloworld.proto 得到了两个文件：lm.helloworld.pb.h ， 定义了 C++ 类的头文件lm.helloworld.pb.cc ， C++ 类的实现文件。有了这两个文件，之后我们想读写都可以用类操作实现了。 读写数据数据写到磁盘代码12345678910111213141516171819 #include "lm.helloworld.pb.h"… int main(void) &#123; lm::helloworld msg1; msg1.set_id(101); msg1.set_str(“hello”); // Write the new address book back to disk. fstream output("./log", ios::out | ios::trunc | ios::binary); if (!msg1.SerializeToOstream(&amp;output)) &#123; cerr &lt;&lt; "Failed to write msg." &lt;&lt; endl; return -1; &#125; return 0; &#125; 在代码中，其实重要的只是前三行，定义了helloworld类的对象，设置id的值，设置str的值。最后用SerializeToOstream输出到文件流。 读取数据代码 12345678910111213141516171819202122 #include "lm.helloworld.pb.h" … void ListMsg(const lm::helloworld &amp; msg) &#123; cout &lt;&lt; msg.id() &lt;&lt; endl; cout &lt;&lt; msg.str() &lt;&lt; endl; &#125; int main(int argc, char* argv[]) &#123; lm::helloworld msg1; &#123; fstream input("./log", ios::in | ios::binary); if (!msg1.ParseFromIstream(&amp;input)) &#123; cerr &lt;&lt; "Failed to parse address book." &lt;&lt; endl; return -1; &#125; &#125; ListMsg(msg1); … &#125; 在读取代码中，声明类 helloworld 的对象 msg1，然后利用 ParseFromIstream 从一个 fstream 流中读取信息并反序列化。此后，ListMsg 中采用 get 方法读取消息的内部信息，并进行打印输出操作。 分别运行后得到如下结果： &gt;writer &gt;reader 101 Hello 验证了程序。 Peotocol Buffer 编码在标签中说到了Varint，现在再结合编码讲一下，主要是参考了引用2的网页。Varint 中的每个 byte 的最高位 bit 有特殊的含义，如果该位为 1，表示后续的 byte 也是该数字的一部分，如果该位为 0，则结束。其他的 7 个 bit 都用来表示数字。因此小于 128 的数字都可以用一个 byte 表示。大于 128 的数字，比如 300，会用两个字节来表示：1010 1100 0000 0010。下图演示了 Google Protocol Buffer 如何解析两个 bytes。注意到最终计算前将两个 byte 的位置相互交换过一次，这是因为 Google Protocol Buffer 字节序采用 little-endian(小端在前) 的方式。消息经过序列化后会成为一个二进制数据流，该流中的数据为一系列的 Key-Value 对。如下图所示：采用这种 Key-Pair 结构无需使用分隔符来分割不同的 Field。对于可选的 Field，如果消息中不存在该 field，那么在最终的 Message Buffer 中就没有该 field，这些特性都有助于节约消息本身的大小。假如生成如下的消息： Test1.id = 10; Test1.str = “hello”； 则最终的 Message Buffer 中有两个 Key-Value 对，一个对应消息中的 id；另一个对应 str。Key 用来标识具体的 field，在解包的时候，Protocol Buffer 根据 Key 就可以知道相应的 Value 应该对应于消息中的哪一个 field。Key 的定义如下： (field_number &lt;&lt; 3) | wire_type //&lt;&lt;是左移运算 可以看到 Key 由两部分组成。第一部分是 field_number，比如消息 lm.helloworld 中 field id 的 field_number 为 1。第二部分为 wire_type。表示 Value 的传输类型。其中wire_type有如下几种：在我们的例子当中，field id 所采用的数据类型为 int32，因此对应的 wire type 为 0。细心的读者或许会看到在 Type 0 所能表示的数据类型中有 int32 和 sint32 这两个非常类似的数据类型。Google Protocol Buffer 区别它们的主要意图也是为了减少 encoding 后的字节数。在计算机内，一个负数一般会被表示为一个很大的整数，因为计算机定义负数的符号位为数字的最高位。如果采用 Varint 表示一个负数，那么一定需要 5 个 byte。为此 Google Protocol Buffer 定义了 sint32 这种类型，采用 zigzag 编码。Zigzag 编码用无符号数来表示有符号数字，正数和负数交错，这就是 zigzag 这个词的含义了。具体编码如图所示：使用 zigzag 编码，绝对值小的数字，无论正负都可以采用较少的 byte 来表示，充分利用了 Varint 这种技术。其他的数据类型，比如字符串等则采用类似数据库中的 varchar 的表示方法，即用一个 varint 表示长度，然后将其余部分紧跟在这个长度部分之后即可。总之，Protocol Buffer的编码确费尽心机，效果当然也不错，特别是与常用的XML相比，包括解包的速度。 了解了一下Google Protocol Buffer，算是一些课外知识了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（7）损失层、通用层]]></title>
      <url>%2F2016%2F11%2F07%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%887%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%B1%82%E3%80%81%E9%80%9A%E7%94%A8%E5%B1%82%2F</url>
      <content type="text"><![CDATA[Caffe LayersCaffe学习系列(5)：其它常用层及参数，denny402 损失层Loss Layers 损失通过将输出与目标进行比较，并不断优化减小loss。 Softmax（with loss） 层类型：SoftmaxWithLoss 示例： 1234567 layer &#123; name: "loss" type: "SoftmaxWithLoss" bottom: "ip1" bottom: "label" top: "loss"&#125; 在概念上等同于softmax layer+多项对数损失层（multinomial logistic loss layer），但提供了更稳定的梯度。softmax只是输出每一类的概率，并没有与label做比较。 Sum-of-Squares / Euclidean 层类型：EuclideanLoss这是比较传统的求偏差的方法，$\frac 1 {2N} \sum_{i=1}^N | x^1_i - x^2_i |_2^2$，直接计算欧氏距离。 Hinge / Margin 层类型：HingeLoss 参数(HingeLossParameter hinge_loss_param)： 可选 norm [default L1]:应该是正则化方法，目前只有L1、L2。 输入： n c h * w Predictions预测值 n 1 1 * 1 Labels标签 输出：1 1 1 * 1 Computed Loss 示例 12345678910111213141516171819# L1 Norm L1正则layer &#123; name: "loss" type: "HingeLoss" bottom: "pred" bottom: "label"&#125;# L2 Norm L2正则layer &#123; name: "loss" type: "HingeLoss" bottom: "pred" bottom: "label" top: "loss" hinge_loss_param &#123; norm: L2 &#125;&#125; Hinge loss主要用于SVM。 Accuracy 层类型：Accuracy 示例 12345678910layer &#123; name: "accuracy" type: "Accuracy" bottom: "ip2" bottom: "label" top: "accuracy" include &#123; phase: TEST &#125;&#125; 只有test阶段才有，因此需要加入include参数。它实际上不是损失并且没有后退步骤。 通用层Common Layers Inner Product 层类型：InnerProduct 参数 (InnerProductParameter inner_product_param)： 必须参数 num_output (c_o):滤波器数量。 推荐参数 weight_filler [default type: ‘constant’ value: 0]：全职初始化方式、值。还可以选择”xavier”算法来进行初始化，也可以设置为”gaussian”。 可选参数 bias_filler [default type: ‘constant’ value: 0]：偏置初始化。 bias_term [default true]: 是否启用偏置项。 输入：n c_i h_i * w_i 输出：n c_o 1 * 1 示例： 123456789101112131415161718192021layer &#123; name: "fc8" type: "InnerProduct" # learning rate and decay multipliers for the weights param &#123; lr_mult: 1 decay_mult: 1 &#125; # learning rate and decay multipliers for the biases param &#123; lr_mult: 2 decay_mult: 0 &#125; inner_product_param &#123; num_output: 1000 weight_filler &#123; type: "gaussian" std: 0.01 &#125; bias_filler &#123; type: "constant" value: 0 &#125; &#125; bottom: "fc7" top: "fc8"&#125; Reshape 层类型：Reshape 参数 (ReshapeParameter reshape_param)： 可选参数： shape 输入：单独的blob 输出：变形后的blob 示例： 1234567891011121314layer &#123; name: "reshape" type: "Reshape" bottom: "input" top: "output" reshape_param &#123; shape &#123; dim: 0 # copy the dimension from below dim: 2 dim: 3 dim: -1 # infer it from the other dimensions &#125; &#125; &#125; 这一操作不改变数据，只改变维度，也没有在过程中拷贝数据。输出的尺寸有shape参数的值规定，正数是对应的维度，除此外还有两个特殊值： 0表示复制底层对应的维度。bottom第一维度值为2，top第一维度也是2。 -1表示从其他维度推断。为了保证数据总数不变，可以根据其他维数值计算。 特别的，当时用参数：reshape_param { shape { dim: 0 dim: -1 } }时，reshape层相当于flatten层，将n c h w的数据变为n (chw)。 Concatenation 层类型： Concat 参数 (ConcatParameter concat_param)： 可选参数 axis [default 1]: 0表示沿着数量（n），1表示沿着通道（C）。 输入：n_i c_i h * w 对于每个blob输入，i= 1 到 K。 输出： 当 axis = 0: (n_1 + n_2 + … + n_K) c_1 h * w, 所有的c_i应该相同。 当 axis = 1: n_1 (c_1 + c_2 + … + c_K) h * w, 所有的n_i 应该相同。 这个层把多个blob连接为一个blob。 层的学习暂时到这里。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（6）激活层]]></title>
      <url>%2F2016%2F11%2F07%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%886%EF%BC%89%E6%BF%80%E6%B4%BB%E5%B1%82%2F</url>
      <content type="text"><![CDATA[激活（Activation）层又叫神经元（Neuron）层，最主要的是激活函数的设置。 Activation / Neuron LayersCaffe源码解析6：Neuron_Layer，楼燚航的blog 一般来说，这一层是元素级的运算符，从底部blob作为输入并产生一个相同大小的顶部blob： 输入：n c h * w 输出：n c h * w ReLU / Rectified-Linear and Leaky-ReLU 层类型：ReLU 参数(ReLUParameter relu_param)： 可选参数 negative_slope [default 0]: 用来指定负斜率部分的因子$\nu$。完整的函数表达式为：$y = \max(0, x) + \nu \min(0, x)$。反向传播的公式为$$\frac{\partial E}{\partial x} = \left{\begin{array}{lr}\nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0\end{array} \right.$$ 示例（./models/bvlc_reference_caffenet/train_val.prototxt）： 123456789101112131415161718192021layer &#123; name: "relu1" type: "ReLU" bottom: "conv1" top: "conv1"&#125; ``` 支持in-place计算，bottom输入和top输出可以相同避免内存消耗。 **Sigmoid** - 层类型：Sigmoid - 示例( ./models/bvlc_reference_caffenet/train_val.prototxt)： ```pythonlayer &#123; name: "relu1" type: "ReLU" bottom: "conv1" top: "conv1"&#125; 激活函数表达式为$y = (1 + \exp(-x))^{-1}$，由于收敛速度问题现在用的不多了。 TanH、AbsVal、BNLL 层类型：TanH、AbsVal、BNLL 示例： 123456 layer &#123; name: "layer" bottom: "in" top: "out" type: "TanH"#"AbsVal"、“BNLL”官网上BNLL没有加双引号，应该是有误&#125; 分别是双曲正切函数、绝对值、binomial normal log likelihood（$f(x)=log(1 + e^x)$）的简称。 Power 层类型：Power 参数 (PowerParameter power_param)： 可选 power [default 1] scale [default 1] shift [default 0] 示例： 1234567891011 layer &#123; name: "layer" bottom: "in" top: "out" type: "Power" power_param &#123; power: 2 scale: 1 shift: 0 &#125;&#125; 幂运算函数为$f(x)= (shift + scale * x) ^ p$。 Caffe中的激活层还有很多，也有一些是加速的层。比如DropoutLayer现在是非常常用的一种网络层，只用在训练阶段，一般用在网络的全连接层中，可以减少网络的过拟合问题。具体的使用再具体看./src/caffe/layers/下的文件吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（5）视觉层]]></title>
      <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89%E8%A7%86%E8%A7%89%E5%B1%82%2F</url>
      <content type="text"><![CDATA[上一篇是数据层，这一篇是视觉层（Vision Layers）。参考官网和网友博客。 Vision LayersCaffe学习系列(3)：视觉层（Vision Layers)及参数，denny402Caffe源码解析5：Conv_Layer，楼燚航的blog 视觉层通常将图像作为输入，产生其他图像作为输出。图像输入可以是灰度图（通道C=1），RGB图（通道C=3）。同样图像也具有二维的空间结构，其高度$h&gt;1$宽度$w&gt;1$。大多数视觉层通过对输入区域应用特定操作产生输出的相应区域，这里就有点像传统的数字图像处理的工作了。相比之下其他层常常忽略输入的空间结构，视其为具有$chw$维度的大向量。 卷积层Convolution 层类型：Convolution CPU实现：./src/caffe/layers/convolution_layer.cpp CUDA GPU实现： ./src/caffe/layers/convolution_layer.cu 参数 (ConvolutionParameter convolution_param)： 必须参数 num_output (c_o):卷积核（filter）的个数。 kernel_size (or kernel_h and kernel_w): 卷积核大小，非方阵用_h _w。 推荐参数 weight_filler [default type: ‘constant’ value: 0]：卷积核的初始化，默认为全0，可以用”xavier”算法来进行初始化，也可以设置为”gaussian”。 可选参数 bias_term [default true]:是否开启偏置项，默认为true, 开启。 pad (or pad_h and pad_w) [default 0]: 填零操作，默认为0，不填零。是对原图进行填零，使卷积核在图像边缘能够进行卷积操作，运算后和原图的尺寸相同。扩充的时候是左右、上下对称的，比如卷积核的大小为5*5，那么pad设置为2，则四个边缘都扩充2个像素，即宽度和高度都扩充了4个像素。 stride (or stride_h and stride_w) [default 1]: 卷积核的移动步长，默认为1。 group (g) [default 1]: 分组，默认为1组。如果大于1，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第i个输出分组只能与第i个输入分组进行连接。groups是代表filter 组的个数。引入gruop主要是为了选择性的连接卷基层的输入端和输出端的channels，否则参数会太多。 It was there to implement the grouped convolution in Alex Krizhevsky’s paper: when group=2, the first half of the filters are only connected to the first half of the input channels, and the second half only connected to the second half. 当group=2时，前半部分filter与输入的前半部分通道连接，后半部分filter与后半部分输入通道连接。 输入：n c_i h_i * w_i 输出：n c_o h_o w_o, where h_o = (h_i + 2 pad_h - kernel_h) / stride_h + 1 and w_o likewise。 示例 (./models/bvlc_reference_caffenet/train_val.prototxt) 1234567891011121314151617181920212223242526272829303132layer &#123; name: "conv1" type: "Convolution" bottom: "data" top: "conv1" # learning rate and decay multipliers for the # filters param &#123; lr_mult: 1 decay_mult: 1 &#125; # learning rate and decay multipliers for the biases param &#123; lr_mult: 2 decay_mult: 0 &#125; convolution_param &#123; num_output: 96 # learn 96 filters kernel_size: 11 # each filter is 11x11 stride: 4 # step 4 pixels between each filter # application weight_filler &#123; type: "gaussian" # initialize the filters from a Gaussian std: 0.01 # distribution with stdev 0.01 (default # mean: 0) &#125; bias_filler &#123; type: "constant" # initialize the biases to zero (0) value: 0 &#125; &#125;&#125; 卷积层将输入图像与一组可学习的滤波器进行卷积，每个在输出图像中产生一个特征图。 池化层Pooling Pooling 层一般在网络中是跟在Conv卷积层之后，做采样操作，其实是为了进一步缩小feature map，同时也能增大神经元的视野。 层类型：Pooling CPU实现：./src/caffe/layers/pooling_layer.cpp CUDA GPU实现：./src/caffe/layers/pooling_layer.cu 参数(PoolingParameter pooling_param)： 必须 kernel_size (or kernel_h and kernel_w)：池化核大小。 可选参数 pool [default MAX]: 池化方法，默认为MAX，还有 AVE, or STOCHASTIC。 pad (or pad_h and pad_w) [default 0]:填零。 stride (or stride_h and stride_w) [default 1]:步长。 输入：n c h_i * w_i 输出：n c h_o * w_o，h_o and w_o 与卷积层计算方法相同。 示例 ( ./models/bvlc_reference_caffenet/train_val.prototxt) 12345678910111213layer &#123; name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param &#123; pool: MAX kernel_size: 3 # pool over a 3x3 region stride: 2 # step two pixels (in the # bottom blob) between pooling # regions &#125;&#125; 局部响应归一化层RNL 局部响应归一化层通过对局部输入区域进行归一化来执行一种“横向抑制”。具体作用感觉和特征缩放有点像，使梯度下降在所有方向上具有相同的曲率。而RNL这种方法的计算相比对每个神经元输入归一化要简单。 层类型：LRN CPU实现： ./src/caffe/layers/lrn_layer.cpp CUDA GPU实现：./src/caffe/layers/lrn_layer.cu 参数 (LRNParameter lrn_param)： 可选参数 local_size [default 5]:需要求和的通道数数目（对于跨通道LRN），或者是方形区域求和的变长（对于通道内LRN）。 alpha [default 1]: 比例参数。 beta [default 5]: 指数参数。 norm_region [default ACROSS_CHANNELS]:ACROSS_CHANNELS表示在相邻的通道间求和归一化，但没有空间延伸，即大小为local_size x 1 x 1；WITHIN_CHANNEL表示在一个通道内部特定的区域内进行求和归一化，其大小为：1 x local_size x local_size。每个输入值被除以$(1 + (\alpha/n) \sum_i x_i^2)^\beta$，$n$是每个局部区域的大小。 示例： 1234567891011layers &#123; name: "norm1" type: LRN bottom: "pool1" top: "norm1" lrn_param &#123; local_size: 5 alpha: 0.0001 beta: 0.75 &#125;&#125; Im2col Caffe中卷积操作需要先对数据进行im2col，再进行内积运算，如下图所示：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（4）数据层]]></title>
      <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89%E6%95%B0%E6%8D%AE%E5%B1%82%2F</url>
      <content type="text"><![CDATA[数据是学习的原料，参考官网和网友的资料，来看一下数据与数据层。 Data：Ins and OutsCaffe学习系列(2)：数据层及参数，denny402 数据：输入与输出 在Caffe中，数据是以Blobs流动的（见caffe学习（1）caffe模型三种结构）。数据层的输入输出便需要由其他格式与Blobs进行相互转换。一些常见的变换如平均减法（mean-subtraction）、特征缩放是通过data layer配置完成。新的输入类型需要开发新的数据层，网络的其余部分遵循Caffe层目录的模块化。下段加载了MNIST数据：1234567891011121314151617181920212223layer &#123; name: "mnist" # Data layer loads leveldb or lmdb storage DBs for high-throughput.加载leveldb 或 lmdb类型的数据实现高吞吐量 type: "Data" # the 1st top is the data itself: the name is only convention top: "data" # the 2nd top is the ground truth: the name is only convention top: "label" # the Data layer configuration data_param &#123; # path to the DB source: "examples/mnist/mnist_train_lmdb" # type of DB: LEVELDB or LMDB (LMDB supports concurrent reads) backend: LMDB # batch processing improves efficiency. batch_size: 64 &#125; # common data transformations transform_param &#123; # feature scaling coefficient: this maps the [0, 255] MNIST data to [0, 1] scale: 0.00390625 &#125;&#125; name: 表示该层的名称，可随意取，本层为”mnist”。 type: 层类型，如果是Data，表示数据来源于LevelDB或LMDB。根据数据的来源不同，数据层的类型也不同（后面会详细阐述）。一般在练习的时候，我们都是采用的LevelDB或LMDB数据，因此层类型设置为Data。 top或bottom: 每一层用bottom来输入数据，用top来输出数据。如果只有top没有bottom，则此层只有输出，没有输入。反之亦然。如果有多个 top或多个bottom，表示有多个blobs数据的输入和输出。 data 与 label: 在数据层中，至少有一个命名为data的top。如果有第二个top，一般命名为label。 这种(data,label)配对是分类模型所必需的。本例中第一个top是数据本身，第二个top是label（ground truth）（这些名字只是约定的）。 include: 一般训练的时候和测试的时候，模型的层是不一样的。该层（layer）是属于训练阶段的层，还是属于测试阶段的层，需要用include来指定。如果没有include参数，则表示该层既在训练模型中，又在测试模型中。（上例中没有出现） 123include &#123; phase: TRAIN #仅在训练中出现 &#125; Transformations: 数据的预处理，可以将数据变换到定义的范围内。如设置scale为0.00390625，实际上就是1/255, 即将输入数据由0-255归一化到0-1之间。除了缩放，还有其他的一些预处理操作： 12345678transform_param &#123; scale: 0.00390625 mean_file_size: "examples/cifar10/mean.binaryproto" # 用一个配置文件来进行均值操作 mirror: 1 # 1表示开启镜像，0表示关闭，也可用ture和false来表示 # 剪裁一个 227*227的图块，在训练阶段随机剪裁（random cropping），在测试阶段从中间裁剪 crop_size: 227 &#125; prefetching：预取，对于吞吐量数据层获取下一批数据，并在Net计算当前批处理时在后台准备。 具体的还需要分析data_param，data_param部分，就是根据数据的来源不同，来进行不同的设置。 数据来自于数据库（如LevelDB和LMDB）层类型（layer type）:Data 必须设置的参数：source: 包含数据库的目录名称batch_size: 每次处理的数据个数，如64 可选的参数：rand_skip: 在开始的时候，跳过一定数量的数据输入，通常对异步的SGD很有用（useful for asynchronous sgd）。backend: 选择是采用LevelDB还是LMDB, 默认是LevelDB.示例： 123data_param &#123; source: "examples/mnist/mnist_train_lmdb" batch_size: 64&#125; 数据来自于内存层类型：MemoryData 必须设置的参数：batch_size：每一次处理的数据个数，比如2channels：通道数height：高度width: 宽度即指定要从内存中读取的输入块的大小。存储器数据层直接从存储器读取数据，而不复制它。为了使用它，必须调用MemoryDataLayer :: Reset（C ++）或Net.set_input_arrays（Python），以便指定一个连续数据源（作为4D行主数组），一次读取一个批处理大小的块。示例： 1234567891011121314151617 layer &#123; top: "data" top: "label" name: "memory_data" type: "MemoryData" memory_data_param&#123; batch_size: 2 height: 100 width: 100 channels: 1 &#125; transform_param &#123; scale: 0.0078125 mean_file: "mean.proto" mirror: false &#125;&#125; 数据来自于HDF5(Input)层类型：HDF5Data 必须设置的参数：source: 读取的文件名称batch_size: 每一次处理的数据个数示例： 12345678910layer &#123; name: "data" type: "HDF5Data" top: "data" top: "label" hdf5_data_param &#123; source: "examples/hdf5_classification/data/train.txt" batch_size: 10 &#125;&#125; 数据输出到HDF5(Output)层类型：HDF5Data 必须设置的参数：file_name: 输出到的文件名称HDF5输出层执行与本节中其他层相反的功能：它将其输出blob写入磁盘。 数据来自于图片层类型：ImageData 必须设置的参数：source: 一个文本文件的名字，每一行给定一个图片文件的名称和标签（label）batch_size: 每一次处理的数据个数，即图片数 可选参数：rand_skip: 在开始的时候，跳过一定的数据输入。通常对异步的SGD很有用。shuffle: 随机打乱顺序，默认值为falsenew_height,new_width: 如果设置，则将图片进行resize示例：1234567891011121314151617layer &#123; name: "data" type: "ImageData" top: "data" top: "label" transform_param &#123; mirror: false crop_size: 227 mean_file: "data/ilsvrc12/imagenet_mean.binaryproto" &#125; image_data_param &#123; source: "examples/_temp/file_list.txt" batch_size: 50 new_height: 256 new_width: 256 &#125;&#125; 数据来源于Windows层类型：WindowData 必须设置的参数：source: 一个文本文件的名字batch_size: 每一次处理的数据个数，即图片数示例：1234567891011121314151617181920212223 layer &#123; name: "data" type: "WindowData" top: "data" top: "label" include &#123; phase: TRAIN &#125; transform_param &#123; mirror: true crop_size: 227 mean_file: "data/ilsvrc12/imagenet_mean.binaryproto" &#125; window_data_param &#123; source: "examples/finetune_pascal_detection/window_file_2007_trainval.txt" batch_size: 128 fg_threshold: 0.5 bg_threshold: 0.5 fg_fraction: 0.25 context_pad: 16 crop_mode: "warp" &#125;&#125; DummyDummyData用于调试，详见DummyDataParameter。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（3）接口]]></title>
      <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89%E6%8E%A5%E5%8F%A3%2F</url>
      <content type="text"><![CDATA[接口Interfaces Interfaces Caffe提供丰富的接口，比如命令行，python，matlab。先说一下命令行 命令行 caffe命令及其参数解析，Single、Dog Caffe的程序位于caffe / build / tools，运行时可以在根目录执行./build/tools/caffe &lt;command&gt;&lt;args&gt;。其中&lt;command&gt;有四种： train：训练或finetune模型(model) test：测试模型 device_query：显示gpu信息 time：显示程序执行时间 其中的&lt;args&gt;参数有： -solver -gpu -snapshot -weights -model -sighup_effect -sigint_effect 训练traincaffe train可以从头开始学习模型、从已保存的快照中恢复学习或添加新数据进行fine-tunes。具体来说，所有训练需要通过-solver solver.prototxt参数进行求解器配置；恢复需要使用-snapshot model_iter_1000.solverstate参数来加载求解程序快照；fine-tunes微调需要模型初始化的-weights model.caffemodel参数。 12345678# train LeNet 训练LeNetcaffe train -solver examples/mnist/lenet_solver.prototxt# train on GPU 2 在特定的GPU上caffe train -solver examples/mnist/lenet_solver.prototxt -gpu 2# resume training from the half-way point snapshot 从快照恢复caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate# fine-tune CaffeNet model weights for style recognition 完整例子参阅examples/finetuning_on_flickr_style，仅调用可使用：caffe train -solver examples/finetuning_on_flickr_style/solver.prototxt -weights models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel 对于train的参数，功能为： -solver：必选，后跟一个protocol buffer类型(.prototxt)的文件，即模型的配置文件。 -gpu：可选，指定某一块GPU运行，-gpu all是所有运行： 1234# train on GPUs 0 &amp; 1 (doubling the batch size)caffe train -solver examples/mnist/lenet_solver.prototxt -gpu 0,1# train on all GPUs (multiplying batch size by number of devices)caffe train -solver examples/mnist/lenet_solver.prototxt -gpu all -snapshot：可选，从快照中恢复，设置快照可从solver配置中进行，保存为solverstate。 -weights：可选参数。用预先训练好的权重来fine-tuning模型，需要一个caffemodel，不能和-snapshot同时使用。 -iterations： 可选参数，迭代次数，默认为50。 如果在配置文件文件中没有设定迭代次数，则默认迭代50次。 -model：可选参数，定义在protocol buffer文件中的模型。也可以在solver配置文件中指定。 -sighup_effect：可选参数。用来设定当程序发生挂起事件时，执行的操作，可以设置为snapshot, stop或none, 默认为snapshot。 -sigint_effect： 可选参数。用来设定当程序发生键盘中止事件时（ctrl+c), 执行的操作，可以设置为snapshot, stop或none, 默认为stop。测试test测试时输出每个batch得分，最后返回平均值。test参数用在测试阶段，用于最终结果的输出，要模型配置文件中我们可以设定需要输入accuracy还是loss. 假设我们要在验证集中验证已经训练好的模型，就可以这样写123# score the learned LeNet model on the validation set as defined in the# model architeture lenet_train_test.prototxtcaffe test -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 100 意思是利用训练好了的权重（-weight)，输入到测试模型中(-model)，用编号为0的gpu(-gpu)测试100次(-iteration)。 时间timetime参数用来在屏幕上显示程序运行时间。如：123# (These example calls require you complete the LeNet / MNIST example first.)# time LeNet training on CPU for 10 iterationscaffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10 这个例子用来在屏幕上显示lenet模型迭代10次所使用的时间。包括每次迭代的forward和backward所用的时间，也包括每层forward和backward所用的平均时间。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（2）前后传播，loss，solver]]></title>
      <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%E5%89%8D%E5%90%8E%E4%BC%A0%E6%92%AD%EF%BC%8Closs%EF%BC%8Csolver%2F</url>
      <content type="text"><![CDATA[向前和向后传播 Forward and Backward 前后传播是Net的重要组成，如下图所示： 向前Forward通过给定的参数计算每层的值，就像函数一样top=f(bottom)。上图表示数据通过内积层输出，再由softmax给出损失。 向后Backward向后是计算loss的梯度，每层梯度通过自动微分来计算整个模型梯度，即反向传播。从这个图上可以看出，由loss开始，通过链式法则，不断求出结果对各层的导数。Net::Forward()和Net::Backward()是针对网络，Layer::Forward()和Layer::Backward()是针对每一层。同时也可以设置CPU、GPU模式。过程大概是：solver调用forward计算输出和loss，再生成梯度，并尝试更新权重减小loss。 损失Loss Loss Loss使loss变小，是学习中的一个目标。如上所说，loss是由forward计算而出。1234567layer &#123; name: "loss" type: "SoftmaxWithLoss" bottom: "pred" bottom: "label" top: "loss"&#125; 这一段就是上面流程图最后一层loss的表达。 Loss weights一般的loss只是最后一层才有，其他层只是中间计算，不过每层都可以通过增加loss_weight: &lt;float&gt;到该层生成的每个顶（top）层中。对于后缀有loss的层都隐含着loss_weight: 1（对第一个top，其他的loss_weight: 0）。因此上面代码也等价于在最后加上loss_weight: 1。然而任何能反向传播的层都可以赋予非零loss_weight，最终的loss由网络上各层loss权重求得1234loss := 0for layer in layers: for top, loss_weight in layer.tops, layer.loss_weights: loss += loss_weight * sum(top) Solver Solver 分类Solver通过forward和backward形成参数更新，从而改善loss。包括： Stochastic Gradient Descent (type: “SGD”), AdaDelta (type: “AdaDelta”), Adaptive Gradient (type: “AdaGrad”), Adam (type: “Adam”), Nesterov’s Accelerated Gradient (type: “Nesterov”) and RMSprop (type: “RMSProp”) 方法对于数据集$D$，优化目标是使整个$|D|$的平均loss最小，即：$$L(W) = \frac{1}{|D|} \sum_i^{|D|} f_W\left(X^{(i)}\right) + \lambda r(W)$$其中$f_W\left(X^{(i)}\right)$是对于数据$X^{(i)}$输入的损失，$r(W)$是加权$\lambda$的权重$W$正则项。通常用于学习的数据量$|D|$很大，在学习时常常将其分为很多大小为$N$的batch，其中$N&lt;&lt;|D|$，可以将原式中的$|D|$换为$N$。$$L(W) \approx \frac{1}{N} \sum_i^N f_W\left(X^{(i)}\right) + \lambda r(W)$$模型向前计算$f_W$，向后返回梯度$\nabla f_W$。参数的更新$\Delta W$由solver从$\nabla f_W$得到，正则化梯度每种方法得到的不同。具体每种solver，网上讲的很多，这里就不讲了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[caffe学习（1）caffe模型三种结构]]></title>
      <url>%2F2016%2F11%2F05%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89caffe%E6%A8%A1%E5%9E%8B%E4%B8%89%E7%A7%8D%E7%BB%93%E6%9E%84%2F</url>
      <content type="text"><![CDATA[caffe模型三种结构 自己写的然而CSDN出bug了，绑定三方账户原来的博客无法编辑，只好转发过来 Blobs, Layers, and Nets: anatomy of a Caffe modelBlob：存储和传递（communication）blob是数据存储和传输的包装，并且还在底层提供CPU和GPU之间的同步能力。Blob提供了保存数据的统一存储器接口； 例如图像批次，模型参数和用于优化的导数。 在数学上，blob是以C连续方式（C-contiguous fashion）存储的N维数组。 关于C连续方式，stackoverflow有一个解释。该方式主要与Fortran和Matlab相反，是一种以行为主顺序（Row-major order）的存储方式，简单的说就是把一行存完，再存下一行，把第一个通道（channel）的所有行写完后写完再写下一个通道。例如对一批（batches）图像，用4维blob存储，表示为number N（数据批量大小） x channel K（通道、维度特征） x height H （高）x width W（宽），对于索引 (n, k, h, w) 的物理地址就是：((n K + k) H + h) W + w，注意区分大小写，大写是总的，小写是索引值。对于图像是4D的，当然也可以不用4D。具体参数需要根据层类型和数据大小配置。 blob使用了两块存储区域，为data（数据）和diff（网络梯度），实际值可以存储在CPU或GPU上，访问也可以不变（const）访问或者可变（mutable）访问。 const Dtype cpu_data() const; Dtype* mutable_cpu_data(); 同理可得GPU上diff类型数据操作。官网上有一个example，展示了数据在CPU和GPU上流动操作。Layer计算和连接Layer包括很多计算方法，如Vision Layers：Convolution、Pooling、LRNLoss Layers：Softmax、Sum-of-Squares既然作为计算，就有输入输出，输入从底部（bottom）获取，并通过顶部（top）连接输出。每层须有三个关键计算：setup, forward, and backward。setup：初始化层和连接。forward：底部向顶部计算。backward：给定梯度，从top计算传回bottom。A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.（是说存在layer中吗）forward和backward也分为CPU和GPU两个版本。 If you do not implement a GPU version, the layer will fall back to the CPU functions as a backup option. This may come handy if you would like to do quick experiments, although it may come with additional data transfer cost 这里好像是说使用GPU会因为数据需要从CPU复制到GPU上，因此会有数据传输成本，但GPU跑的还是快一些，所以是quick experiments。Net定义和操作Net由Layer组成（The net is a set of layers connected in a computation graph有向无环图）。模型初始化由Net :: Init（）处理：主要是创建blob和layer，并调用layer里的setup，同时会输出INFO。模型格式模型定义在.prototxt文件中，训练好的模型在model目录下.binaryproto格式的文件中，模型的格式由caffe.proto定义。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe运行MNIST | example]]></title>
      <url>%2F2016%2F10%2F09%2Fcaffe-e8-bf-90-e8-a1-8cmnist-example%2F</url>
      <content type="text"><![CDATA[参考教程测试一下caffe。 1.获取数据包。 cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh 官网上是这样的，caffe_root是根目录，看网上有人说必须在根目录下运行，否则会出错，具体没有验证。。但是windows平台下应该是没有wget的，需要自己下载一下。得到四个文件，测试与训练，样本与标签。 第二句是个坑，直接执行的话会提示找不到convert_mnist_data.bin（好像是这个）。这个环境还是linux下的，windows下编译出来的是exe，和caffe在一起。用法是 convert_mnist_data [FLAGS] input_image_file input_label_file output_db_file执行两次将mnist date转化为可用的lmdb格式的文件。并将新生成的2个文件mnist-train-lmdb 和 mnist-test-lmdb放于create_mnist.sh同目录下。 2.测试 参数文件用的是 ./build/tools/caffe train –solver=examples/mnist/lenet_solver.prototxt mnist_test_lmdb mnist_train_lmdb 两个文件夹需要放在\examples\mnist。如果像我没用GPU，还需要在.prototxt里面更改solver_mode为 CPU。 感觉跑了一个半小时吧，终于跑完了。生成了四个文件……数据都训练好之后，接下来就是如何将模型应用到实际数据了（记录的博客）： ./build/tools/caffe.bin test -model=examples/mnist/lenet_train_test.prototxt -weights=examples/mnist/lenet_iter_10000.caffemodel -gpu=0 如果没有GPU则使用 ./build/tools/caffe.bin test -model=examples/mnist/lenet_train_test.prototxt -weights=examples/mnist/lenet_iter_10000.caffemodel看起来准确率很高。 Training LeNet on MNIST with Caffe]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe | Installation caffe 安装]]></title>
      <url>%2F2016%2F10%2F05%2Fcaffe-installation-caffe-e5-ae-89-e8-a3-85%2F</url>
      <content type="text"><![CDATA[尝试下caffe在windows的安装。 本本用的还是A卡，干脆禁掉CUDA和cuDNN吧，换新的重新安装再配置。 &lt;CpuOnlyBuild&gt;true&lt;/CpuOnlyBuild&gt;&lt;UseCuDNN&gt;false&lt;/UseCuDNN&gt;&lt;PythonSupport&gt;true&lt;/PythonSupport&gt;之后是python，之前装过又删掉了，这次再装一遍吧。windows下推荐的是Miniconda环境，里面集成了python2.7。运行 conda install --yes numpy scipy matplotlib scikit-image pip pip install protobuf 是在Miniconda2文件夹中cmd里面运行的，会自动下载安装一些库，本机下载了250M左右，第一次下载到第二个就卡住了QAQ，然后关了之后再输说之前存在任务触发保护了需要解锁，输入以下命令解锁。 conda clean –lock虚拟环境创建操作参考了conda简单使用，一开始一直在python里面输命令，好蠢= =miniconda只是一个方便配置python的虚拟环境，里面python可以根据需要再加自己要的库。这个步骤应该是在前面的库安装完了之后再进行，否则新安装的库好像不会在老的环境里出现。删除原有环境： conda remove –name 此处是环境名字 –all其他没有怎么设置了，直接用vs2013打开编译…… 但是感觉过了一年，卡在了这里： nuget好像也是一个管理库的插件，会自动下载各种东西，然而下载速度简直无语。。感觉过了几个小时下载好了，多了900MB东西。然后build……又是一堆错误。 首先是libcaffe.lib无法生成，其中两个小问题，找不到layer_factory.h和pyconfig.h，分别在caffe目录中和python目录中。之后找不到python27.lib，也在python中libs里，但是还会有一堆warning，不管了。。 最后终于都build完成，Debug版的出来1.8G，release版的提示nuget超时，不太清楚原因。python部分把生成出来的拷到python目录\lib\site-packages，import一下应该就可以了。 测试的话用的生成出来的test_all.exe，一开始跑得还快，后面的单项都能到40多秒，果然学习不是轻松的事情= =出现一些错误，好像是找不到测试图片，所以程序上应该没有错了。 哎，编译不易，还须多珍惜。 来源： Caffe | Installation]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[出发|关服]]></title>
      <url>%2F2016%2F09%2F21%2Fe5-87-ba-e5-8f-91-e5-85-b3-e6-9c-8d%2F</url>
      <content type="text"><![CDATA[明天出发，跑个三角形，树莓派可以休息啦。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机会还是会留给有准备的人啊~]]></title>
      <url>%2F2016%2F09%2F19%2Fe6-9c-ba-e4-bc-9a-e8-bf-98-e6-98-af-e4-bc-9a-e7-95-99-e7-bb-99-e6-9c-89-e5-87-86-e5-a4-87-e7-9a-84-e4-ba-ba-e5-95-8a%2F</url>
      <content type="text"><![CDATA[这段时间这么盲目、慌张，也好久没有这样的感觉了，可能习惯了按部就班的生活…… 简单说学到了几点： 1.计划可能真的赶不上变化，事情没有发生就不能确定，也要留后手。 2.多准备，不能把鸡蛋装一个篮子里。 3.要有一定的紧张感，居安思危。 &nbsp; PS：树莓派工作了好几天没休息，不知道有没有事。。 过几天如果出远门就关了]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[从flash到手游]]></title>
      <url>%2F2016%2F09%2F13%2Fe4-bb-8eflash-e5-88-b0-e6-89-8b-e6-b8-b8%2F</url>
      <content type="text"><![CDATA[近日室友又重新怀旧，玩起了金庸群侠传这个flash游戏。老实说我对这个游戏印象并不是很深，因为一没读过金庸，二来游戏确实很复杂，系统、任务众多，不是简单的休闲游戏。在我看来，这样一个出色的rpg游戏，可以算是flash游戏的巅峰了。如果是一个pc游戏，可能反而没有这么大的影响力。 03、04年，大概是小学三四年级的时候，电脑已经比较普遍了，特别是在我们小学已经开始上微机课了。更重要的一点是宽带取代了拨号上网，再也没有了奇奇怪怪叽叽喳喳的拨号声音，速度也能到几百k。有了网络物质基础，就需要寻求网络娱乐内容。在家里，梦幻西游、大话西游成了大家的首选（至少是男生吧），回家总是会打开玩一会儿。神奇的是我没有在这两个游戏上充过点卡，要不是自己就玩个免费的阶段（10级前吧），要不就玩同学的号。网吧更多的可能是cs、流星蝴蝶剑的天下，但是我也一次没有去过。而在学校，flash游戏成了班上所有同学的选择，优点显而易见：无需安装打开就玩；体积小，最多加载个一分钟；类型多，打开4399这类网站，光游戏分类就琳琅满目，每个人都能找到自己所需要的。于是微机课上完自由活动的时间，也就成了班上flash游戏的时间。flash游戏分类里面有一个双人小游戏，这是我和小伙伴最喜欢的。常常和小伙伴两个人挤在一张电脑桌前，还好当时手小，键盘放四只手也没问题，愉快的游戏时间就开始了。这可能就是小时候的“开黑”吧~ 那时我们突然意识到，除了打球、踢球这种运动，原来还有游戏这个虚拟的空间能够让我们站在一起，或是并肩作战，或是相互厮杀。因此本质上，游戏应该和其他的运动项目没有什么区别。胜利有时也不是必须的，不是常说友谊第一比赛第二吗？人与人之间的关系才是重要的。 不过这么多年过去了，无论是游戏，还是玩游戏的人，似乎都变了很多。 flash游戏受限于平台，难以做得更好，移动端随着智能手机逐渐发力，手游如雨后春笋不断出现。金庸群侠传原作者半瓶神仙醋，也亲自投身于手游开发。然而手游看似更为休闲，但实际上往里面投钱的并不少。这又是为什么呢？ 或许是曾经的关系搞反了。现在网络更方便，人们可以从网上认识到更多的人，甚至早于在现实中相识。因此想要得到对方的认同的话，游戏中的一堆数字是最好的证明。而最简单的方法就是充值……这么看来，游戏更是一种炫耀、渴望被认同的一种手段。当初和现实朋友一起奋战的感觉变得更少了。 但是另一方面，游戏业也在这种氛围中钱越赚越多。半瓶神仙醋的微博，还停留在去年，其中靠前的一条是给自己的手游《冒险王2》宣传，而金庸群侠传4好像已经无限跳票，做免费游戏当然不能是他的义务，玩家自然不能强迫。毕竟如今35岁，可能再也追不回之前的梦了。我想我可能也是如此吧，十年后又能记得起如今的梦吗？还是定一个小目标：十年后还能记得起十年前身边的人，和与这些人一起的故事吧。 &nbsp; 自慰诗 半瓶神仙醋， 生于冀中， 学于湘潭， 混迹于京师。 苟活二十有五， 孑然一身， 惨淡经营， 做此游戏， 聊已慰藉。 不求闻达于诸侯， 不甘堕落于俗事。 虽勤勉学习， 艰苦奋斗， 仍然一事无成， 可叹造化弄人也. （此为2006年金庸群侠传2中所作）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[初次评论无需审核]]></title>
      <url>%2F2016%2F09%2F08%2Fe5-88-9d-e6-ac-a1-e8-af-84-e8-ae-ba-e6-97-a0-e9-9c-80-e5-ae-a1-e6-a0-b8%2F</url>
      <content type="text"><![CDATA[应该是这个设置吧，以后大家评论也不用经过审核了，我审起来也麻烦。。 PS：检查卫生说我桌子乱不是一次两次了，这次为了收拾桌子就把树莓派收起来了，今天服务器就没开= =]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[祝你有美好的一天'√']]></title>
      <url>%2F2016%2F09%2F07%2Fe7-a5-9d-e4-bd-a0-e6-9c-89-e7-be-8e-e5-a5-bd-e7-9a-84-e4-b8-80-e5-a4-a9-e2-88-9a%2F</url>
      <content type="text"><![CDATA[如果你有幸和我到ktv，那么一定不会错过一首歌：Have a nice day。 虽然嗓音不行，但是瞎喊喊就很爽了，唱歌不就是为了表达宣泄感情吗？ 不过说到这首歌，不得不提这首歌的乐队：Bon Jovi。 最初接触他们是小学在某电视节目上，放的CS的视频，BGM就是It’s my life，这也是我认为是最被国人所知的一首歌。视频里精彩的击杀镜头被这首歌衬托得十分酷炫，不过当时可能更多的激起了玩游戏的欲望…… 这首歌始终被埋在记忆深处，直到初中开始学习英语，英语老师让我们学唱一首英文歌。一开始选的是You are my sunshine，反光镜乐队版，很短很简单= =会唱之后很没有成就感，就想再试试其他歌，不知怎么，It’s my life就悄悄在脑中响起……最后在课上老师让我演唱一曲，我也不知道最后唱的哪一个，但是五音不全是肯定的。 唱歌受挫，听歌总不会如此。我便更多地去了解这个乐队，Have a nice day、Bounce、Always等一首首加入了我的mp3。至少在初中、高中这段紧张的学习生涯中给了我很多动力。 Oh, if there’s one thing I hang onto that gets me through the night I ain’t gonna do what I don’t want to; I’m gonna live my life Shining like a diamond, rolling with the dice Standing on the ledge, I show the wind how to fly When the world gets in my face, I say Have a nice day！ 《Have a nice day》 初高中听这种歌是为了提神，因为每天辛苦的学习可能都不是我们所希望做的，或者说”live my life”。但我们可以把这个过程当作”live my life”的准备，“考上大学就能live my life”了。事实如此吗？大学里好像也只是按着课程，为了成绩、排名而努力，量化到了一个数字，这就是“life”吗？是自己的“life”还是别人规划好的“life”呢？可能我只是把别人交给的任务努力做的优秀罢了。 [caption id=”attachment_39” align=”aligncenter” width=”300”] ‘√’[/caption] 不过我也希望有一天能够找到自己的“life”，因此从初二开始，这个专辑封面一直作为了我的头像，希望Have a nice day，也希望live my life。PS：今年Bon Jovi又发了新专，This House Is Not for Sale，毕竟62年出生的人了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[发微博啦~ Joy是小一一]]></title>
      <url>%2F2016%2F09%2F03%2Fe5-8f-91-e5-be-ae-e5-8d-9a-e5-95-a6-joy-e6-98-af-e5-b0-8f-e4-b8-80-e4-b8-80%2F</url>
      <content type="text"><![CDATA[如果再加上if weibo then wordpress会不会无限循环啊。。[笑cry] (via Weibo http://ift.tt/2c0xa0b)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[微博的朋友们你们好吗]]></title>
      <url>%2F2016%2F09%2F03%2Fe5-be-ae-e5-8d-9a-e7-9a-84-e6-9c-8b-e5-8f-8b-e4-bb-ac-e4-bd-a0-e4-bb-ac-e5-a5-bd-e5-90-97%2F</url>
      <content type="text"><![CDATA[把wordpress和weibo连起来了，这么说我发一条文章就会自动发一条微博咯？ 测试一下]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Real or Not]]></title>
      <url>%2F2016%2F09%2F03%2Freal-or-not%2F</url>
      <content type="text"><![CDATA[美剧有个特点，就是有点长，还拖成好几季，比如之前看的南方公园，目前估计都要二十季了吧。入坑还是要谨慎。 然而我还是经不住诱惑开始看Rick and Morty了……不得不说脑洞太大，动画的形式充分展现了无边的想象力。 比如虚拟（Rick and Morty (e4)）和梦境（Rick and Morty (e2) ）的两集，分别讲述二人在人工制造的虚拟城市和多层梦境（有点像盗梦空间）的冒险。这种经历往往会让人对虚拟和现实产生一定的疑问。 当然很多人都开始了这种思考，特别是人们逐渐了解人体、生命之间的秘密和联系之后，发现很多都是可以解释和人工产生的。最疯狂的可能是“缸中之脑”这个概念。这是希拉里·普特南（Hilary Putnam）1981年在他的《理性，真理与历史》（Reason, Truth, and History）一书中，阐述的假想。艺术作品也对这种设定情有独钟，频频出现于各种科幻剧里，个人感觉《黑客帝国》是最著名的。 那么问题来了，如果有一天发现世界并不是真实的，应该怎么办？似乎这比今天吃什么还要难以抉择。 即使是虚拟的世界，但通过努力完成了一定的工作量，在这个世界中有所贡献，似乎也是不能轻言放弃的，就像minecraft中一个个宏伟的工程。同样与他人之间的关系也很重要，不过如果得知其他所有的人都是虚拟出来，只是AI的话一定很难以接受。哪个选择才是正确的，或许并不重要，而在虚拟中和真实中度过一生又有什么不同呢？ 如果这种事情发生在我身上，具体应该如何选择，的确是需要好好考虑一番。但是让我用一句话总结的话，还是选择一个更需要我的世界吧。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[去健身吧~]]></title>
      <url>%2F2016%2F09%2F01%2Fe5-8e-bb-e5-81-a5-e8-ba-ab-e5-90-a7%2F</url>
      <content type="text"><![CDATA[去健身吧~ （其实去年我也是这么说的） 最初的原因其实是看了一部JoJo的动漫，人物的画风大概是这样的： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 以及这样 感觉好强啊= =，对于习惯正常日漫萌系画风的我还是有一定的冲击（特别是这些羞耻？的造型。。）。不过就剧情来讲，差不多每一场战斗都是靠的脑子取胜的，看起来还是很有意思。 于是我也开始了跟风办了健身卡，大二下学期尽量保证每周至少去了一次……发现没什么用。。。 现在有了时间（才不是看了新的动漫），尽量养成健身的习惯吧。而且这办卡还是去了三次才办成，来之不易，更应该好好珍惜。 那么一起加油吧~ 最后！！！！！《99》完整版出了！！！！不来听一下吗？ [audio mp3=”http://yanjoy.win/wp-content/uploads/MOB-CHOIR-99.mp3&quot;][/audio]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[上传文件限制解除]]></title>
      <url>%2F2016%2F08%2F31%2Fe4-b8-8a-e4-bc-a0-e6-96-87-e4-bb-b6-e9-99-90-e5-88-b6-e8-a7-a3-e9-99-a4%2F</url>
      <content type="text"><![CDATA[想添加首歌，然而打开添加媒体…… “最大上传文件大小：2MB。” ？？？什么年代了这种限制简直不够看啊！ 还好大家都是这么想的 先是参考wpyou.com，三种方法都尝试了结果都不行，特别是第二个找不到php.ini，自己在根目录创建了一个。但是眉头一皱感觉事情没有那么简单 查找资料发现限制大小的其实是php，那么php.ini应该在php目录中。参考冰莫言php安装路径，果然找到了，然而不止一个： 这里面3、4个目录都有php.ini。。所以都改了吧= =，把upload_max_filesize 由2M改到64M。心满意足~~~~ 干完之后发现果然增加了！！！！增到8M了！！ ？？？？跟说好的不一样啊！！！莫非是大意了？ 再看一下文章，果然我只改了upload，还有post_max_size 没改。一看post_max_size=8M，对这个数字很熟悉啊，看来上传媒体文件不能大于post大小限制。 而且jb51.net上有更详细的说明，顺便改了wp-config.php的内容。 一切完工，成功了~ PS：早知道改这么麻烦就把限制再放开些了 [playlist ids=”74”]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[树莓派搭建wordpress|心路历程（雾]]></title>
      <url>%2F2016%2F08%2F30%2Fe6-a0-91-e8-8e-93-e6-b4-be-e6-90-ad-e5-bb-bawordpress-e5-bf-83-e8-b7-af-e5-8e-86-e7-a8-8b-ef-bc-88-e9-9b-be%2F</url>
      <content type="text"><![CDATA[项目这几个月都没有动，两个树莓派就放那里不断吃灰= = 为了废物利用（？）还是随便建个小站玩玩吧。 虽然说wp是个很傻嗨的东西，但现在还没有完全搞定。。 首先参考的是果壳及其引文，采用的是nginx。照着做到最后一步，wp-admin/install.php 打不开QAQ是一片空白。。 之后删了nginx换apache，然而……还是这样 联想到第四步和实际情况的不符 在/var/www目录下新建一个index.php文件： $ sudo nano /var/www/index.php 在这个文件里只需写入一行： 保存并退出编辑。删除该目录下的index.html文件，再次用浏览器打开Apache服务器的默认起始页面，应该能够看到PHP的配置信息。 感觉可能是index没放对地方。/www 里面还有个 /html，放进这里才能显示。。 这么说我把wordpress放在/www 里面没有卵用，放到/www/html 里面才行。 结果肯定是成功了（要不就没有这些东西了） 可是更新功能全跪（无法创建目录），另外无法从wp上传文件（显示上传成功在服务器上找不到） 网上所有的方法其实都一样——给权限。但是疯狂给权限之后还是不行啊~~~ 上传服务器有人说改一下uploadspath，改成/wp-content/uploads 就可以了，实践后貌似没用，就干脆改成全路径/usr/share/wordprees 也加上竟然可以了（玄学 所以忙了大概一天，解决了这些东西 感觉自己什么都不会 用树莓派建LAMP+WordPress服务器树莓派Raspberry Pi讨论区技术论坛云汉电子社区来源： 用树莓派建LAMP+WordPress服务器树莓派Raspberry Pi讨论区技术论坛_云汉电子社区_ &nbsp; PS：拿这个当第一篇博文貌似并不文艺，写博客的初衷是啥呢？重拾文艺撩妹]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[小站开工]]></title>
      <url>%2F2016%2F08%2F29%2Fe5-b0-8f-e7-ab-99-e5-bc-80-e5-b7-a5%2F</url>
      <content type="text"><![CDATA[差不多干了一天，只剩下插件和主题两片乌云]]></content>
    </entry>

    
  
  
</search>

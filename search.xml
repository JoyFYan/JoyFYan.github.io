<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[half-life]]></title>
    <url>%2F2017%2F07%2F21%2Fhalf-life%2F</url>
    <content type="text"><![CDATA[仍然是游戏专题，这次要说的是Half-Life（半条命）。作为上世纪末的经典游戏，“《半条命》自发行以来，不论是在玩家反应、游戏评价、销售量都创佳绩。至2004年11月16日，销售量已达800万套。2008年时，销售量突破930万套。”但作为那些年才出生的我们（包括我），却很少有机会体验这样一款“佳作”。今日我重温了这款20年前的游戏，至少理解了这个名字的”含义“。 十年前 Half-Life（半条命）对我来说绝对是一个神秘的存在。当时最流行的是CS，让所有人都有机会拿起各式各样的枪相互对决，绝对是一个极具吸引力，激发荷尔蒙的游戏。可以说当时的网吧被CS、流星蝴蝶剑、红警、魔兽所占据，玩家数不排第一也是第二。但我玩过的CS目录下都会有两个可执行性文件，一个是CS的，另一个则是hl.exe，并且上面有一个中文”入“。年少的我也知道这个东西也可以运行，但打开之后往往会提示错误，因此这个程序就成为了童年一大谜题。渐渐的，CS被CF的热潮所替代，这个谜题似乎难以得到回答了。 如今 现在我也不知不觉成为了一个正版游戏用户，一是为了方便安装管理，二是为了弥补儿时游玩欠下的债。在Steam上，Valve Complete Pack包含了Valve公司十余款经典游戏，不乏Half-Life、Portal、CS这类大作，总价也才50多元，于是购买之。半条命这款20年前的大作终于有机会正版地运行了。看了一下好友中有10名拥有此游戏，但没有人真正玩过，唯一一个只运行了0.4小时，看来玩这款游戏还是需要一点情怀的。为了让大家快速熟悉，下面复制了一段剧情介绍： 2000年5月5日，弗里曼和他的工作小组正在进行的试验出现了可怕的错误（可能有人故意引起）。结果时空的连续统一体发生了破裂，外星人进入了研究所内，杀死了它们发现的所有人类。弗里曼发现自己腹背受敌：凶残的外星人和奉命执行清理（杀死一切外星人和类似人的生物）任务的政府军队让他陷入两难境地。为了能够活下来，这个没有受过任何专业训练的理论物理学家决定拯救人类于混乱之中，他的英雄举动感动了一些幸存下来的科学家，结果他们一同成为军方最主要的消灭目标。在经过了一系列冒险、杀死了无数外星人和士兵之后，弗里曼最终从试验室经过时空传送到了外星人的老家：Xen，他决定先消灭外星人的头领Nihilanth。从最后的遭遇战中恢复知觉后，弗里曼发现自己面对是谜一般的人物——G-Man，这个神秘的科学家自始至终都在观察弗里曼的一举一动，或者说他一直在操纵弗里曼的命运。G-Man使用他的能力让弗里曼在地球和Xen之间传送了数次。摆在弗里曼面前的有两条路：同意为G-Man和他的“神秘雇员”工作，或者在没有武器的情况下死在那些凶残的外星人手下。《半条命2》中已经说明，弗里曼接受了G-Man提供给他的工作。 点开游戏，陌生而熟悉的戈登·弗里曼博士（Dr.Gordon Freeman）出现在屏幕上。很难想象一个物理博士要去面对如此残酷的挑战。（顺便说一句，我目前的英文名全称为Joy Freeman Yan，当时Freeman也确实从这里得到启发的。） 进入游戏，我身处一辆类似地铁的车中，当车缓缓驶停，你可以控制我们的主角时，便是“虐待”的开始。最大的不适应是游戏菜单中，没有地图，没有任务提示，是一个“完全”的第一视角射击游戏。这对于玩惯现代游戏的我造成了一定的困扰，该干什么完全需要靠NPC的对话。有时候没有听明白，就只好乱在地图中转圈乱按了。当然这些只要适应了还好，真正恐怖的在于敌人与解密关卡。初期是一些丑陋的外星生物，面目狰狞，看起来还挺恶心的，还好行动速度和伤害都不算高。但中后期出现的人类军队以及外星战士就不是那么好对付了，有时根本不能暴露在掩体外，否则迎接自己的只有一通扫射，更惨的时候还会成为两边共同的靶子。最终Boss也是，你需要边躲避各种大型伤害子弹，还有绿色的传送门——被打中就会进入满是怪物的房间。另一方面，你的资源：弹药和生命护甲补给就显得十分宝贵了，每一发子弹都不能轻易浪费。有的补给处还隐藏着遥感地雷，真是充满着恶意。解密要素有时甚至难于射击，同时这也要求玩家具有很好的跳跃、移动、反应操作水平，不然一不小心就落入深渊，也可能是盛满辐射物质的容器内。特别是在铁轨上的那一关，你需要不断调整轨道方向，将轨道车开到正确的地方。之后进入传送带房间，错综复杂的立体传送带，有时感觉自己就在走迷宫。除此之外，真正的传送门关卡也让人摸不着头脑，玩家需要一层一层通过传送门向上，稍有不慎就会从旋转平台跌落，玩的时候常常会让我产生对我是谁我在哪的哲学问题的思考……经过九九八十一难，终于完成了所有剧情。即使是在控制台的帮助下，我也感到十分心累，总共花费了5小时左右。如果没有这些帮助，恐怕50小时也难以完成，联想到这个游戏的名字，除了被虐待成半条命，可能也有花一半生命时长才能通关的意思吧。 半衰期 玩罢了游戏，却仍有一个最大的问题留在心中：游戏名半条命到底有什么真正含义呢？如果说单纯直译的话还是勉强可以理解，而港台译作“战栗时空”明显与游戏剧情背景相互联系的，似乎更好一点。查阅更多资料后，我又有了新的认识： 半条命（Half-Life）最初原定名为“Quiver”，名字灵感来自于史蒂芬·金小说《迷雾惊魂》的Arrowhead军事基地。加布·纽维尔解释说最终选择“Half-Life”（物理学上代表半衰期）是因为它与游戏的主题相符却不陈腐；而该名又和希腊字母 λ（lambda）相呼应于一个希腊标志，因为λ代表着半衰期中的一条数式衰退常数。 嗨，原来人家用的不是中文“入”而是希腊字母“λ”啊（其实游戏里出现了很多λ，从对话中可得知主角要去寻找λ小队解决问题）。这样的确是更能理解这个游戏名称的含义了，只能说当初翻译游戏名字的人和我的英语水平差不多嘛。 20年来 用现在的话讲，它重新定义了第一人称射击游戏（FPS）：CS作为竞技类FPS的标杆，而它可CS的爹啊。很难想象这样的游戏出现在20年前，无论是武器的功能，还是关卡的设计，都体现出很高的水平。相比于传统的无脑杀敌，玩家与场景的互动也起着重要的作用，上文也说过包括解密、技巧跳跃元素，都让人感到游戏并不简单。记得还有一关的boss需要玩家在一个机器的地图上设定横纵位置进行炮击才能打败它。在玩的时候，真的会是一遍遍的失败，特别是跳箱子，我总觉得跳下来落地落不稳，会往前滑一段距离，前后调整又会调整过度，总之很不舒服。可能是因为我们已经有了很好的游戏，不必在这上面浪费时间了吧，也肯能是这种稍微硬核一点的游戏要求，我已经不习惯了，总之没有坚持下去凭借自己的双手通关（即使开了作弊，有的地方也是看攻略才知道怎么走的）。但这也足以让我领略到当时游戏的“耐玩性”。毕竟买到一款游戏就要值这么多价钱啊，再加上后来的联机mod可以说完全超出了游戏本身的价值了。20年来，游戏不断出新，但其平台却让更多更新的玩法层出不穷。早期CS有打僵尸那样的自定义地图，而最新的CS：GO里创意工坊有躲猫猫，每每看到这些，心中总忘不掉这一切的起源。说道起源，这不就是Valve开发的Source游戏引擎的名字吗？]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>half-life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感谢伴我四年的——饭卡]]></title>
    <url>%2F2017%2F06%2F24%2Fmycard%2F</url>
    <content type="text"><![CDATA[即将要毕业了，在这四年中，陪伴我时间最长的恐怕是我的饭卡和钥匙了。 饭卡的确是大一入学发的，整整用了四年，是学校生活的通行证。每一次吃饭，每一次接水，每一次洗澡，都少不了它的陪伴。图上的卡贴，是CD漫展上买的七海千秋，当时的还收获有：这也是我第一次去漫展~ 钥匙由于搬寝室换过一次，但我似乎也没有配钥匙的记忆，那就当没有吧。 感谢我的饭卡，更要感谢背后一个个陌生人。我的饭卡并不是没有丢过，恰恰相反，至少找不到过五六次。最常见丢的地方是水房、洗澡间，常常是把卡插进去，办完了事情就忘了拿出来。特别是大一大二，一到晚上用水多，导致热水出水很慢，常常是边拿着手机边等水接满。这个过程非常容易让人焦躁，想快点离开，之后就提着水壶溜了，留下孤独的饭卡风中凌乱。运气好一点，我一回寝室就发现不见了，迅速返回水房，还能把卡拿回来。但有一次不知道脑子里在思考着什么，完全没有饭卡的存在，直到第二天肚子提醒了我。到了水房，发现卡槽中没有饭卡，正在怀疑人生之际，看到热水器上面贴着纸条，说捡到饭卡，请到某某寝室认领。还有这种操作？？怀着忐忑的心情去那里，没想到真的就是我的饭卡。我只记得我当时真的很感谢，也完全没想到还会有人费心留个纸条，并且帮人保管。我也暗下决心，之后也要这样做，但看来我只是丢饭卡而不是捡饭卡的命。 额外说一点，我大三之前没有拿钱包的习惯，感觉装在裤兜中太麻烦，因此大部分时间饭卡和钥匙都是在裸奔。之后再谈谈钥匙吧。之前有一次是打完篮球回寝室，发现没钥匙了，回到篮球场找到了。另一件发生在前几天班上在别墅玩，回来时是第二天中午，当时还处于刚睡醒又不是很清醒的状态。一张班群的图片瞬间又让我清醒，摸摸兜一切又明白了：钥匙丢在别墅，业主发给班委图片求认领。难道刚坐地铁到寝室，又要返回去吗？单程50分钟还是挺累的。因此我让班委问一下业主是否方便快递过来。班委说他同意了。第二天，我成功拿到了这两把钥匙。但奇怪的是领快递的时候并没有向我要钱，并不是货到付款！这让我感到很愧疚。遇到班委我想向他要到业主的联系方式，但班委却说聚会多出来一些钱，邮费就已经用班费出过了。一瞬间，我想说出一万句谢谢，却我又不知道具体应该感谢哪一个人了。 我感觉这个世界很温暖，如果我找不到回报、感谢的人，那么只好感谢你们的每一个人了。有的时候我做些好事，与人方便，并不是期望这个人对你有什么回报，而是希望我将来也能被这个世界温柔对待。既然“己所不欲勿施于人”，那么“己所欲”的，应该也要多“多施于人”。 （题外话：丢饭卡、钥匙还是我自己的责任，无论如何还是要小心些，少给别人找麻烦^_^）]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>饭卡</tag>
        <tag>感恩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[建立个人博客-进阶]]></title>
    <url>%2F2017%2F06%2F18%2Fblog-1%2F</url>
    <content type="text"><![CDATA[引言 在上一篇文章中，我把建免费个人博客的的步骤简单总结为了三点： 申请空间 博客框架搭建 上传与更新 有同学尝试后最大的问题是Github桌面版版本与教程不符。目前最新的版本是Desktop Beta，界面有了较大的变化，操作也可能有所不同。为了简单还是下载稳定版比较好。 本篇文章主要针对于域名、解析、主题等进行更深的介绍，帮助大家搭建更个性化的博客。 属于自己的域名 每个人都有名字，这是我们区分他人最主要的方式之一。网页也如此，并且更严格：没有重名的网站。有一个属于自己的域名才能说有了自己的个人博客。 为了方便支付和管理，还是选择阿里爸爸旗下的万网作为服务商。 输入你想要的域名进行查询，如果结果是未注册便可使用，记得要查看全部后缀哦。 不同的后缀会有不同的价格，我的.win是最便宜的一类，10年才50元（去年价）。 加入清单，再输入一些个人信息，便可以拿到自己的域名了（当然还要记得付款= =）~ 域名解析配置 有了域名，相当于有了一个门牌号；有了网站，相当于有了房子；解析的功能简单来说就是通过门牌号找房子。 返回万网首页，右上角有控制台按钮，点击进入。在左侧栏中选择域名-云解析DNS。出现购买的域名列表，点击需要解析更改的域名。在出现的设置中，首先点左上角的添加解析，会在下面出现一行需要添加的解析设置信息。 记录类型：选择CNAME 主机记录：www和@，分别建立两个记录 解析路线：可以对国内外的访客解析到不同的地址，比如使用国内代码托管网站coding代替Github 记录值：你的Github博客地址 好啦保存。还差一小步，就是在你本地博客public目录下建立一个CNAME的文件（注意没有格式），用笔记本编辑，内容就是你的域名，并更新到Github。稍等访问你的域名，就会看到已经出现网页了~ 主题配置 hexo默认会有一套主题，位于themes\landscape。而我是用的是NexT主题（Github）。这个主题相对简洁，扩展也够用。 对不同主题的配置，往往需要仔细阅读其配置手册。 这里需要注意站点配置文件和主题配置文件的区别，我当时就搞混了，配了很长时间没有成功。 比如在上文中，我们初始化的博客目录为E:\github\pages，那么在此路径下会有一个_config.yml为站点配置文件；而主题目录下E:\github\pages\themes\landscape也有一个_config.yml，为主题配置文件，记得不要搞错了。 除了NexT主题，还有其他的一些流行主题，比如可以参考知乎回答有哪些好看的 Hexo 主题？。 额外的话 好了，到此为止建立个人博客的基本、进阶教程基本结束了，如果遇到了什么问题，欢迎评论私信~花一些时间去写一点东西，无论是否有人看，对我来说都是一种释放，感觉很不错。]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[0元3步建博客]]></title>
    <url>%2F2017%2F06%2F12%2Fblog-0%2F</url>
    <content type="text"><![CDATA[引言 在目前新媒体如微博、公众号盛行的情况下，15年前最热网络平台博客已经很少出现在人们的视野中了。这也是得益于移动应用的快速发展，碎片化时间的增多，让人很难有时间去坐在电脑前完整阅读一篇博客。不过博客也趋向专业化、特色化发展，虽然传播性不强，但其开放性、专一性也往往是其他平台比不上的。本文就简单教大家使用Github+hexo搭建一个个人博客，只需3步，并且是免费的哦~ 申请空间 网页也需要有一个存放的空间，不用担心，这不要钱，就是有这种操作！Github就是我们所需要申请的网页空间。在大部分人的印象中，Github是全球最大的同性交友网站，是程序员抄程序的地方（是的）。但除了免费托管代码，还有一个一般人不知道的功能：GitHub Pages，这是一个静态站点托管服务。简单来说你把网站发送上去，再访问相应的域名，就可以让世界各地的人浏览网页啦~ 点击进入全球最大同性交友网站→Github出现一个网页，注册一下：没问题的话让你选择是否公开仓库，然后Continue。接下来让你选经历之类的，我就跳过了。注意你填写的邮箱，这时候会有邮件去让你验证，不然无法创建新的仓库。你可以随时在右上角点加号创建：在新页面中创建仓库，需要注意仓库名是你的用户名 + .github.io接下来需要下载Github客户端用于网页的上传更新，点击进入下载。先下载安装登录好，过会儿再用。第一部分申请空间就到此结束了~ 博客框架搭建 网页并不是需要我们一句一句代码敲上去的，而是有相应的网页框架，如Hexo、Jekyll等。本教程以Hexo为例，首先下载Node.js，官网在此。LTS是稳定版，Current是尝鲜版，怕麻烦就稳一些吧。安装后再安装Hexo，1npm install -g hexo 这个命令需要在命令行中输入，按住键盘上的微软徽标（Ctrl和Alt之间），再按R，在出现的黑色命令行中输入。但命令行的使用需要注意路径问题，为了简单起见，还有一种命令行打开方式：假设我们的路径为：E:\github\pages，进入目录后注意左上角菜单栏，从这里进入，会发现闪动的光标前有E:\github\pages&gt;，代表目前的命令就是在当前路径下。这时在输入1hexo init 完成博客文件的初始化。之后就可以写文章啦~（在哪里写啊对着这个黑乎乎的东西吗？）对了其实不同的操作会有不同的命令，新建文章就用1hexo new "hello" 冒号间是文件名（不是博客中的标题，推荐英文）。创建成功，按照路径打开它（提示没有可用的程序时，可以选择记事本）：这里才是文章的正文，title是文章标题，date是编写时间，tags是标签，下面的是正文。具体的编辑并不像word一样，而是需要遵循一定的markdown语法。具体可以参考这里。写好之后，回到命令行，输入1hexo g 生成页面需要预览的话，就输入1hexo s 然后在浏览器输入http://localhost:4000/这个网站：恭喜你成功了~ 上传与更新 这时需要刚才下载的Github客户端了。登陆后Clone刚才建立的仓库：建议在刚才hexo 初始化的路径E:\github\pages。之后会在该路径出现yanpages.github.io文件夹，即是我们clone下来的。为了全程图形操作，下面操作可能有点绕： 将E:\github\pages\public中所有文件复制到E:\github\pages\yanpages.github.io中； 删除E:\github\pages\public目录 将E:\github\pages\yanpages.github.io改名为public 其实就是偷梁换柱，之后打开Github会出现错误，注意这时点第一个locate，打开E:\github\pages\public目录。成功的话会出现：把Summary和下面的填一下，再点下面的对勾~最后一步！然后再去看看你的网站：像我的https://yanpages.github.io/！恭喜你成功啦~ 额外的话 目前Github网页个人空间大小不能超过1G，其实也不小，但是图片这些东西，完全可以放在别的地方。比如我采用的方案是用CSDN博客功能写Markdown文件，再导出，基本上是完美兼容的。 删除文章就是对E:\github\pages\source\_posts内的修改，删除某个md文件后，再次使用hexo g生成，新的页面就会删除文章。 Markdown语法还是要多熟悉一下，之后你会发现真的很好用。]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
      <tags>
        <tag>建站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人生无存档]]></title>
    <url>%2F2017%2F06%2F09%2Flife-without-save%2F</url>
    <content type="text"><![CDATA[作为一个业余的单机游戏爱好者，感觉有必要写一些东西纪念在游戏中收获的快乐、悲伤、智慧与思考。这一篇是写给巫师3的，其他一些有趣的游戏，我也会不定期更新。 0《巫师3：狂猎》已早有耳闻，但限于硬件一直没能玩到这款作品。直到今年三四月份，一是打折促销，二是配置达到要求，终于一睹游戏真容。 除了精美的画面，剧情内容的丰富程度肯定是远超大部分作品，这也是我很少跳过剧情对话过场的游戏。相比之下，刺客信条2一周目30小时，GTA5一周目（可能也加上线上游戏时间）52小时，而巫师3目前花费了32小时，却觉得玩了不到1/3。 2015年10月，获第33届金摇杆奖最佳剧情、最佳视觉设计、最佳游戏时刻，更获得了年度最佳游戏大奖。并获得IGN 2015年度最佳游戏。2016年7月，波兰开发商CD Projekt Red正式确认，《巫师3：狂猎》将迎来年度版（Game of the Year Edition），将会囊括游戏发售后的所有追加内容，包括两个大型资料片“石之心”和“血与酒”。其中血与酒大型资料片获得由2016年The Game Awards(简称TGA)于北京时间12月2日举办颁奖典礼,所颁布的年度“最佳游戏角色扮演游戏”奖。 这是一个凭借DLC就能拿年度奖的游戏。 1目前的主线较为简单，就是追踪希里（Cirilla）的下落。主角杰洛特在路上不断通过完成任务来换取情报，当然总是慢人一步。目前这些任务中，血腥男爵应该是我最印象深刻的。 任务中，为了获得情报，杰洛特来到血腥男爵的领地，男爵让他寻找离家出走的老婆和女儿。追查中认识了三名丑陋的巫女，称之为老婆妪，杰洛特也希望从中打听希里的信息。老婆妪让他去找村民，帮助他们去除伤害他们的恶魔。主角找到之后发现是洞中一个被封印的树魔。此时树魔却突然说老婆妪正在杀害她领养的孤儿，如果主角放他一条生路，就可以解救那些孩子。 这时我是不相信的，因为有村民、老婆妪的两方认定，我认为这是欺骗我的缓兵之计，因此击杀了它。回到村中，村长突然割下了自己的耳朵，让我交给老婆妪，说这是为了老婆妪保护他们所需的祭品，我也一下就明白了老婆妪身上以耳朵串成的项链的来历了。这时我突然心头一震，莫非树魔没有欺骗我？找到老婆妪，这次她的出场带了几分得意与邪恶：大大的肚兜上挂着小孩子纤细的腿，一晃一晃像是在诉说着什么……我再也无法忍耐，而剧情中却无法与之战斗，只好选择重新读档。 在第二次的剧情中，我选择解救了树魔，对村民说恶魔已经跑了，村长依然割下了耳朵。这次去见老婆妪，她气急败坏，却把责任推给了照顾小孩的一位老奶奶，责备她没有看管好这些孩子，便对她下了诅咒。之后杰洛特离开了，找到女儿后，却又从各种线索中惊奇地发现那个照顾小孩老奶奶就是男爵的老婆，女儿的母亲。这时男爵带领一队人马与杰洛特去寻找老婆妪，路上经过之前帮助过的村子，却发现已经被树魔所破坏而不复存在了。来到曾经的地方，老奶奶也消失了，等待着的只有一个被诅咒而异化的怪物。虽然最终解除了诅咒，但仍然离开了人世。一切的发生让男爵无法接受，他也选择结束自己的生命。而杰洛特依然独自前行…… 2无论从网友和我来看，这都是一个坏结局，我甚至想再次读档，去拯救这个悲惨的故事。但我突然又想起一个之前的支线任务，我救下了一个战场的受伤的逃兵，但当我再次遇到他时，他却变成了烧杀抢掠的强盗…… 什么样的选择才是正确的呢？如果这么看，我不断追求一个完美的结局，除非知道了所有选择导致的结果，否则一定会不断地去读档。有时现有的条件很难去判断一个事情是否去做，如何去做，而对于这样一个开放的游戏，的确是让有选择困难的我很抓狂。是否会有这样的一种情况呢？我当初救下的孩子们，在过了十几年、几十年后，也能成为了伟大的人，为世界造福的人。但我清楚，这一定不会在游戏中出现，如果真的有，那么我真的很佩服这一群制作人了。以上只是自我安慰吧，仅仅是对未来的猜测，我连那些孩子的名字都记不得了，将来该如何相认呢？ 3这一切的事与愿违，其实也很正常，因为你能控制的只是你自己，他人的想法、做法往往是不得而知的。不然一个透明的世界也是索然无味的。人与人之间的共鸣与互斥，有时仅仅取决于一个行为，一个动作，甚至一句话。当人们的观念取向相同时，自然会有一种认同感，感觉自己不是孤独的；当观念不同时，即使他人做的事情和自己没有任何影响，却也会有一种厌恶之情。就像那个逃兵，或许他处于乱世，只有让自己强大才能不受欺负，才能不饿肚子，但以我的观点，他罪大恶极。只是不知道为什么，本来我有杀掉他的机会，但最后还是放了他一条生路。或许是因为一路上我杀的人太多了吧。 4上面的一段似乎又有一点跑题了。我想说的是人生没有存档。无论是做了什么，都会在历史的书卷中留下无法改变的一笔，虽然可能被遗忘、被误解，但无法抹消它的存在，更不可能读档再来。发生过的就让它过去吧，只有做好现在的事，规划好未来的事。命运是注定的，因此没有改变一说。一个人很贫困，经过努力变得成功，这是对命运的改变吗？还是说他命运本就如此？这点无法得知，但我们还是有选择的权利，去选择相信什么。 5杰洛特的故事仍在继续，但我也不再会去尝试读档改变些什么了。我是什么，他就是什么。]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>witcher</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shell 脚本初学]]></title>
    <url>%2F2017%2F05%2F28%2Fshell-1%2F</url>
    <content type="text"><![CDATA[其实这个是因为服务器卡不够用，为了充分占用一块卡的资源，需要写个shell脚本帮忙运行网络= =shell是一种脚本语言（区别于编译语言），在UNIX上基本都通用。在我看来，这个脚本主要并不是用来计算，而是用来管理打杂的。 Hello world！ 第一个程序，还是输出Hello world吧。首先建立一个脚本文件1vim hello.sh 进入vim程序，输入：12#!/bin/bashecho &quot;Hello World !&quot; “#!” 是一个约定的标记，它告诉系统这个脚本需要什么解释器来执行，即使用哪一种Shell，bash是比较通用的一个shell。echo命令用于向窗口输出文本，其实你直接在shell中输入echo Hello World!。也会有相同的效果。即可。之后运行需要退出，1./hello.sh 但会出现：1permission denied: ./hello.sh 没有权限，增加之后再试试：123chmod +x ./test.sh #使脚本具有执行权限./test.sh #执行脚本Hello world! 成功~有了输出也会有输入，输入的命令是read。1234#!/bin/bashecho &quot;What is your name?&quot;read Nameecho &quot;Hello, $Name&quot; 这时候执行会等待你输入，并显示出来。 变量 shell中变量的赋值很简单，只需要=即可1variableName=&quot;value&quot; 但是要注意： 变量名和等号之间不能有空格，这可能和你熟悉的所有编程语言都不一样 首个字符必须为字母（a-z，A-Z） 中间不能有空格，可以使用下划线（_） 不能使用标点符号 不能使用bash里的关键字（可用help命令查看保留关键字） 如果观察上面读取语句，会发现使用的时候需要在变量前加上$符号：123your_name=&quot;Myname&quot;echo $your_nameecho $&#123;your_name&#125; 推荐在变量外加上大括号，用于和其他字符区分。既然是变量，就可以多次赋值修改，但也有例外：1readonly variableName 可将变量改为只读变量（也就是常量吧……）。最后也可以删除变量：1unset variable_name 但不能删除只读变量。 还是先干工作吧…… 其实在linux中输入的命令，就是在解释器中运行程序，而shell脚本让这个过程更为集中、简单。做类比的话，shell就是python，.sh就是.py。那么我们如果想要运行程序，直接写我们在命令行里的命令就好了。123#!/bin/bashpython work1.pypython work2.py 这样就不用担心卡在完成工作前被占用了~]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NIPS 2016 Tutorial- Generative Adversarial Networks GAN简介]]></title>
    <url>%2F2017%2F04%2F21%2Fnips2016gan%2F</url>
    <content type="text"><![CDATA[如果说新手如何快速了解GAN，那么这篇论文tutorial应该会被大家推荐。首先作者牛，Ian Goodfellow就是GAN之父；其次文章详细，不仅有技术，也有背景、思想、技巧。我也同样是一名GAN新手，读了之后理解的并不一定很准确，也希望和大家多交流。 NIPS 2016 Tutorial: Generative Adversarial Networks 为什么要学习GAN 第一部分首先解答了why，总结来说，有以下几点： 生成模型是对我们表达高维概率分布的考验 生成模型可以通过几种方式与强化学习相结合（包括model-based、model-free） 时序生成模型可以预测未来 能够在虚拟环境中学习，并应用到真实环境 可以使用缺失数据进行训练，并对缺失数据进行预测 处理多模态输出（单一输入对应不同正确输出） 许多任务本质上需要从一些分布中现实地生成样本 可以说，这是能让计算机进行自我创作的一种途径。在过去的几年中，我们一直去教给计算机很多知识，比如这是一只狗，那是一只猫，计算机也很听话的记住了这些知识。如今我们向计算机提出这个问题：狗是什么样的，计算机能够解答吗？其实之前也有人尝试过，用反卷积的方法，恢复出狗的样子，但是效果并不好，一股抽象派画风，毕竟没有经过专门的训练。 GAN在擅长的领域： 超分辨率 艺术创作 Image-to-image translation 超分辨率重建是图像复原的全新手段。这让我想起了16年光电设计，据说就有队采用这种方法，搬台电脑跑很长时间。从光学上恢复，就要考虑镜头啊、点扩散函数啊之类的，而深度学习的超分辨率，是需要靠计算机自己脑补的。艺术创作好说，风格迁移技术目前也比较成熟了。最后一个大家可能还是比较陌生的，这是一个类似于神笔马良的工具，通过简单的简笔画，就能生成较为复杂的实物图片。之前网上也流行过一阵，现在一时找不到链接了。。 GAN如何工作？ 如果详细地讲，那会很复杂，特别是概率、损失来看。不过我不会着重讲这些，一个是因为难于理解，另一个是因为我也怕讲错了= = 还是拿Goodfellow的比喻吧：G是一个生产假币的队伍，想尽办法让假币无法被识别出来；D是警察，想尽办法查出假币。双方不断在竞争，不断学习，从而达到生成器可以以假乱真的效果。这些也在我之前写的博客中有介绍： Generative Adversarial Nets。GAN可以被看作是一个强化学习，但又有所区别。生成器并没有直接接触到训练数据，所有有关训练数据的信息只是通过判别器学到的。 稍微复杂一点说，我们所做的生成器$p{model}$，是需要学习模型的概率分布$p{data}$，如果我们做得很完美很精确，那么我们完全可以恢复出$p{data}$。但我们无法访问$p{data}$本身，只有$p_{data}$采样出来的$m$个样本。如何用部分去尽可能得恢复全部，是我们所需要思考的。我们不可能把世界上的所有猫都看个遍，但我们仍有分辨猫、创作猫的能力，这就是因为我们学到了猫的特征和共性的特点。按说计算机也有相同的能力，至少在分辨上，为什么生成就更难一些呢？这就跟我们之前给的评判标准（或者是损失）有关了。比如上图是视频序列预测，真实值是左边，中间是优化均方差（mean squared error ）得到的。看起来很模糊，这就是因为预测就带有不确定性，稍微上下左右偏一些，MSE就会很大。因此在使MSE最小这个过程中，就自动平均了各种可能出现的情况，导致图像模糊。因此这也不能够怪计算机算得不准，而是评价标准不好。有时多种人能接受的预测，我们只需要选择其中一种即可，而不是多种的平均。 损失函数GAN分为判别器和生成器两个损失，判别器损失$J^{(D)}$较为固定： 而生成器损失$J^{(G)}$分为多种，比如： Minimax Heuristic, non-saturating game Maximum likelihood game MinimaxMinimax game也就是零和游戏，直接将判别器的损失取反：$$J^{(G)}=-J^{(D)}$$判别器使交叉熵变小，而生成器又最大化相同的交叉熵，当然其优化的参数是不一样的。从直观上很好理解，既然两个的任务相反（造假和验假），那么干脆损失相反就好了。但这样会导致当判别器以高置信拒绝生成的样本时，生成器梯度消失，从而难以训练。 non-saturating game这个就是为了让每方出现错误时都能产生很强的梯度：也采用交叉熵最小，且是非零和的。 Maximum likelihood game极大似然的方式，减小数据与模型的KL散度，其中$\sigma$是logistic sigmoid function。 GAN结构具体的GAN结构大多数都是在Goodfellow的框架下，很多也只是在修改loss function，github上有一个项目：generative-models收集了很多GAN模型，我们拿来看一下，例子是LSGAN。主要的代码和注释：1234567891011121314151617181920212223242526272829303132333435X = tf.placeholder(tf.float32, shape=[None, X_dim]) # X为图像输入，mnist 784z = tf.placeholder(tf.float32, shape=[None, z_dim]) # z为噪声输入，64......def sample_z(m, n): return np.random.uniform(-1., 1., size=[m, n]) # 生成随机噪声def generator(z): # 生成器模型 G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1) # 把噪声作为输入，经过两层全链接 G_log_prob = tf.matmul(G_h1, G_W2) + G_b2 G_prob = tf.nn.sigmoid(G_log_prob) return G_prob # 输出图像def discriminator(x): # 鉴别器模型 D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) # 把图像作为输入，经过一层全链接输出 out = tf.matmul(D_h1, D_W2) + D_b2 return out # 输出判别结果 G_sample = generator(z) # 生成结果D_real = discriminator(X) # 对于真数据，判别器给出的结果D_fake = discriminator(G_sample) # 对于假数据，判别器给出的结果# LS损失函数D_loss = 0.5 * (tf.reduce_mean((D_real - 1)**2) + tf.reduce_mean(D_fake**2)) # 鉴别器损失G_loss = 0.5 * tf.reduce_mean((D_fake - 1)**2) # 生成器损失D_solver = (tf.train.AdamOptimizer(learning_rate=lr) .minimize(D_loss, var_list=theta_D)) # 优化鉴别器G_solver = (tf.train.AdamOptimizer(learning_rate=lr) .minimize(G_loss, var_list=theta_G)) # 优化生成器 DCGAN上面项目的代码都是以mnist为数据集，且层数很少，只是拿来理解用的。真正实用的还是deep convolution 结构。这种结构也是借鉴了CNN，把卷积层反向换成了反卷积：文中提到了几点需要注意的地方： 使用batch normalization（除了G最后一层和D第一层） 主要结构为全卷积网络，无pooling、unpooling层（没有这两种层是趋势） 使用Adam优化器而不是SGD GAN还有一点有趣的是可以在潜在空间中用简单运算进行有意义的语义解释，比如我们用这样一个公式：眼镜男-男+女=眼镜女，就可以在没有眼镜女数据的情况下生成眼镜女图像： GAN tips and tricks 训练技巧 train with labels也就是class-conditional GAN。传统的GAN在生成时输入的是随机的噪声，因此很难控制输出的结果。如果在训练时加入类别条件信息，生成时也加入其中，就可以实现定向、定类生成。 One-side label soothing方法是对于真实图片，鉴别器的标签不设置为1，而是0.9这种接近于1的数字。目的是避免D过于自信。 virtual batch normalization如果使用不同的minibatch进行归一化处理，会导致参数的波动，特别是batch小的时候，minibatch间应该相互独立。 G与D的平衡G与D两个的地位有区别吗？哪个更重要一些呢？Goodfellow这样说：GAN通过估算数据密度和模型密度的比例来工作，只有D最佳时，才能正确估计，因此D应该占主导地位。从另一个角度说，D的层可以更深一些。但有时D太准确又导致G的梯度消失，这也就是之前minimax game中所提到的问题。有时D过于自信，拒绝G的生成，又会导致G梯度爆炸，这种情况可以用one-sided label smoothing解决。 还有一个问题是G与D的训练方法，既然D比较重要，那么G训练一次，D训练多次，会有改进吗？这个问题不同论文给出的结果不尽相同，没有统一的答案，有空的话都试试好了。 GAN的问题 最重要的问题就是难以收敛（Non-convergence），两个players 在训练中相互干扰，自然很难。目前在function space更新，可以证明是收敛的；但实际上训练是在parameter space更新，是否收敛没有明确的证明。 另一个问题是mode collapse： 在学习中只学习到了一部分的分布，没有学习到所有模式。而鉴别器在鉴别时会拒绝单一模式，因此会导致生成器在单一模式中循环生成，无法收敛，如上图所示。 针对这一问题，目前有minibatch feature、unrolled GAN等方法，具体不再讲解。 写到现在感觉还是疏漏了很多东西，其中一部分也是自己还没有能明白的，不敢班门弄斧。具体还是要去看57页原文才能全面了解。国内也有一些资料，比如今年的《生成对抗网络GAN的研究与展望》，也是比较新，可以当作参考和了解。 王坤峰 ，苟超 ，段艳杰 ，林懿伦 ，郑心湖，王飞跃 . 生成对抗网络GAN的研究与展望. 自动化学报, 2017，43(3): 321-332]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe2 安装与介绍]]></title>
    <url>%2F2017%2F04%2F19%2Fcaffe2%2F</url>
    <content type="text"><![CDATA[一早发现caffe2的较成熟的release版发布了（the first production-ready release），那么深度学习平台在之后一段时间也是会出现其与tensorflow相互竞争的局面。从打开这个caffe2的官网就会发现，有了Facebook的支持，连界面也好看多了。不过再仔细看看，觉得又和tensorflow有一丝像，从内到外。 Caffe 2 Caffe2 Caffe2 中基本计算单元之一是 Operators。每个 Operator 包含给定适当数量和类型的输入和参数来计算输出所需的逻辑。Caffe 和 Caffe2 功能的总体差异如下图所示： One of basic units of computation in Caffe2 are the Operators. Each operator contains the logic necessary to compute the output given the appropriate number and types of inputs and parameters. The overall difference between operators’ functionality in Caffe and Caffe2 is illustrated in the following graphic, respectively: 看到这段话，是不是更觉得像是tensorflow了？之前layer的概念被弱化，数据与操作完全分开，不就是tensorflow里面需要定义的tf.matmul和tf.Variable这类吗？其次提出的workspace概念很像是tf中的Session： 1234567891011121314# Create the input datadata = np.random.rand(16, 100).astype(np.float32)# Create labels for the data as integers [0, 9].label = (np.random.rand(16) * 10).astype(np.int32)workspace.FeedBlob("data", data)workspace.FeedBlob("label", label)# Create model using a model helperm = cnn.CNNModelHelper(name="my first net")fc_1 = m.FC("data", "fc1", dim_in=100, dim_out=10)pred = m.Sigmoid(fc_1, "pred")[softmax, loss] = m.SoftmaxWithLoss([pred, "label"], ["softmax", "loss"]) 网络的编写也向tf靠拢了（学了点tf还是有点用的）。 最后还要说一点就是对python的支持大大增强了，当然这也是深度学习的趋势。 安装 4.18发布的版本号为v0.7.0，官网上的安装教程比较详细，也比较好操作Install。 依赖库1234567891011sudo apt-get updatesudo apt-get install -y --no-install-recommends \ build-essential \ cmake \ git \ libgoogle-glog-dev \ libprotobuf-dev \ protobuf-compiler \ python-dev \ python-pip sudo pip install numpy protobuf GPU支持这一部分主要是CUDA与cuDNN，在之前的博客中有讲到过。 可选库12345678910111213141516171819202122232425# for both Ubuntu 14.04 and 16.04sudo apt-get install -y --no-install-recommends \ libgtest-dev \ libiomp-dev \ libleveldb-dev \ liblmdb-dev \ libopencv-dev \ libopenmpi-dev \ libsnappy-dev \ openmpi-bin \ openmpi-doc \ python-pydotsudo pip install \ flask \ graphviz \ hypothesis \ jupyter \ matplotlib \ pydot python-nvd3 \ pyyaml \ requests \ scikit-image \ scipy \ setuptools \ tornado 针对Ubuntu 14.04： 1sudo apt-get install -y --no-install-recommends libgflags2 针对Ubuntu 16.04： 1sudo apt-get install -y --no-install-recommends libgflags-dev Clone &amp; Build123git clone --recursive https://github.com/caffe2/caffe2.git &amp;&amp; cd caffe2make &amp;&amp; cd build &amp;&amp; sudo make installpython -c &apos;from caffe2.python import core&apos; 2&gt;/dev/null &amp;&amp; echo &quot;Success&quot; || echo &quot;Failure&quot; 这一步在我这边很慢，clone的速度大概只有几十kb，而且中途也出现了错误。如果选择用zip打包下载，就会出现caffe2/third_party/目录中第三方包缺失的情况，这时还要自己手动下载。 GPU测试可以执行：1python -m caffe2.python.operator_test.relu_op_test 这步安装完成之后，会在/usr/local/caffe2、/home/user/caffe2/build（你的build路径）路径生成caffe2的python文件，在/usr/local/lib路径生成库文件。 设置环境变量设置正确的话，执行命令会有#后这样的输出12345echo $PYTHONPATH# export PYTHONPATH=/usr/local:$PYTHONPATH# export PYTHONPATH=$PYTHONPATH:/home/ubuntu/caffe2/buildecho $LD_LIBRARY_PATH# export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH 具体设置更改对应shell的配置文件，一般来说1sudo vim /etc/profile 在最后加上123export PYTHONPATH=/usr/local:$PYTHONPATHexport PYTHONPATH=$PYTHONPATH:/home/ubuntu/caffe2/buildexport LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH 就可以了。 总结 目前caffe2还不是很成熟，某些文档也还不齐，安装也可以不用很着急，熟悉一下操作就好。mask-rcnn不就应该就会在这上面放出源码，可以稍稍再期待一下。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.cond 与 tf.control_dependencies 的控制问题]]></title>
    <url>%2F2017%2F04%2F18%2Ftfcond%2F</url>
    <content type="text"><![CDATA[问题引入 在搜索tf.cond的使用方法时，找到了这样的一个问题： 运行下面的一段tensorflow代码：12345678910pred = tf.constant(True)x = tf.Variable([1])assign_x_2 = tf.assign(x, [2])def update_x_2(): with tf.control_dependencies([assign_x_2]): return tf.identity(x)y = tf.cond(pred, update_x_2, lambda: tf.identity(x))with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval()) 从代码上看，tf.cond经过判断pred的值对x进行更新。但实际上无论在pred = Ture 还是 False，输出的结果都是2，都是pred = tf.constant(True)的情况。 Confused by the behavior of tf.cond 这是怎么回事呢？ 顺序执行 先不进行解释，有人在回复中给出了一个可以正确运行的代码，看一下有什么区别：12345678910pred = tf.placeholder(tf.bool, shape=[])x = tf.Variable([1])def update_x_2(): with tf.control_dependencies([tf.assign(x, [2])]): return tf.identity(x)y = tf.cond(pred, update_x_2, lambda: tf.identity(x))with tf.Session() as session: session.run(tf.initialize_all_variables()) print(y.eval(feed_dict=&#123;pred: False&#125;)) # ==&gt; [1] print(y.eval(feed_dict=&#123;pred: True&#125;)) # ==&gt; [2] 区别也不大，只是把assign_x_2 = tf.assign(x, [2])这句整体移动到了tf.control_dependencies([tf.assign(x, [2])])的内部。给出的解释是： 如果要让tf.cond()在其中一个分支中执行命令（如分配），你必须在你要传递给的函数创建执行副命令的操作。 If you want to perform a side effect (like an assignment) in one of the branches, you must create the op that performs the side effect inside the function that you pass to . 因为在TensorFlow图中的执行是依次向前流过图形的，所以在任一分支中引用的所有操作必须在条件进行求值之前执行。这意味着true和false分支都接受对tf.assign() op 的控制依赖。 Because execution in a TensorFlow graph flows forward through the graph, all operations that you refer to in either branch must execute before the conditional is evaluated. This means that both the true and the false branches receive a control dependency on the tf.assign() op. 翻译的可能不够准确，大意就是assign_x_2 = tf.assign(x, [2])这句话在tf.cond已经执行过了，因此无论执行update_x_2（让x=2）或lambda: tf.identity(x)（保持x不变），得到的结果都是x=2。这么来看其实是一个很简单的问题，定义时不仅定义了模型，也隐含着定义了执行顺序。 tf.control_dependencies() 这个函数加不加看起来没有什么区别，比如：123456789101112import tensorflow as tf pred = tf.placeholder(tf.bool, shape=[])x = tf.Variable([1])# x_2 = tf.assign(x, [2])def update_x_2(): # with tf.control_dependencies([x_2]): #[tf.assign(x, [2])]): return tf.assign(x, [2])y = tf.cond(pred, update_x_2, lambda: tf.identity(x))with tf.Session() as session: session.run(tf.global_variables_initializer()) print(y.eval(feed_dict=&#123;pred: False&#125;)) # ==&gt; [1] print(y.eval(feed_dict=&#123;pred: True&#125;)) # ==&gt; [2] 去掉之后运行结果和正确的相同。具体作用还是看一下官网吧……直接搜tf.control_dependencies得到的信息并不多： Wrapper for Graph.control_dependencies() using the default graph.See tf.Graph.control_dependencies for more details. 在tf.Graph.control_dependencies这里确实讲得很详细，其作用简单来说就是控制计算顺序。 1234with g.control_dependencies([a, b, c]): # `d` and `e` will only run after `a`, `b`, and `c` have executed. d = ... e = ... 有了这句话，with中的语句就会在control_dependencies()中的操作执行之后运行，并且也支持嵌套操作。在给出的错误例子中，很像开头提出的问题：1234567891011121314# WRONGdef my_func(pred, tensor): t = tf.matmul(tensor, tensor) with tf.control_dependencies([pred]): # The matmul op is created outside the context, so no control # dependency will be added. return t# RIGHTdef my_func(pred, tensor): with tf.control_dependencies([pred]): # The matmul op is created in the context, so a control dependency # will be added. return tf.matmul(tensor, tensor) 上面t操作在tf.control_dependencies之前已经被执行了，因此就无法控制t的先后顺序。如果我们把my_func看作是tf.cond中的分支操作函数，那么很可能在pred更新之前就已经进行了操作，因此可能造成一些错误。 总结 这么一看，好像我自己写的没有注意这么多细节，但目前从结果上看好像还都没什么问题，或许需要重新改写一下。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多任务深度学习论文阅读]]></title>
    <url>%2F2017%2F04%2F11%2Fmulti-task-paper%2F</url>
    <content type="text"><![CDATA[Deep Learning Face Representation by Joint Identification-Verification 这篇论文主要是针对人脸识别，分为两个任务： face identification task face verification task 前者目的是增大类间间距，即不同人的差距；后者是为了减小类内差距，即相同人在不同环境下的差异。 在网络的设计中，最终生成的DeepID2是由conv3和conv4融合得到的，反映了不同层的特征信息。其中conv4设置了权重在神经元间不共享，即不同位置的卷积核不同（locally-connected layer）。 Identification这个任务采用的是传统的softmax n分类器，直接跟在DeepID2后面，以交叉熵为损失函数。 Verification目的是让从同一个人提取的DeepID2特征类似，从而降低类内的差距。其约束条件可以是L1、L2范数或余弦相似度，其计算分别为：值得注意的是，公式中出现了两个图片输入（$f_i$、$fj$），当为同类时，$y{ij}=1$否则为-1，从而提供了类内与类间两种损失。 梯度更新梯度更新有一点疑惑的是最后更新参数不应该是使用参数的梯度去相减吗？ Text-Attentional Convolutional Neural Network for Scene Text Detection 这一篇是讲文字识别（其实是找到哪里是文字）。为了解决这一个看起来简单的问题，加了两个较为复杂的辅助任务。 网络结构 看到这个输入我是有点震惊的，网络输入是32*32的图片，主要任务是输出是否为字符，辅助任务是字符分割和字符分类（感觉是辅助带大哥飞啊= =）。整个网络解决了三个问题：where（区域回归，或是字符分割）、what（字符分类）、whether（是否为字符），其中前两个较为困难，最后的也是最主要的比较简单。 loss设计总体的loss为：针对于每个任务，具体的loss为（这里又觉得少了后括号）：输出分别为： b：text、notext二分类 l：0-9，a-z，A-Z所有字符的62分类 r：分割区域，32*32大小，取值范围{0,1}的二进制mask 辅助任务的只在训练过程中进行，测试中停止。 Embedding Label Structures for Fine-Grained Feature Representation 这一篇主要是对triplet loss的改进，变成了四元组损失。 triplet loss文中说，传统的softmax是 “squeeze” the data from the class into a corner of the feature space 而没有注重类内、类间的关系。triplet loss 可以说又是第一篇论文二输入的一次改进，输入分别为：参考原始图像（$r_i$），与参考图像同类别图像（$p_i$），与参考图像不同类别图像（$n_i$）。并且设置一个边界m，使类似图像的距离（L2范数）加上m小于不同类图像的距离。 其实我觉得右边可以改成$ (\mathscr D(r_i,n_i)+\mathscr D(p_i,n_i))/2$，这样是不是数据利用率高一些呢。最终选择hinge loss： 缺点是如果有N张图片，那么triplet将会有$N^3$量级的三元组合，训练速度很慢。整个任务使用了softmax与triplet的组合： 四元组其实就是把类划得更细了，加入了细粒度的标签与特征。$p^+_i$与原图大类、小类都一样，而$p^-_i$是大类相同，小类不同（颜色、年份等）。公式表示为：loss表示为：]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda 学习 - GPU的归约、扫描、直方图算法]]></title>
    <url>%2F2017%2F04%2F05%2Fcuda-3%2F</url>
    <content type="text"><![CDATA[由于表格画不出来，这一篇没有加入表格。完整版 两种复杂度 Step complexity即步骤复杂度，完成一个工作需要多少步。 Work complexity即工作复杂度，完成工作一共需要的工作量。 对于并行计算，由于可以采取多线程的运算，可以对每一步的运算时间进行很大的缩减。但对于整个程序，有时需要分很多步骤，后续步骤需要等待前面的步骤处理完得到结果才能继续执行。因此有时步骤的复杂度反而决定了整个程序运行的时间。 Reduce 归约归约的输入如下： 一组元素集合 可归约的运算符 什么是可归约的运算符呢？需要满足两个要求：二元性和结合律。所谓二元性，是运算符是二元运算符，对两个元素进行操作。加减乘除，逻辑与、逻辑或，比较大小都属于二元运算符。结合律是满足：$(a+b)+c=a+(b+c)$的运算符，加减乘是，除法不是。因为$(8/2)/4 = 1 \neq 8/(2/4)=16$。 用简单的函数表示为：reduce[(1,2,3),+]=6。在上图中，如果是传统的串行计算，那么总的计算量为7，耗时为7；当我们使用并行计算时，可以先把邻近的相加，并重复操作，直到得到最后的输出。这样总的计算量为7，而耗时仅为3，缩短了一大半的时间！ shared memory调用之前说到的调用内核函数，除了平时串行程序的输入参数，还需要告诉GPU启动的block与thread数。其实还可以告诉GPU shared memory大小，调用时将其作为第三个参数。1shmem_kernel&lt;&lt;&lt;blocks, threads, Mem&gt;&gt;&gt; 在线程内，需要使用1extern __shared__ float sdata[]; 来获取相应的空间。 Scan 扫描扫描和归约很像，简单来说扫描是多次的归约。扫描需要有三个输入： 一组元素集合 可扫描的运算符 标识元素（identity element） 可扫描的运算符与归约的要求相同，也是二元性和结合律。第三点identity element是指用该元素作为输入，与任何值进行给定的运算符操作，得到的仍是该值。即：$\text{[I op a =a]}$对于加法来说，0是标识元素，因为0加上任何值不改变大小；对于乘法来说，1是标识元素；对于取小值来说，在unsigned char型中，0xFF是标识元素。 标识元素是第一点扫描与归约不同的地方，第二点在于输出。归约输出是一个值，而扫描输出是与输入相同大小的一个数组。具体还可以分为exclusive和inclusive，区别在于输出第一个元素是否为标识元素。Step complexity 为$O(log(n))$，Work complexity 为$O(n^2)$。 Hills Steele ScanHills Steele Scan 是一种优化Step complexity的算法，第一步是相邻求和放入下一栏，接着间隔1位求和，然后间隔两位求和……以此类推。Step complexity 为$O(log(n))$，Work complexity 为$O(n^2)$。 Blelloch Scan这是一种优化Work complexity的算法，比上面的要复杂一些。主要分为reduce和downsweep两步。Step complexity 为$O(2log(n))$，Work complexity 为$O(n)$。downsweep的操作方法如图右下角，L、R为上一行两个输入，R镜像到左下角值不变，右下角为L、R之和。 Histogram直方图在图像中也是经常用到，比如灰度直方图反映了灰度的分布情况，能从整体把握图像的亮暗、对比度信息。传统的串行直方图统计，需要遍历每一个像素，然后对应的统计灰度值加1，效率较低，而并行计算中有三种算法。 atomic上节中讲到过这个方法。如果每个线程负责一个像素，独自相加，最大的问题就是内存访问与修改。通过atomicadd可以很好的解决这个问题，但是缺点是速度较慢。 local histogram每个线程负责一部分的图像区域，无需使用atomic。之后再进行归约操作变成全局直方图。 sort这种方法目前还没有具体说明，只是了解一下。]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda 学习 | GPU硬件与并行通信模式]]></title>
    <url>%2F2017%2F03%2F31%2Fcuda-2%2F</url>
    <content type="text"><![CDATA[通信方式通信方式主要以课程截图为主…… Map这是一种一一对应的方式。 Gather多对一的方式。 Scatter一对多的方式。 Stencil模板，多对多的方式。图中左中为输入，左下为输出，不同颜色为不同线程的读取、输出位置。 Transpose转置操作，改变形状、顺序等。进行合理的顺序改变在数据读取速度上会提升速度。 GPU结构从大到小来说，结构为：Kernel -》 Block -》 Thread硬件上GPU有很多流处理器（streaming multiprocessors），GPU给这些sm分配block，同一个sm可能运行多个block。GPU的优势在于多个线程同时工作，但缺点是无法知道这些线程执行的先后顺序。可以确定的有： 线程同时在同一个SM运行 所有kernel中的blocks在下个kernel启动前结束 内存关系每个Thread都有自己的local mem。每个Block有sheared mem，其中的Thread均可以访问。声明时需要加上__shared__前缀。global mem可以被不同Block的访问。 Barrier同步为了避免因Thread执行不同步带来的数据存储问题，有时候需要等待所有的Thread都执行结束再继续执行。这时需要在kernel中加入__syncthreads();进行同步。比如在分线程读取同一数组其他位置并修改当前位置时，需要在读取后同步并存入临时变量。在写操作后也要同步。对于两个kernel间可以不加同步，默认存在隐式同步。 优化GPU的优化目标是吞吐量，及最大化计算强度（Maximize arithmetic intensity）：$$\frac{\text{math we do}}{\text{memory we access}}$$从分子上，要最大化每个线程上的操作；从分母上，要减小对内存访问的时间。这就要求需要把频繁读取的数据放在更快的memory上。从速度上来说，mem的排序为：local &gt; shared &gt;&gt; global &gt;&gt; host mem这是因为local存在寄存器或L1缓存中。 Coalesce在读取数据时，GPU对于连续的数据有高效的访问效率。 atomic 操作为了避免内存访问读取冲突，也可以使用cuda中的atomic操作。比如对于相加的操作，可使用：atomicAdd(&amp;g[i],1)代替g[i]=g[i]+1。不过这种操作存在局限，只支持部分运算类型和部分数据类型，并且会降低运算速度。 divergence发散发散出现在内核中非顺序的操作，如判断、循环。这会导致不同线程运行的时间明显不同。加入同步会让所有线程完全结束后继续执行，耗费时间较多。 总结 homework这次作业比上次麻烦点，不过也有很多收获。 图像处理中统一的行列定义作业里recombineChannels函数是已经给定的，需要从这里得到有关的行列信息。而我一开始定义的gridSize把行列的顺序定义反了。 grid、block、thread的区别官网上有相关的介绍：Kernel。另外Value of threadidx.x (.y, .z), blockidx.x etc. in CUDA也给出了blockDim、gridDim、threadIdx、blockIdx之间的关系。Dim是定值，Idx是变量，一般来说0 &lt;= Idx &lt; Dim = Dim_define(是通过dim3 定义的定值)。为了取得某一维度的线程号，可以使用 1int i = blockIdx .x * blockDim .x + threadIdx .x; 最大化thread以提高速度同样的总线程数运行的速度一样吗？之前我是认为一样的，是因为我觉得不同块（block）间的线程（thread）也是同步执行的，只要&lt;&lt;&lt; numBlocks，threadsPerBlock &gt;&gt;&gt;中两者之积相同即可。但经过实验，发现其实差距很大。在dim3 numBlocks(numCols, numRows)，dim3 threadsPerBlock(1)的情况下，速度大约为58ms；在dim3 numBlocks(numCols, numRows)，dim3 threadsPerBlock(2)的情况下，速度大约为31ms；在dim3 numBlocks(numCols, numRows)，dim3 threadsPerBlock(4)的情况下，速度大约为17ms；在dim3 numBlocks(numCols, numRows)，dim3 threadsPerBlock(4，4)的情况下，速度大约为8ms；在dim3 numBlocks(numCols/4+1, numRows/4+1)，dim3 threadsPerBlock(4，4)的情况下，速度大约为5ms；在dim3 numBlocks(numCols/32+1, numRows/32+1)，dim3 threadsPerBlock(32，32)的情况下，速度大约为1.4ms。由此可见，不同block之间并不是一种完全的并行关系，加快速度一定要加大thread的数目！在官方的示例中： 123456789int main() &#123; ... // Kernel invocation dim3 threadsPerBlock(16, 16); dim3 numBlocks(N / threadsPerBlock.x + 1, N / threadsPerBlock.y + 1);//官方没有+1操作 MatAdd&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(A, B, C); ...&#125; 线程数是固定的256（16*16），而块数是可变的，根据数据大小不同而进行变化。既然加大线程可以加快速度，把所有的都用上不就可以了吗？实际上试验中最下面的dim3 threadsPerBlock(32，32)已经达到了目前线程上限1024，速度的确较一开始的58ms快了40倍。当然在图像处理中线程也要注意访问越界的问题，得到线程负责的像素位置之后，先进行判断，越界退出即可：12if (thread_2D_pos.x &gt;= numCols || thread_2D_pos.y &gt;= numRows) return;]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cuda 并行计算 | GPU 编程模型]]></title>
    <url>%2F2017%2F03%2F30%2Fcuda-1%2F</url>
    <content type="text"><![CDATA[udacity上的课程，有nvidia的工程师上课，比较基础也比较易懂。 CUDA程序的特点相比于CPU的单线程串行计算，CUDA程序的多线程对速度提升有很大的作用。这就是优化时间与优化吞吐量的区别。 程序编译后分别在CPU和GPU上运行； CPU是主机（host），GPU是从机（device）； 各自有各自的存储位置，不能相互访问。 GPU不能发起运算，只能相应运算 CUDA程序的执行步骤 CPU在GPU上申请空间cudaMalloc(起始地址,大小) CPU将数据从内存拷贝到显存cudaMemcpy(源,目标,大小,方向) CPU启动GPU上的内核进行计算kernel_name &lt;&lt;&lt;blocks，threads&gt;&gt;&gt;(函数参数) CPU将处理结果从显存拷贝到内存cudaMemcpy(源,目标,大小,方向) kernel 函数123456_global_ void square(float *d_in, float *d_out)&#123; int idx = threadIdx.x; float f = d_in[idx]; d_out[idx] = f * f;&#125; 对于每个kernel，其计算流程类似于串行计算。 Block 与 Thread在CPU启动GPU上的内核进行计算时，使用的是kernel_name &lt;&lt;&lt;blocks，threads&gt;&gt;&gt;，定义了blocks，threads的大小。这两者可以是1、2或是3D的结构，代表使用多少块，每块线程数目。数据结构为dim(x,y,z)，对于一维w等价于dim3(w)也等价于dim3(w,1,1)。总的线程数为二者的乘积。对于每个线程，其索引号是比较重要的，访问方法有： threadIdx blockDim blockIdx 等。 映射是一种数据与方法的关系（其实感觉就是函数）。Map(Elements, Function)元素为待处理的数据集合，Function为对每个元素处理的方法。 彩图转灰度图这是lesson 1的homework，不是很难。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374// Homework 1// Color to Greyscale Conversion//A common way to represent color images is known as RGBA - the color//is specified by how much Red, Green, and Blue is in it.//The 'A' stands for Alpha and is used for transparency; it will be//ignored in this homework.//Each channel Red, Blue, Green, and Alpha is represented by one byte.//Since we are using one byte for each color there are 256 different//possible values for each color. This means we use 4 bytes per pixel.//Greyscale images are represented by a single intensity value per pixel//which is one byte in size.//To convert an image from color to grayscale one simple method is to//set the intensity to the average of the RGB channels. But we will//use a more sophisticated method that takes into account how the eye //perceives color and weights the channels unequally.//The eye responds most strongly to green followed by red and then blue.//The NTSC (National Television System Committee) recommends the following//formula for color to greyscale conversion://I = .299f * R + .587f * G + .114f * B//Notice the trailing f's on the numbers which indicate that they are //single precision floating point constants and not double precision//constants.//You should fill in the kernel as well as set the block and grid sizes//so that the entire image is processed.#include "reference_calc.cpp"#include "utils.h"#include &lt;stdio.h&gt;__global__void rgba_to_greyscale(const uchar4* const rgbaImage, unsigned char* const greyImage, int numRows, int numCols)&#123; //TODO //Fill in the kernel to convert from color to greyscale //the mapping from components of a uchar4 to RGBA is: // .x -&gt; R ; .y -&gt; G ; .z -&gt; B ; .w -&gt; A // //The output (greyImage) at each pixel should be the result of //applying the formula: output = .299f * R + .587f * G + .114f * B; //Note: We will be ignoring the alpha channel for this conversion int ind_x = blockIdx.x; int ind_y = blockIdx.y; uchar4 pixel_in = rgbaImage[ind_x * numCols +ind_y]; unsigned char R = pixel_in.x; unsigned char G = pixel_in.y; unsigned char B = pixel_in.z; unsigned char output = .299f * R + .587f * G + .114f * B; greyImage[ind_x * numCols +ind_y] = output; //First create a mapping from the 2D block and grid locations //to an absolute 2D location in the image, then use that to //calculate a 1D offset&#125;void your_rgba_to_greyscale(const uchar4 * const h_rgbaImage, uchar4 * const d_rgbaImage, unsigned char* const d_greyImage, size_t numRows, size_t numCols)&#123; //You must fill in the correct sizes for the blockSize and gridSize //currently only one block with one thread is being launched const dim3 blockSize(1, 1, 1); //TODO const dim3 gridSize( numRows, numCols, 1); //TODO rgba_to_greyscale&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(d_rgbaImage, d_greyImage, numRows, numCols); cudaDeviceSynchronize(); checkCudaErrors(cudaGetLastError());&#125; Intro to Parallel Programming]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从零开始配置深度学习环境：ubuntu16.04 cuda opencv caffe 需要的库]]></title>
    <url>%2F2017%2F03%2F29%2Fdeepenv-config%2F</url>
    <content type="text"><![CDATA[有一台空闲的服务器，上面有一块K40的卡，原来的系统进不去了可以拿来搞一搞。。 nvidia驱动这一步好像可以跳过，因为之后安装cuda能选择是否安装驱动。上官网NVIDIA Driver Downloads找自己显卡的型号，看看适合的驱动编号是什么。之后输入命令：12345sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get update sudo apt-get install nvidia-375 sudo apt-get install mesa-common-dev sudo apt-get install freeglut3-dev 之后重启系统让显卡驱动生效。但是，Ubuntu自带的驱动更新也可以完成这样的操作而且不用输命令！点左边栏的设置，下面找到“Software &amp; Updates”，在点上面的“Additional Drivers”，会发现会有一个显卡的驱动，恰好和要找的版本相同~选中之后确认更新即可。其实这也是碰巧，这里面只识别出了Quadro K600，和K40的驱动型号相同，但如果是其他的卡，也需要自己确认一下。 CUDA还是先进网页下载CUDA。我选的配置如下：之后按照要求，运行：1sudo sh cuda_8.0.61_375.26_linux.run 之后会有三个提示： 是否安装驱动（装过的不要装） 是否安装CUDA（yes） 是否安装sample（看自己） 结束之后会有摘要：1234567891011=========== = Summary = ===========Driver: Not Selected Toolkit: Installed in /usr/local/cuda-8.0 Samples: Not Selected Please make sure that – PATH includes /usr/local/cuda-8.0/bin – LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as root 接下来是添加到环境变量。首先打开文件：1sudo vim /etc/profile 在最下面添加两行：12export PATH=/usr/local/cuda-8.0/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:$LD_LIBRARY_PATH 至此安装结束~ cudnn1.下载cuDNN压缩包；2.对下载文件进行解压：1tar -zxvf cudnn-8.0-linux-x64-v5.1.tgz 3.解压后会看到一个cuda文件夹，里面包含了include以及lib64两个子目录。我们需要做的就是将这两个字母里面的文件复制到cuda对应的安装目录。这里以cuda的安装目录为/usr/local/cuda/，这个目录下也会包含include/以及lib64/这两个文件夹，将之前目录的文件复制过来即可。12sudo cp -p cuda/include/cudnn.h /usr/local/cuda/includesudo cp -p cuda/lib64/* /usr/local/cuda/lib64/ 远程桌面 How to Remote Access to Ubuntu 16.04 from Windows 在windows通过VNC进行远程桌面连接。首先在服务器上打开桌面分享：如图所示勾选，即可通过vnc访问。 如果有连接错误，可以尝试以下方式解决：运行dconf-editor（没有的话先安装sudo apt install dconf-editor）。导航至：org -&gt; gnome -&gt; desktop -&gt; remote-access，反选“require-encryption”即可。 opencv下载源码，解压。可以先在目录下建立release文件夹作为输出目录。我的编译参数为：1cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local -D WITH_TBB=ON -D BUILD_NEW_PYTHON_SUPPORT=ON -D WITH_V4L=ON -D WITH_OPENGL=ON -D WITH_CUBLAS=ON -D WIHT_CUDA=ON -D WITH_OPENGL=ON .. 之后12make -jsudo make install Protocol Buffers v3 安装caffe需要这个东西。。到github上下载源码。解压之后进入目录，在Ubuntu上先需要确保前置程序的安装：1sudo apt-get install autoconf automake libtool curl make g++ unzip 安装之后，再运行脚本：1./autogen.sh 这样才会生成configure文件。之后12345./configuremakemake checksudo make installsudo ldconfig # refresh shared library cache. MKL 安装去官网下载，会让你留一个邮箱。Intel® Math Kernel Library (Intel® MKL)Build date: 16 Feb 2017下载之后解压，运行1sudo ./install.sh 按着提示继续走，默认安装就行。结束后添加环境目录：1vim /etc/profile 在末尾处加入代码：12export PATH=":/opt/intel/bin:$PATH"export LD_LIBRARY_PATH=$LD_LIBRARY_LIB:/opt/intel/lib/intel64:/opt/intel/mkl/lib/intel64 退出，使设置生效。 1source /etc/profile boost库下载boost库，本文使用的是1.58版本。解压：1tar --bzip2 -xvf boost_1_58_0.tar.bz2 进入解压后的文件夹，运行：1./bootstrap.sh 如果失败尝试使用sudo sh ./bootstrap.sh之后会在当前目录下生成bjam文件，继续编译。。。 1./bjam 编译时间比较长将生成的库默认安装到 /urs/local/lib 目录 1./bjam install 其实整个过程中没有什么出错的地方，不过不知道怎么回事电脑运行很慢，为这还重装一回。而且有开机不显示自检的情况。师兄帮忙插拔了内存好像解决了，原理不是很懂。。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>tensorflow</tag>
        <tag>cuda</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mask RCNN 论文阅读]]></title>
    <url>%2F2017%2F03%2F26%2Fmaskrcnn%2F</url>
    <content type="text"><![CDATA[mask rcnn 是对Faster R-CNN的功能上的提升，速度上仍然在200ms（5fps）。 Faster R-CNN回顾Faster R-CNN由两个阶段组成。 第一阶段为RPN网络，提出候选对象bounding boxes。第二阶段，本质上是Fast R-CNN，从每个候选框中提取使用RoIPool的特征，并执行分类和边界框回归。 Mask R-CNN特点Mask R-CNN在概念上很简单，与Faster R-CNN前相同只是又多加了一个输出：Faster R-CNN每个候选对象有两个输出，类标签（label）和边框偏移（bounding-box offset）; 为提高精度，又添加了输出对象mask（二进制掩码）的第三个分支。但附加的Mask输出与类和框输出不同，需要提取对象的更精细的空间布局。同时，分类也取决于掩模预测。 损失上，在训练期间，将每个抽样RoI的多任务丢失定义为L = L{cls} + L{box} + L{mask}。 分类损失L{cls}和bounding-box损失L{box}与Fast R-CNN中定义的相同。mask分支对于每个RoI具有Km^2维输出，即K个分辨率m×m的二进制掩模编码，每个对应K中某个类别。 为此，对每个每像素使用sigmoid，并将L{mask}定义为平均二进制交叉熵损失（the average binary cross-entropy loss）。对于与ground-truth类k相关联的RoI，L_{mask}仅在第k个掩码上定义（其他掩码输出不会造成损失）。 与FCN我们对$L_{mask}$的定义允许网络为每个类生成掩码，而不会在类之间有竞争；我们依靠专用分类分支预测用于选择输出掩码的类标签。这将隔离mask和类预测输出。 这与将FCN应用于通常使用每像素softmax和多项交叉熵损失的语义分割的常见做法不同。 在这种情况下，类上的mask存在竞争；在我们这种情况下，使用每像素的sigmoid和二进制损失，不会存在mask间的竞争。 通过实验显示，这种方案是良好的实例分割结果的关键。 Mask的表示一个mask对输入对象的空间布局进行编码，如图1。 因此，无需像全链接一样reshape，可以通过卷基层进行运算输出。具体来说，我们使用FCN预测每个RoI的m×m掩码。 这允许掩模分支中的每个层保持显式的m×m对象空间布局，而不会将其折叠成缺少空间维度的向量（fc）表示。完全卷积表示需要更少的参数，并且如实验所证明的更准确。 这种像素到像素的行为需要我们的RoI特征，它们本身是小的特征图，以便良好地对齐以保留显式的每像素空间对应（应该是得到的mask与物体的像素对其）。 这促使我们开发出在掩模预测中发挥关键作用的以下RoIAlign层。 RoIAlignRoIAlign层是对RoIPool的改进，用于可以消除RoIPool的严格量化。其实提出的改变很简单：为了避免了RoI边界或块的任何量化（即我们使用x / 16而不是[x / 16]）。我们使用双线性插值来计算每个RoI中四个定期采样位置的输入特征的精确值，并将结果聚合（使用最大值或平均值）。如展示的那样，RoIAlign带来了很大的改进。其实这一部分就是对feature map的插值计算。 网络结构效果最好的结构是ResNeXt + FPN。这两部分也有其各自的作用： 用于整个图像上的特征提取的卷积网络结构 分别应用于每个RoI的边界盒识别（分类和回归）和掩模预测的网络头。 是下图右边的结构： 总结这是把图像分割运用到了检测里来，算是检测功能的进一步增强。之前我就觉得这两者之间其实还是比较类似的，更进一步从检测做到分割也是理所当然。其实最大的创新应该是分割中使用了非竞争的sigmoid代替了softmax，但具体各部分哪一点是最关键的，最提升性能的，只凭想也不能确认。以上是初步的阅读，可能有一些错误，希望与大家多交流。 S. Xie, R. Girshick, P. Doll´ar, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. arXiv:1611.05431, 2016.T.-Y. Lin, P. Doll´ar, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. arXiv:1612.03144, 2016.]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow多任务学习]]></title>
    <url>%2F2017%2F03%2F15%2Ftensorflow-multitask%2F</url>
    <content type="text"><![CDATA[之前在caffe上实现了两个标签的多任务学习，如今换到了tensorflow，也想尝试一下，总的来说也不是很复杂。 建立多任务图多任务的一个特点是单个tensor输入(X)，多个输出(Y_1,Y_2...)。因此在定义占位符时要定义多个输出。同样也需要有多个损失函数用于分别计算每个任务的损失。具体代码如下：123456789101112131415161718192021222324252627282930313233# GRAPH CODE# ============# 导入 Tensorflowimport Tensorflow as tf# ======================# 定义图# ======================# 定义占位符X = tf.placeholder("float", [10, 10], name="X")Y1 = tf.placeholder("float", [10, 20], name="Y1")Y2 = tf.placeholder("float", [10, 20], name="Y2")# 定义权重initial_shared_layer_weights = np.random.rand(10,20)initial_Y1_layer_weights = np.random.rand(20,20)initial_Y2_layer_weights = np.random.rand(20,20)shared_layer_weights = tf.Variable(initial_shared_layer_weights, name="share_W", dtype="float32")Y1_layer_weights = tf.Variable(initial_Y1_layer_weights, name="share_Y1", dtype="float32")Y2_layer_weights = tf.Variable(initial_Y2_layer_weights, name="share_Y2", dtype="float32")# 使用relu激活函数构建层shared_layer = tf.nn.relu(tf.matmul(X,shared_layer_weights))Y1_layer = tf.nn.relu(tf.matmul(shared_layer,Y1_layer_weights))Y2_layer = tf.nn.relu(tf.matmul(shared_layer,Y2_layer_weights))# 计算lossY1_Loss = tf.nn.l2_loss(Y1-Y1_layer)Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer) 用图表示出来大概是这样的： Shared_layer的输出分别作为Y1、Y2的输入，并分别计算loss。 训练有了网络的构建，接下来是训练。有两种方式： 交替训练 联合训练 下面分别讲一下这两种方式。 交替训练这次先放图，更容易理解：选择训练需要在每个loss后面接一个优化器，这样就意味着每一次的优化只针对于当前任务，也就是说另一个任务是完全不管的。123# 优化器Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss) 在训练上面我一开始也有些疑惑，首先是feed数据上面的，是否还需要同时把两个标签的数据都输入呢？后来发现的却需要这样，那么就意味着另一任务还是会进行正向传播运算的。123456789101112131415161718192021222324# Calculation (Session) Code# ==========================# open the sessionwith tf.Session() as session: session.run(tf.initialize_all_variables()) for iters in range(10): if np.random.rand() &lt; 0.5: _, Y1_loss = session.run([Y1_op, Y1_Loss], &#123; X: np.random.rand(10,10)*10, Y1: np.random.rand(10,20)*10, Y2: np.random.rand(10,20)*10 &#125;) print(Y1_loss) else: _, Y2_loss = session.run([Y2_op, Y2_Loss], &#123; X: np.random.rand(10,10)*10, Y1: np.random.rand(10,20)*10, Y2: np.random.rand(10,20)*10 &#125;) print(Y2_loss) 由此看来这种方法效率还是有点低。 联合训练两个优化器需要分别训练，我们把他俩联合在一起，不就可以同时训练了吗？原理很简单，把两个loss相加即可。得到的图是这样的：代码：12345678910111213141516171819202122232425# 计算LossY1_Loss = tf.nn.l2_loss(Y1-Y1_layer)Y2_Loss = tf.nn.l2_loss(Y2-Y2_layer)Joint_Loss = Y1_Loss + Y2_Loss# 优化器Optimiser = tf.train.AdamOptimizer().minimize(Joint_Loss)Y1_op = tf.train.AdamOptimizer().minimize(Y1_Loss)Y2_op = tf.train.AdamOptimizer().minimize(Y2_Loss)# 联合训练# Calculation (Session) Code# ==========================# open the sessionwith tf.Session() as session: session.run(tf.initialize_all_variables()) _, Joint_Loss = session.run([Optimiser, Joint_Loss], &#123; X: np.random.rand(10,10)*10, Y1: np.random.rand(10,20)*10, Y2: np.random.rand(10,20)*10 &#125;) print(Joint_Loss) 这是原文的代码，其中定义的Y1_op和Y2_op并没有使用，应该是多此一举了。 如何选择？什么时候交替训练好？ Alternate training is a good idea when you have two different datasets for each of the different tasks (for example, translating from English to French and English to German). By designing a network in this way, you can improve the performance of each of your individual tasks without having to find more task-specific training data. 当对每个不同的任务有两个不同的数据集（例如，从英语翻译成法语，英语翻译成德语）时，交替训练是一个好主意。通过以这种方式设计网络，可以提高每个任务的性能，而无需找到更多任务特定的训练数据。 这里的例子很好理解，但是“数据集”指的应该不是输入数据X。我认为应该是指输出的结果Y_1、Y_2关联不大。 什么时候联合训练好？交替训练容易对某一类产生偏向，当对于相同数据集，产生不同属性的输出时，保持任务的独立性，使用联合训练较好。 这两种方式在实际中也成功实现了，不过目前准确率还不是很高，有待改进。 Multi-Task Learning in Tensorflow: Part 1multi-task-part-1-notebook]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[影响力 - 互惠]]></title>
    <url>%2F2017%2F03%2F11%2Finfluence1%2F</url>
    <content type="text"><![CDATA[周末没事看一下书，感谢实验室的师姐~ 《影响力（经典版）》 [Influence] 出版社： 万卷出版公司 ISBN：9787547012123 首先选择这本书吧，慢慢看，目前看完了第二章互惠，暂时总结一下。 扫码送礼这个是前一段时间最常见的一种营销方式了。在学校里、商场前，都会有一个小摊位，宣称只要扫描二维码，或是下载应用，或是关注微信，就可以领取礼物，包括玩偶到被吹上天的50度杯。现场也是非常热闹，经常挤满了人。其实这种方式就是利用了互惠的原理。我从来没有参加过这样的活动，至少我是抱着一种他们以后的收入肯定要比目前的投入多，当你想偿还这种互惠的愧疚时，一定会替他人（即收了礼品却没有在后续使用服务甚至付费的用户）一并偿还的。羊毛出在羊身上嘛。既然既不想偿还，又不想被戴上爱占小便宜的帽子，干脆一开始就不要了。 政治互惠政治、商业互惠这一方面，比扫码送礼要复杂的多，并且很难避免。根本原因是你身处在这个圈子之中，你的接受与拒绝，都会被他人看在眼中，并且能够流传开来；不像扫码那样，你可能根本不认识向你推销的人，也不了解推销的业务，置之不理、占小便宜都不会在将来对你产生什么大的影响。因此官商勾结这些行为十分容易被理解，不过另一方面，也可以说这里把互惠发挥到了极致，极致到了尽头，可能就无法相互剥离，成了一条绳上的蚂蚱。 互惠营销在中国？作为一种营销手段，我觉得这种方式在中国用的最好的是在老年人身上：某公司在小区举办活动，某天某时向大家发礼物。然后一群老年人就会蜂拥而至。那些营销人员依次把锅碗瓢盆拿出来，分发给大家，说这个多少钱，那个多少钱，这些都是免费的。过一会再拿出一些东西，却说我们也不容易，最后这些卖给大家，希望体谅一下……结果就是，大家抱着一些高价买来却质量不高的锅碗瓢盆高高兴兴回家了。当然我也在网上见过其他结局：当听说要钱时，立刻就走了。但是老年人这么做的应该不是很多（要不就不会有这种活动了）。个人感觉一方面老年人经济方面没有什么忧虑，另一方面也是为了内心的“不亏欠”。其实我觉得这些互惠的营销，只不过是一种商业行为，还是不要过于放在心上。既然是商业行为，那么自然有成功也有失败。当没人愿意付钱来报偿时，就要想想这个活动是不是不吸引人，所赠与的礼物是否不合适。找找商家的毛病，不要过于让自己愧疚，毕竟不是你亏就是商家亏，这个选择是在自己的。理想的情况是互惠，但还是我之前说的，这种活动如果前期商家投入过多，当你想偿还这种互惠的愧疚时，一定会替他人一并偿还的。 多方互惠这里就不是商业上了。看过一个公益广告，是说一个小伙子帮助别人，别人得到帮助又去帮助另外的人，最终转了一圈，这个小伙子也受到了他人的帮助。别人帮助你，你心存感激，但又并不一定非要去马上帮助他，而是将帮助传递给更需要的人。这反映的是一种多方的互惠，是一种人性的善良。但商业上彼此大多是竞争关系，在没有利益的情况下很难完成多方互惠。 先来后到这里考虑的是日常人际关系上的。熟人应该是介于“扫码营销”与“政治互惠”之间的一种情况：既不是以后可以不相往来，也不是利益关系紧密。这时候互惠关系很重要，也很难衡量相互的利益大小。有了一次收到的“惠”，人情债就已经欠下了，至于什么时候还，什么方式还，往往是不确定的。这样的话看来较早行动的更有优势，更掌握主动权。当然这种情况下，受“惠”一般也会及时偿还，确保双方的利益平衡。 让步式的互惠这种方式就高明了一点，其主要作用是让人接受。一开始可以拿一个较为珍贵物品，当别人不接受的时候再换一个正常价值的物品。当然这里应该不会用到营销上吧。。总之，经过了前后对比，就很难拒绝了。其实拒绝也会让人感到一种愧疚感，连续的拒绝可能让别人觉得自己难以沟通交流，因此这种方式较为高（jiao）明（hua）。]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>书</tag>
        <tag>影响力</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorBoard 在1.0 版本后的使用]]></title>
    <url>%2F2017%2F03%2F08%2Ftensorboard%2F</url>
    <content type="text"><![CDATA[注意：在阅读本文之前，请务必更新你的浏览器。Chrome大法好！数据、模型可视化是TensorFlow的一项重要的功能，安装后自带的TensorBoard是一个很强大的工具，但目前的教程大多都停留在TensorFlow 1.0 版本之前，一些函数已经改名无法使用，因此写一篇比较新的使用说明。 主要区别如果之前使用过TensorBoard，其实只是换一下函数名就可以了。在Github上新版本说明文档中，已经有了对这一方面的说明： Replace tf.scalar_summary, tf.histogram_summary, tf.audio_summary, tf.image_summary with tf.summary.scalar, tf.summary.histogram, tf.summary.audio, tf.summary.image, respectively. The new summary ops take name rather than tag as their first argument, meaning summary ops now respect TensorFlow name scopes. 也就是说，summary独立出来了，以前tf.XXX_summary这样的下划线变成了tf.summary.XXX的格式。 数据可视化对于标量如果我们想对标量在训练中可视化，可以使用tf.summary.scalar()，比如损失loss：12loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction),reduction_indices=[1])) tf.summary.scalar('loss',loss) 得到一个loss的summary。 对于参数应使用tf.summary.histogram()，如全链接的权重：1tf.summary.histogram("/weights",Weights) merge并运行就像变量需要初始化一样，summary也需要merge：1merged = tf.summary.merge_all() 之后定义一个输出器记录下在运行中的数据：1writer = tf.summary.FileWriter("output/",sess.graph) 最后记得在训练过程中执行这两个模块：12345for i in range(1000): sess.run(train_step,feed_dict=&#123;xs:x_data,ys:y_data&#125;) if i%50==0:# 50次记录一次 result = sess.run(merged,feed_dict=&#123;xs:x_data,ys:y_data&#125;) writer.add_summary(result,i) TensorBoard 运行安装TensorFlow时已经自带TensorBoard，如果直接在命令行中输入tensorboard而没有对应指令，可以从安装目录下执行：1python ~/.local/lib/python2.7/site-packages/tensorflow/tensorboard/tensorboard.py --logdir=output/ 运行成功后，会显示：1(You can navigate to http://XXX.XXX.XXX.XXX:6006) 然后在浏览器中输入这个地址即可。 注意IE以及低版本的Chrome都对TensorBoard不兼容（firefox据说也不好用），会出现白屏或者点开loss图没有内容的情况。因为用的台式电脑，之前用的人装了360，我也就继续用了，结果在这里纠结了很久…… 行了，这就成功了~ 现在越学越觉得TensorFlow复杂了。。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 新手入门]]></title>
    <url>%2F2017%2F03%2F07%2Ftensorflow%2F</url>
    <content type="text"><![CDATA[刚装上TensorFlow，还是不太会用，主要去官网还要翻墙太麻烦了。。随手翻一下教程备用 初识TensorFlow初期准备： 安装好TensorFlow 知道如何在Python中编程 懂一点数组知识 最好了解机器学习（不必要） TensorFLow提供多种APIs，从低级到高级，满足不同使用需求，越高级越容易学习和使用。下面的一些模型都可以用tf.contrib.learn高级API实现。 TensorsTensorFlow最重要的数据单元就是tensor（张量）。一个tensor包括了任意维度的数组的原始值。tensor的rank代表其维度，如：12343 # a rank 0 tensor; this is a scalar with shape [][1. ,2., 3.] # a rank 1 tensor; this is a vector with shape [3][[1., 2., 3.], [4., 5., 6.]] # a rank 2 tensor; a matrix with shape [2, 3][[[1., 2., 3.]], [[7., 8., 9.]]] # a rank 3 tensor with shape [2, 1, 3] 直观感觉有几层[]括号就有几个rank。而shape是从括号外向里，数,的个数。 TensorFlow 核心教程 导入TensorFlowpython下基本的导入声明：1import tensorflow as tf 大多数的文档都假设已经导入了tf模块。 计算图（The Computational Graph）你可以把TensorFlow核心程序想成两个独立的模块： 搭建计算图 运行计算图 计算图computational graph是一系列布置为节点图的TensorFlow操作。每个节点将0个或更多张量作为输入并产生一个张量作为输出。常数是其中一个节点类型，他没有输入，输出其储存的常量，建立两个浮点数张量node1和node2：123node1 = tf.constant(3.0, tf.float32)node2 = tf.constant(4.0) # also tf.float32 implicitlyprint(node1, node2) 打印为：1Tensor("Const:0", shape=(), dtype=float32) Tensor("Const_1:0", shape=(), dtype=float32) 注意其输出并不是3.0和4.0。相反，他们是节点，当被评价（when evaluated）时，就会输出3.0和4.0。为了确切评价节点，我们必须用session运行一个计算图。一个session封装了TensorFlow运行时的控件和状态。 下面是一个使用Session运行的例子：12sess = tf.Session()print(sess.run([node1, node2])) 得到输出：1[3.0, 4.0] 我们可以将Tensor节点与运算（运算操作也是节点）结合搭建更为复杂的计算图。比如我们可以将两个常数相加：123node3 = tf.add(node1, node2)print("node3: ", node3)print("sess.run(node3): ",sess.run(node3)) 输出：12node3: Tensor("Add_2:0", shape=(), dtype=float32)sess.run(node3): 7.0 TensorFlow提供了一个名为TensorBoard的工具用来显示计算图，上面的计算过程可以可视化表示为下图： 由于输入是常数，这张图的输出结果是恒定的。一个图可以参数化为接受外部输入，称为placeholders（占位符），用于为之后的数据占取空间。 123a = tf.placeholder(tf.float32)b = tf.placeholder(tf.float32)adder_node = a + b # + provides a shortcut for tf.add(a, b) 下面三行有点像一个函数或一个lambda，其中我们定义两个输入参数（a和b），然后对它们进行操作。 我们可以通过使用feed_dict参数指定为这些占位符提供具体值的Tensors，使用多个输入来评估此图：12print(sess.run(adder_node, &#123;a: 3, b:4.5&#125;))print(sess.run(adder_node, &#123;a: [1,3], b: [2, 4]&#125;)) 输出的结果：127.5[ 3. 7.] 在TensorBoard，图看起来是这样的：我们可以使用计算图做一些更复杂的操作：12add_and_triple = adder_node * 3.print(sess.run(add_and_triple, &#123;a: 3, b:4.5&#125;)) 结果：122.5 可视化之后：在机器学习中，我们通常需要一个可以接受任意输入的模型，例如上面的模型。 为了使模型可训练，我们需要能够修改图以获得具有相同输入的新输出。 Variables（变量）允许我们向图中添加可训练的参数。 它们使用类型和初始值构造：1234W = tf.Variable([.3], tf.float32)b = tf.Variable([-.3], tf.float32)x = tf.placeholder(tf.float32)linear_model = W * x + b 当调用tf.constant时，常量被初始化，它们的值永远不会改变。 相比之下，当调用tf.Variable时，变量不会被初始化。 要初始化TensorFlow程序中的所有变量，必须显式调用特殊操作，如下所示：12init = tf.global_variables_initializer()sess.run(init) 重要的是理解init是TensorFlow子图的句柄，它初始化所有的全局变量。 直到我们调用sess.run，变量是未初始化的。 由于x是一个占位符，我们可以同时对多个值进行输入：1print(sess.run(linear_model, &#123;x:[1,2,3,4]&#125;)) 结果为：1[ 0. 0.30000001 0.60000002 0.90000004] 我们已经创建了一个模型，但无法评价其性能。为了评估训练数据的模型，需要一个占位符y来提供所需的值，我们需要写一个损失函数。损失函数测量当前模型与提供的数据之间的距离。 我们将使用用于线性回归的标准损失模型，其将当前模型和提供的数据之间的增量的平方求和。 用linear_model - y创建一个向量，其中每个元素是对应的示例的误差增量。 我们调用tf.square来平方误差。 然后，我们对所有平方误差求和，创建一个单一的标量，使用tf.reduce_sum抽象所有示例的错误：1234y = tf.placeholder(tf.float32)squared_deltas = tf.square(linear_model - y)loss = tf.reduce_sum(squared_deltas)print(sess.run(loss, &#123;x:[1,2,3,4], y:[0,-1,-2,-3]&#125;)) 结果为：123.66 我们可以通过将W和b的值重新赋值为-1和1的完美值来手动改进。变量初始化为提供给tf.Variable的值，但可以使用类似tf.assign的操作来更改。 例如，W = -1和b = 1是我们模型的最佳参数。 我们可以相应地改变W和b：1234fixW = tf.assign(W, [-1.])fixb = tf.assign(b, [1.])sess.run([fixW, fixb])print(sess.run(loss, &#123;x:[1,2,3,4], y:[0,-1,-2,-3]&#125;)) 最终的输出为0：10.0 我们人为猜测了W和b的“完美”值，但机器学习的整个要点是自动找到正确的模型参数。 我们将在下一节中说明如何完成这一任务。 tf.train API 机器学习的完整讨论超出了本教程的范围。 然而，TensorFlow提供了优化器，其缓慢地改变每个变量以便最小化损失函数。 最简单的优化器是梯度下降。 它根据相对于该变量的损失导数的大小来修改每个变量。 一般来说，人工计算符号导数是繁琐的并且容易出错。 因此，TensorFlow可以使用函数tf.gradients自动产生仅给出模型描述的导数。 为了简单起见，优化器通常会为您执行此操作。 例如，12optimizer = tf.train.GradientDescentOptimizer(0.01)train = optimizer.minimize(loss) 12345sess.run(init) # reset values to incorrect defaults.for i in range(1000): sess.run(train, &#123;x:[1,2,3,4], y:[0,-1,-2,-3]&#125;)print(sess.run([W, b])) 输出的最终结果：12[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)] 现在我们已经做了实际的机器学习！ 虽然做这个简单的线性回归不需要太多的TensorFlow核心代码，但是更复杂的模型和方法来将数据输入到模型中需要更多的代码。 因此，TensorFlow为通用模式、结构和功能提供了更高级别的抽象。 我们将在下一节中学习如何使用这些抽象。 完整程序12345678910111213141516171819202122232425262728import numpy as npimport tensorflow as tf# Model parametersW = tf.Variable([.3], tf.float32)b = tf.Variable([-.3], tf.float32)# Model input and outputx = tf.placeholder(tf.float32)linear_model = W * x + by = tf.placeholder(tf.float32)# lossloss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares# optimizeroptimizer = tf.train.GradientDescentOptimizer(0.01)train = optimizer.minimize(loss)# training datax_train = [1,2,3,4]y_train = [0,-1,-2,-3]# training loopinit = tf.global_variables_initializer()sess = tf.Session()sess.run(init) # reset values to wrongfor i in range(1000): sess.run(train, &#123;x:x_train, y:y_train&#125;)# evaluate training accuracycurr_W, curr_b, curr_loss = sess.run([W, b, loss], &#123;x:x_train, y:y_train&#125;)print("W: %s b: %s loss: %s"%(curr_W, curr_b, curr_loss)) 运行后结果：1W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11 其可视化图： tf.contrib.learn tf.contrib.learn是一个高级TensorFlow库，它简化了机器学习的机制，包括以下内容： 运行训练 运行评价 管理数据集 管理输入 tf.contrib.learn定义了许多常见的模型。 基本用法注意，线性回归程序用tf.contrib.learn变得更简单：123456789101112131415161718192021222324252627282930import tensorflow as tf# NumPy is often used to load, manipulate and preprocess data.import numpy as np# Declare list of features. We only have one real-valued feature. There are many# other types of columns that are more complicated and useful.features = [tf.contrib.layers.real_valued_column("x", dimension=1)]# An estimator is the front end to invoke training (fitting) and evaluation# (inference). There are many predefined types like linear regression,# logistic regression, linear classification, logistic classification, and# many neural network classifiers and regressors. The following code# provides an estimator that does linear regression.estimator = tf.contrib.learn.LinearRegressor(feature_columns=features)# TensorFlow provides many helper methods to read and set up data sets.# Here we use `numpy_input_fn`. We have to tell the function how many batches# of data (num_epochs) we want and how big each batch should be.x = np.array([1., 2., 3., 4.])y = np.array([0., -1., -2., -3.])input_fn = tf.contrib.learn.io.numpy_input_fn(&#123;"x":x&#125;, y, batch_size=4, num_epochs=1000)# We can invoke 1000 training steps by invoking the `fit` method and passing the# training data set.estimator.fit(input_fn=input_fn, steps=1000)# Here we evaluate how well our model did. In a real example, we would want# to use a separate validation and testing data set to avoid overfitting.estimator.evaluate(input_fn=input_fn) 运行后输出：1&#123;'global_step': 1000, 'loss': 1.9650059e-11&#125; 自定义模型tf.contrib.learn不会只能运行其预定义的模型。 假设我们想创建一个未内置到TensorFlow中的自定义模型。 我们仍然可以保留tf.contrib.learn的数据集，馈送，训练等的高级抽象。 为了说明，我们将演示如何使用我们的低级TensorFlow API的知识来实现我们自己的等效模型到LinearRegressor。要定义与tf.contrib.learn一起使用的自定义模型，我们需要使用tf.contrib.learn.Estimator。 tf.contrib.learn.LinearRegressor实际上是tf.contrib.learn.Estimator的子类。 而不是子类别Estimator，我们只是提供Estimator一个函数model_fn，告诉tf.contrib.learn如何评估预测，训练步骤和损失。 代码如下：1234567891011121314151617181920212223242526272829303132import numpy as npimport tensorflow as tf# Declare list of features, we only have one real-valued featuredef model(features, labels, mode): # Build a linear model and predict values W = tf.get_variable("W", [1], dtype=tf.float64) b = tf.get_variable("b", [1], dtype=tf.float64) y = W*features['x'] + b # Loss sub-graph loss = tf.reduce_sum(tf.square(y - labels)) # Training sub-graph global_step = tf.train.get_global_step() optimizer = tf.train.GradientDescentOptimizer(0.01) train = tf.group(optimizer.minimize(loss), tf.assign_add(global_step, 1)) # ModelFnOps connects subgraphs we built to the # appropriate functionality. return tf.contrib.learn.ModelFnOps( mode=mode, predictions=y, loss= loss, train_op=train)estimator = tf.contrib.learn.Estimator(model_fn=model)# define our data setx=np.array([1., 2., 3., 4.])y=np.array([0., -1., -2., -3.])input_fn = tf.contrib.learn.io.numpy_input_fn(&#123;"x": x&#125;, y, 4, num_epochs=1000)# trainestimator.fit(input_fn=input_fn, steps=1000)# evaluate our modelprint(estimator.evaluate(input_fn=input_fn, steps=10)) 运行后输出：1&#123;'loss': 5.9819476e-11, 'global_step': 1000&#125; 注意自定义model()函数的内容与下层API的手动模型训练循环非常相似。 用惯了caffe，感觉TensorFlow有点麻烦呢 TensorFlow Develop]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Udacity Linux 命令行基础 Shell 入门]]></title>
    <url>%2F2017%2F03%2F06%2Fshell%2F</url>
    <content type="text"><![CDATA[Linux 命令行基础Shell 入门这上面讲的有点太简单了，总结一下备忘。 Get into the shell错误信息输入包括单引号’、圆括号(、大括号{输出有&gt;（右尖括号），需要补全。ctrl+c退出 简单指令目录文件1ls 下载1curl http://udacity.github.io/ud595-shell/stuff.zip -o things.zip 安装12345Ubuntu and Debian users: sudo apt-get install cowsayRedhat and CentOS users: sudo yum install cowsayOS X users: brew install cowsay(This requires the homebrew, a third party package manager for OS X, http://brew.sh/)Arch Linux users: sudo pacman -S cowsay 更多指令： 日期：date 计算器：expr 2 + 2 显示：echo things（显示&#39;需要在前面加上\，即\&#39;） 操作系统名称：uname、uname -a 服务器名称：hostname 远程服务器ip等信息：host baidu.com bash 版本：bash --version 历史记录：history 删除：rm xxx python:os.remove(&quot;xxx&quot;) 状态：uptime18:41:45 up 2 days, 1:58, 5 users, load average: 2.01, 1.98, 2.00 时间、运行时间、用户数量、平均负载 Shell commands历史记录 ↑：浏览上一条命令，可以多次使用 history：列出所有记录 ctrl+R：搜索历史记录 解压zip1unzip XXX.zip 输出文本到终端1cat AAA.txt BBB.txt cat 代表concatenate。可用于读取其他文件作为输入。 命令补全Tab键自动补全命令、参数等。 文本文件信息统计1wc XXX.txt 输出行数、字数、字节数 比较不同1diff AAA.txt BBB.txt 对比两个文件的不同点 指令手册1man cowsay 查询某个指令的用法 显示隐藏文件1ls -a 默认不显示以.开头的文件 删除1rm -rf / 删除所有文件。。 程序接管shell如：1ping 8.8.8.8 会使shell被ping接管。按ctrl+C退出。另一种交互程序需要手工输入终止符，如：1234sortbca 最后输入ctrl+D（EOF）才会运行排序。 全屏文件显示、编辑工具1less XXX.txt 12nano XXX.txtvim XXX.txt The Linux Filesystem文件名文件和目录都有其名称（”filenames”）Filenames 可以包括任何除了/的字符。当需要写一个包括特殊字符（#!$%&amp;[]等）需要采取引用或转义的形式： 真实名：Great Name! 引用：`Great Name!` 转义：Great\ Name\!文件系统树不像windows，分区为不同盘符，而是统一在(root)下。文件夹与文件夹之间用/连接（windows用\）。 目录操作 当前路径：pwd 进入目录：cd 绝对路径与相对路径 绝对路径：从根目录开始 相对路径：从当前工作目录开始 .指向当前路径 ..指向上一级路径 复制与移动 移动文件mv A B可以重命名 复制文件cp A B 新建目录与删除新建 当前路径：mkdir A 绝对路径：mkdir /home/user/A###删除 删除目录：rmdir A不能对包含文件的目录删除 删除目录及文件：rf -r A Globbing通配模式 包括html的文件：ls *html 大括号中选择：ls app.{css,html} 一个?匹配一个字符：ls bea?.png 匹配[]中其中一个：ls be[aeio]r.png 区别大小写，包括扩展名 codecademy上面好像也有shell的入门，比这个要好一点。]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow 在windows下安装]]></title>
    <url>%2F2017%2F03%2F03%2Ftensorflowonwin%2F</url>
    <content type="text"><![CDATA[蹭的深度学习课程，老师推荐用tensorflow做作业，因此先接触一下吧，不用来做项目，先熟悉一下语句。相比于caffe，tensorflow没有复杂的编译过程，简单的可以把它看成一个python的库。所以安装起来也是很简单的~ 环境准备其实环境比最后的安装更重要= =也遇到了一些小问题。 AnacondaTensorflow基于python，而Anaconda提供了较好的python环境，特别是建立不同虚拟环境，管理起来比较方便。下载需要注意的是，tensorflow在windows下使用的是python3.5，下载所提供的两个默认环境分别是2.7和3.6的，好像在之后pip安装都不行，不过这不是重点，之后建立新环境再配置就行了。选一个下载，我电脑上原本装的是2.7版本的。安装之后，就需要配置Anaconda下的python环境。在开始菜单中，点击Anaconda prompt进入命令行：打开一个窗口(D:\Program Files\Anaconda2) C:\Users\admin&gt;前面括号里面的内容代表着当前使用的环境，目前是默认环境。为了我们使用tensorflow，需要配置一个新的环境，主要需求：python3.5，numpy。官方的说明文档有较为详细的介绍，这里就直接说重点了。 新建环境 1conda create --name tensorflow python=3.5 numpy 这样会自动列出所要安装的所有内容，按y继续。有这样的提示说明安装成功！ 切换环境安装成功之后，要记得切换到新安装的环境，如上图所说，输入： 1activate tensorflow 提示行前面括号变成了（tensorflow），证明切换到了这个环境。 额外命令列出所有的环境: 1conda info --envs 删除一个环境： 1conda remove -n flowers --all 至此已做好了绝大部分的工作。 安装Tensorflow1pip install tensorflow 是的，只有这一句话。最后会提示安装成功。如果想要确认一下，可以从命令行输入python进入python环境，再输入：import tensorflow导入库，没有报错说明安装成功。如果不放心可以运行一下测试程序：12345import tensorflow as tfa=tf.constant(1)b=tf.constant(2)sess=tf.Session()sess.run(a+b) 输出为3，中间可能突然冒出来一堆语句，目前看来没有什么影响。 jupyter notebook如果一直使用命令行输入还是太麻烦了，jupyter notebook提供了很好的编辑、调试、输出环境，推荐还是安装一下，pip install jupyter notebook应该就可以了。而我遇到的坑就出在这里。我一开始Anaconda默认python版本为2.7，新建了3.5的环境，以为这样就可以直接在新环境里运行jupyter notebook，但在import时候一直提示没有这一模块。其实这样并不可以，需要再在新的3.5环境中重新安装一遍jupyter notebook。这样才成功~注意在开始菜单中，不同环境安装的程序都在括号里面写了环境名，使用的时候要注意一下区分。如果不习惯命令行，Anacoda也提供了图形界面Anacoda Navigator，这里也提供了程序启动，环境配置等功能，比较直观，具体不多讲了。 总结Tensorflow安装非常简单。不过对于没有配置过python环境的新手，还是需要注意一下的~]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GAN网络]]></title>
    <url>%2F2017%2F02%2F28%2Fgan%2F</url>
    <content type="text"><![CDATA[相比于传统的识别、分类工作，生成对抗网络以一种逆向的思维，让计算机有了一定的创造能力。这种创造在实际中有更大的意义，甚至在复杂的工作中也能取得良好的效果。首先看一下最初的Goodfellow的工作：Generative Adversarial Nets。 介绍关于GAN，论文中有一个很恰当的比喻： The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles. G是一个生产假币的队伍，想尽办法让假币无法被识别出来；D是警察，想尽办法查出假币。双方不断在竞争，不断学习，从而达到生成器可以以假乱真的效果。 #模型这张图可以让人最直观理解GAN网络。 左图是一个判别式模型D，当输入训练数据x时，期待输出高概率（接近1）；右图下半部分是生成模型G，输入是一些服从某一简单分布（例如高斯分布）的随机噪声z，输出是与训练图像相同尺寸的生成图像。向判别模型D输入生成样本，对于D来说期望输出低概率（判断为生成样本），对于生成模型G来说要尽量欺骗D，使判别模型输出高概率（误判为真实样本），从而形成竞争与对抗。 这是两个模型的value function，就像是在进行一个双人极大极小博弈。其中x为真实训练数据，在输入噪声变量$p_z(z)$上定义先验，然后表示到数据空间的映射为$G(z;θ_g)$，G是由多层感知器表示的可微函数， 参数为θg。判别模型$D(x;θ_d)$，D（x）表示x来自数据而不是$p_g$的概率。 训练网络D使得最大概率地分对训练样本的标签（最大化log D(x)），训练网络G最小化log(1 – D(G(z)))，即最大化D的损失。训练过程中固定一方，更新另一个网络的参数，交替迭代，使得对方的错误最大化，最终，G 能估测出样本数据的分布。生成模型G隐式地定义了一个概率分布Pg，我们希望Pg 收敛到数据真实分布Pdata。论文证明了这个极小化极大博弈当且仅当Pg = Pdata时存在最优解，即达到纳什均衡，此时生成模型G恢复了训练数据的分布，判别模型D的准确率等于50%。 训练在训练中，针对不同数据集，也设计了不同网络。例如MNIST，只需要全链接就可以了：生成模型：判别模型：复杂网络之中，还有反卷积层。 Generative Adversarial Networks生成式对抗网络GAN研究进展（一）]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[北方人的优越感]]></title>
    <url>%2F2017%2F02%2F21%2Fsnow%2F</url>
    <content type="text"><![CDATA[南北方我也去过不少城市了，今天说的优越感仅仅限于气候，不黑不吹。主要有两点：1.北方下雪；2.北方较为干燥（不会有衣服长时间晾不干的情况）。第二点是长时间的属性，难以变更。第一点却是要看老天心情，要真是在北方一场雪不下，总觉得心里难受。今年就差点是这种心情了…… 去年10月多来到北京，天气逐渐转凉。与记忆中的飘雪迎冬不同，雾霾倒是越来越严重了。十一月中旬我回到原来的学校，这时却突然传来下雪的捷报，只恨回去的不是时候。还好看了网上照片，雪并不大，只是零星小雪罢了，只能说为下雪开了个头。这样安慰自己，心情好了些。 后来才发现只是为了吊我胃口啊，一晃直到过年才在老家看到了零星飘雪，似乎想用同样的套路欺骗我。再次回到北京，已经不报什么希望了。第一周末看到天气预报说下周二有雪。哦？可能是老天想在ddl之前在应付一下吧。 中午吃完饭回来，果然飘了点雪，我高兴地急忙拿出相机，拍出了一张上图，是一种只能用心去领会的雪，这连发朋友圈的资格都没有啊……回到办公室，盯着电脑看了一会儿，好像有人在群里说自己在朝阳区感到了强烈的大雪，一扭头，窗外已是一片白茫茫的雪花飞扬。在即将逝去的晚冬，终于用这种方式让人们体会到北方的优越感。不一会儿地上就有了积雪。对于新下的雪，感觉最舒服的就是亲自在上面走走。咯吱咯吱的声音，总能让自己很满足。仔细想想，到成都三年多，印象里的却没有什么像样的雪，回家之后遇到雪也是一个概率性事件，可能早了或者晚了，对雪的印象已经有些淡了。因为记忆是否深刻，往往和雪是否大有关，就像08还是09年的大雪。那时候是上初一还是初二，周末留宿，和一群小伙伴在大雪中踢球，特别是厚厚的积雪让我们突发奇想，绕着球门搭了一圈城墙。下大雪和下大雨，本质上你可以说是一样的，都是水嘛~但感受却大不相同。雨中除了疯玩疯跑，都是要打伞的，否则湿漉漉的总让人感觉不舒服；下雪天打伞的人就很少了。看着整个世界都被披上银装，自己不加点白似乎有些不自然。 当然雪中最主要的活动还是打雪仗和堆雪人，这也都是大学前记忆了。特别是打雪仗，大学没雪，老家没人，因此总是很尴尬。想到这里，不如抓住这机会堆个雪人吧。高中时候操场上各家都在堆自己的雪人，有的真是很大，直径就接近一米了，我单次可能也就30-40cm，或许是因为我没有一个手套吧。小楼下积雪不少，可以堆个小的练练手。握住雪的感觉也是很棒的，松松软软的，总是忍不住想多欺负一下。团了一会儿，手已经不行了，看来还是锻炼的少了= = 好，最后就是这个东西了！直径可能不到10cm吧（堆不大的锅还是甩给手套吧），但感觉还是找到一点。之后楼上学长也坐不住了，也团了一个与我比拼一下。不得不说还是更精致一些： 哈哈，踩了雪地，堆了雪人，北方人的优越感，说多也不多嘛~]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>雪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe-python-layer 的自定义]]></title>
    <url>%2F2017%2F02%2F17%2Fcaffe-python-layer%2F</url>
    <content type="text"><![CDATA[还是caffe的自定义层问题。相比于c，python的自定义层更为简单：代码少、外部文件少、方便执行。因此用这种方法实现有利于开发和实验。 准备首先还是要记得在编译的时候加上WITH_PYTHON_LAYER的选项，如果没有加可以先make clean删除编译后的文件，再重新编译。1WITH_PYTHON_LAYER=1 make &amp;&amp; make pycaffe 如果出现1layer_factory.hpp:77] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Python 这样的错误，说明这一步没有成功。 net中的添加在caffe的配置net里我们要输入有关层的信息，下面以EuclideanLossLayer层为例。简单来说我们可以直接使用，因为原本caffe里面已经包括了用c编写的代码，现在我们把它改编为python层。123456789101112131415layer &#123; type: 'Python' name: 'loss' top: 'loss' bottom: 'ipx' bottom: 'ipy' python_param &#123; # 模块名 -- 通常也是文件名 -- 需要放在 $PYTHONPATH 中 module: 'pyloss' # 层名 -- 模块里的类名 layer: 'EuclideanLossLayer' &#125; # set loss weight so Caffe knows this is a loss layer loss_weight: 1&#125; python的层文件需要在$PYTHONPATH目录下。在prototxt中，模块名是pyloss，这意味着你的包括EuclideanLossLayer类名的py文件名也应该是pyloss.py。 layer文件添加123456789101112131415161718192021222324252627282930313233# pyloss.pyimport caffeimport numpy as npclass EuclideanLossLayer(caffe.Layer): def setup(self, bottom, top): # check input pair if len(bottom) != 2: raise Exception("Need two inputs to compute distance.") def reshape(self, bottom, top): # check input dimensions match if bottom[0].count != bottom[1].count: raise Exception("Inputs must have the same dimension.") # difference is shape of inputs self.diff = np.zeros_like(bottom[0].data, dtype=np.float32) # loss output is scalar top[0].reshape(1) def forward(self, bottom, top): self.diff[...] = bottom[0].data - bottom[1].data top[0].data[...] = np.sum(self.diff**2) / bottom[0].num / 2. def backward(self, top, propagate_down, bottom): for i in range(2): if not propagate_down[i]: continue if i == 0: sign = 1 else: sign = -1 bottom[i].diff[...] = sign * self.diff / bottom[i].num 总结经测试文件应该没问题，可以读取运行。不过没有现成使用EuclideanLossLayer的网络，最后计算结果没有验证。 Caffe Python Layer]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GoogleNet :Going deeper with convolutions 论文阅读]]></title>
    <url>%2F2017%2F02%2F13%2Fgoingdeeper%2F</url>
    <content type="text"><![CDATA[这次读旁边拿了纸笔记录，感觉还是方便一些，之后再写篇博客总结一下加深印象。 问题引出Going deeper考虑的问题： 不在于训练数据、模型大小，希望得到新的模型结构； 可以用于移动计算，需要考虑功率、内存使用等问题。 NIN借鉴到的1*1卷积核： 降维（当然也可以升维），减少参数和计算； 增加深度、宽度，而没有明显性能损失。 目前提高深度神经网络性能的方法：加大size→缺点： 容易造成过拟合； 计算复杂。 →解决： 完全连接到稀疏连接（ultimately moving from fully connected to sparsely connected architectures, even inside the convolutions） 如果数据集的概率分布可以由大而稀疏的深层网络表示，则最佳网络拓扑可通过分析最后层的激活的相关统计数据逐层构建。但对于非均匀稀疏数据计算效率低。提出：是否有中间步骤→一个利用额外稀疏性的架构？ Inception国外的命名还真是很有意思的，“Going deeper”就引用了我最开始的图的台词，Inception（《盗梦空间》）就是这台词的出处。好了，接下来就是这篇文章的核心：Inception结构。其实也很简单，利用多个大小卷积核进行提取特征并融合（个人理解）。具体看图更容易理解：之前我们卷积层很简单，是一个nn的滤波器与上一层输入滑动卷积即可。而这里突然又变成了四兄弟，滤波器分别为：11卷积，33卷积，55卷积，33池化，最后汇总输出。为了保证输出大小相同，只要将pad分别设置为0,1,2，stride设置为1即可。这里出来问题在于55计算量较大，特别是在深层情况下，所以说还是naive啊~还好有了NIN的借鉴，利用上文说到的1*1卷积核降维，可以解决这一问题。例如：上一层的输出为100x100x128，经过具有256个输出的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256。其中，卷积层的参数为128x5x5x256。假如上一层输出先经过具有32个输出的1x1卷积层，再经过具有256个输出的5x5卷积层，那么最终的输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256。结构如下图所示： 总结优势： 每阶段增加了单元数，但也不会过度增加计算复杂； 不同卷积核大小，表示信息在不同尺度上处理，与直观相符合。 感想：之前也有想过不同尺度（也可以说是不同卷积核大小）下进行分别提取特征的，但后面不是很清楚如何能统一得到结果的，其实看完之后也很简单，通过补零即可。更重要的可能还在于1*1卷积层的巧妙使用，从参数量上大大减小了，提出该方法的NIN也需要看一下。 Going Deeper with ConvolutionsGoogLeNet系列解读]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>GoogleNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新年旧聚]]></title>
    <url>%2F2017%2F02%2F03%2Fnewyearparty%2F</url>
    <content type="text"><![CDATA[新年到来，一方面是迎新，另一方面是怀旧。回来这一段，有了两次和同学的聚会，一次是小学，一次是高中。高中的也不能说是很熟的同学，只是一起在外地上学，同校的而已。 首先说一下小学吧。初中之后，我就一直在外地上学，小学同学确实也联系的很少了。还有一点是当时小升初有的同学考上了初高中5年，这样的话还比我们大一级，也就是大学也应该毕业了。这样来看我们的“成分”也是比较复杂的。不过等到同学的提醒，我才发现如今已经离小学毕业过了10年了，真是一转眼。时间越久，也是越难相聚，这次在年前聚会可能也是有点仓促，来的人勉强凑成了一桌。 再次见到阔别的同学，觉得都是成熟了。当年邋遢而调皮捣蛋的同学，如今也可以一身风衣帅气而潇洒。最令我吃惊的是一名同学已经结婚领证了。其实仔细想想也没什么，年龄也到了嘛，以后这消息肯定多着呢。第二点感受是能继续读大家也都不会去轻易工作了，能出去也不会留下了。毕竟是三四线的小城市，况且环境污染也这么严重，以后再聚可能也不止这一个主会场了。 相比于小学，和高中同校不同班的同学相聚，按说应该会有一定的陌生感，但实际上聊的话题更多了，因为就经历来看的确比较像。考上一所还不错的大学，如今不是读研就是出国。特别是刚进门，座位上的四名同学都在讨论毕设的问题，一下有了一种亲切感。之后谈论的不是对大学生活的回顾（比如某校网费按流量计算1G几块钱想想看我所在的地方真是良心），就是对未来读研的期待（恐惧？他们读博的较多）。特别是后来，红酒过了几轮，大家纷纷开始讨论了专业学术问题：从解剖课程到疫苗，从人工智能到机器学习，从清洁能源到可控核聚变（佩服中科大学物理的同学）……感觉如果同行的家长不走我们还能继续聊下去。。吃完了又去玩了我不擅长的狼人，说不定真要多看看网上的节目来长些知识了。 新年易迎，旧友难聚，曾经的同学朋友各奔东西，却反而增加了今后相遇的机会。也希望以后有机会在外地和同学能又一次有共聚的机会。 如果说两次聚会的相同点，那就是——女生太少了(´；ω；`)]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>新年</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy中遇到的问题与解决]]></title>
    <url>%2F2017%2F01%2F23%2Fscrapy%2F</url>
    <content type="text"><![CDATA[Scrapy，Python开发的一个快速,高层次的屏幕抓取和web抓取框架，用于抓取web站点并从页面中提取结构化的数据。 因为好像这个用的比较多，所以看看用这个框架该怎么写爬虫。其实不难，但是中间出了很多神奇的小问题。 输出不正确、改代码结果不变？其实是因为反复使用命令1scrapy crawl spider -o 1.json 时候，增加的输出数据不会覆盖，而是继续往后面添加。 request不执行找了半天不知道为啥，其中一个比较靠谱的是1Request(url,meta=&#123;'item':item&#125;,callback=self.parse2, dont_filter=True) dont_filter=True让allowed_domains失效了。但是改过了还是不行。最终结果发现改的文件和运行的文件不一样……为什么会这样呢？我中间做了一部分实现了初始功能，就重命名了备份，然而执行命令行竟然一直在执行备份文件。。 输出为utf-8格式（保存中文）更改pipeline文件。123456789101112131415import jsonimport codecsclass WebPipeline(object): # def process_item(self, item, spider): # return item def __init__(self): # self.file = open('data.json', 'wb') self.file = codecs.open( 'scraped_data_utf8.json', 'w', encoding='utf-8') def process_item(self, item, spider): line = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(line) return item def spider_closed(self, spider): self.file.close() 123ITEM_PIPELINES = &#123; 'web.pipelines.WebPipeline': 300,&#125; scrapy抓取到中文,保存到json文件为unicode,如何解决 如何在解析函数之间传递值？一种常见的情况：在parse中给item某些字段提取了值，但是另外一些值需要在parse_item中提取，这时候需要将parse中的item传到parse_item方法中处理，显然无法直接给parse_item设置而外参数。 Request对象接受一个meta参数，一个字典对象，同时Response对象有一个meta属性可以取到相应request传过来的meta。所以解决上述问题可以这样做：12345678def parse(self, response): # item = ItemClass() yield Request(url, meta=&#123;'item': item&#125;,callback=self.parse_item) def parse_item(self, response): item = response.meta['item'] item['field'] = value yield item Some Experiences Of Using Scrapy 使用ImagesPipeline下载1234567# setting.pyITEM_PIPELINES = ['demo.pipelines.MyImagesPipeline'] # ImagePipeline的自定义实现类IMAGES_STORE = 'D:\\dev\\python\\scrapy\\demo\\img' # 图片存储路径IMAGES_EXPIRES = 90 # 过期天数IMAGES_MIN_HEIGHT = 100 # 图片的最小高度IMAGES_MIN_WIDTH = 100 # 图片的最小宽度# 图片的尺寸小于IMAGES_MIN_WIDTH*IMAGES_MIN_HEIGHT的图片都会被过滤 ImagePipeline需要在自定义的ImagePipeline类中重载的方法：get_media_requests(item, info)和item_completed(results, items, info)。正如工作流程所示，Pipeline将从item中获取图片的URLs并下载它们，所以必须重载get_media_requests，并返回一个Request对象，这些请求对象将被Pipeline处理，当完成下载后，结果将发送到item_completed方法，这些结果为一个二元组的list，每个元祖的包含(success, image_info_or_failure)。 success: boolean值，true表示成功下载 image_info_or_error：如果success=true，image_info_or_error词典包含以下键值对。失败则包含一些出错信息。 url：原始URL path：本地存储路径 * checksum：校验码。1234567891011121314151617from scrapy.contrib.pipeline.images import ImagesPipelinefrom scrapy.exceptions import DropItemfrom scrapy.http import Requestclass MyImagesPipeline(ImagesPipeline): def get_media_requests(self, item, info): for image_url in item['image_urls']: yield Request(image_url) def item_completed(self, results, item, info): image_paths = [x['path'] for ok, x in results if ok] if not image_paths: raise DropItem("Item contains no images") item['image_paths'] = image_paths return item scrapy 下载图片 ImagesPipeline扩展Media Pipeline]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python小爬虫-糗百]]></title>
    <url>%2F2017%2F01%2F20%2Fspider-QSBK%2F</url>
    <content type="text"><![CDATA[序在家没事本来想弄一下pyqt，做一些python下的界面，但是eric装了半天没成功……于是改做爬虫(:3[__]还好网上教程多，参考了一下，大致的框架都比较简单，难的在于针对不同的网页如何写正则表达式。不过这东西写多了应该就掌握方法了。从网上找了一段代码是爬糗百的，由于改版原来的表达式失效了，正好有了一个锻炼的机会。以下是代码： 代码123456789101112131415161718192021222324252627282930313233343536# -*- coding:utf-8 -*-import urllibimport urllib2import repage = 1url = 'http://www.qiushibaike.com/hot/page/' + str(page)user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64)'# 需要加上headers才能访问headers=&#123;'User-Agent' : user_agent&#125;try: request = urllib2.Request(url,headers=headers) response = urllib2.urlopen(request)except urllib2.URLError, e: if hasattr(e,"code"): print e.code if hasattr(e,"reason"): print e.reasoncontent = response.read().decode('utf-8')# 正则表达式target = 'div.*?="author clearfix".*?title="(.*?)".*?div.*?="content".*?&lt;span&gt;(.*?)&lt;/span&gt;.*?&lt;/a&gt;.*?&lt;img src="(.*?)".*?&lt;/div&gt;'pattern = re.compile(target,re.S)items = re.findall(pattern,content)print "done"num=1;lenth=len(items);for item in items: # 判断是否存在图片 haveImg = re.search("pic.qiushibaike.com/system/pictures",item[2]) print str(num),'of',str(lenth) print item[0] # 用户名 print item[1] # 内容 if haveImg: print item[2] # 输出图片链接 num+=1; 效果原始网页：代码与结果： 总结这算是比较简单的抓取，不用登陆就可以了，之后再学学困难的。 Python爬虫实战一之爬取糗事百科段子]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>spider</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[白色·恐怖·游戏 - 返校]]></title>
    <url>%2F2017%2F01%2F18%2Fdetention%2F</url>
    <content type="text"><![CDATA[恐怖类游戏向来不在我的关注范围之内，但近期的《返校》突然成为了大家关注的热点，steam上好评率达到98%，再加上这是台湾的一款国产游戏，让我产生了一定的好奇。不过我这种没有怎么玩过恐怖类游戏的人，可能很难成功通关，因此选择了看录像流程的方式了解了一下，的确没有让人失望。 离校虽然说名字是《返校》，但整个流程大部分都是为了逃离这个学校，一个怪异、恐怖、充满血与鬼的地方有什么可留恋的呢？可是我们有时的确无法控制自己，在这样氛围笼罩的大环境下，一个人的力量往往是微弱的。学校本应是一个追求真理，渴望自由的思想乌托邦，在当时也无法逃脱管制的命运，游戏中频频出现的眼睛似乎无时无刻都在监视着一切的发生，残肢断臂警告着他人触犯条律的下场。离校，只逃避了一时，更何况对于学生，也无处可去。 白色白，像白鹿项链一样纯洁；白，像白纸飞机一样自由。方苪欣是白色的，苍白的脸，洁白的心。无奈社会更白，容不下一丝色彩。如同在荒芜的雪原，孤独地前行，迷失了自我。 恐怖从游戏上来讲，并没有想象中的那么恐怖，或许是在游戏之外过于恐怖了。当魏仲廷交给方苪欣一份禁书书单后，书单的样子变成了一只手枪，我才感到一丝寒意。也正是这只手枪，给老师判了死刑，让同学被捕入狱。之前对台湾的历史了解的不是很多，搜索之后才发现那时社会的恐怖所在。政党的恐惧导致了社会的恐怖，这似乎也是向来的规律，何时何地都逃不过。还有一句话是“知识越多越反动”，有时想想也确实很有道理。 返校十多年过去了，一切似乎又重归平静，魏仲廷出狱，终于能够再一次返校，却无法继续完成曾经的学业，遇到以前的老师同学。在那里的，只有同样等待着的方苪欣，等待着救赎…… 评故事不长，流程大概三个小时可以完成，除去剧情，在追寻碎片化的记忆过程中，也有很多巧妙的设计，特别是通过录音机调频来进行时间的穿越。另外就是游戏风格，的确很有中国特色，从界面人物的剪纸风，到民间传统鬼怪的融入，让玩家都很亲切。游戏体验中的恐怖感适中，总的来说是一款出色的游戏。 赤烛 返校Steam 返校]]></content>
      <categories>
        <category>Game</category>
      </categories>
      <tags>
        <tag>返校</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deepdream 实现]]></title>
    <url>%2F2017%2F01%2F11%2FDeepdream%2F</url>
    <content type="text"><![CDATA[Deepdream是一年半前谷歌搞的一个深度学习“艺术品”，最近在cs231n课上看到了，感觉还是很interesting。 环境准备 deepdream还是基于python和caffe深度网络的，因此大概需要以下环境： Standard Python scientific stack: NumPy, SciPy, PIL, IPython. Those libraries can also be installed as a part of one of the scientific packages for Python, such as Anaconda or Canopy. Caffe deep learning framework (installation instructions). Google protobuf library that is used for Caffe model manipulation. 代码 导入库相关的环境配置好了之后，可以先试试库能不能被导入进来：12345678910111213141516171819# imports and basic notebook setupfrom cStringIO import StringIOimport numpy as npimport scipy.ndimage as ndimport PIL.Imagefrom IPython.display import clear_output, Image, displayfrom google.protobuf import text_formatimport caffecaffe.set_mode_gpu();caffe.set_device(2);# 默认GPU 为0# 如果GPU 支持 CUDA 并且 Caffe 编译时添加对 CUDA 支持,可以使用caffe.set_mode_gpu()和caffe.set_device(0);def showarray(a, fmt='jpeg'): a = np.uint8(np.clip(a, 0, 255)) f = StringIO() PIL.Image.fromarray(a).save(f, fmt) display(Image(data=f.getvalue())) 加载模型网络采用了GoogLeNet模型，需要提前下好。1234567891011121314151617181920212223242526model_path = '../caffe/models/bvlc_googlenet/' # 替换为自己模型的目录net_fn = model_path + 'deploy.prototxt'param_fn = model_path + 'bvlc_googlenet.caffemodel'# Patching model to be able to compute gradients.# Note that you can also manually add "force_backward: true" line to "deploy.prototxt".# 以下部分是更改了deploy的参数，增加了"force_backward: true"，然后保存成一个临时文件用于网络。当然也可以自己手动改。model = caffe.io.caffe_pb2.NetParameter()text_format.Merge(open(net_fn).read(), model)model.force_backward = Trueopen('tmp.prototxt', 'w').write(str(model))net = caffe.Classifier('tmp.prototxt', param_fn, mean = np.float32([104.0, 116.0, 122.0]), # ImageNet mean, training set dependent # 均值 channel_swap = (2,1,0)) # the reference model has channels in BGR order instead of RGB # 改RGB通道# a couple of utility functions for converting to and from Caffe's input image layout# 为了caffe 数据处理的功能函数def preprocess(net, img): return np.float32(np.rollaxis(img, 2)[::-1]) - net.transformer.mean['data']def deprocess(net, img): return np.dstack((img + net.transformer.mean['data'])[::-1]) 做梦 Making the “dream” images is very simple. Essentially it is just a gradient ascent process that tries to maximize the L2 norm of activations of a particular DNN layer. Here are a few simple tricks that we found useful for getting good images: offset image by a random jitter normalize the magnitude of gradient ascent steps apply ascent across multiple scales (octaves) 做梦其实很简单，本质上，它只是一个梯度上升过程，试图最大化特定DNN层激活的L2范数。 这里有一些简单的技巧，我们发现有用的获得良好的图像： 由随机抖动偏移图像 规则化梯度上升步长的幅度 在多个尺度上应用上升 首先我们实现一个基本的梯度上升阶跃函数，应用前两个技巧：1234567891011121314151617181920212223242526272829303132# 将输入的数据(data)复制给梯度(diff)def objective_L2(dst): dst.diff[:] = dst.data # 核心函数def make_step(net, step_size=1.5, end='inception_4c/output', jitter=32, clip=True, objective=objective_L2): '''Basic gradient ascent step.''' src = net.blobs['data'] # input image is stored in Net's 'data' blob # 输入图像 dst = net.blobs[end] # 目标层，默认为'inception_4c/output' ox, oy = np.random.randint(-jitter, jitter+1, 2) # 生产抖动 src.data[0] = np.roll(np.roll(src.data[0], ox, -1), oy, -2) # apply jitter shift # 应用抖动 net.forward(end=end) # 向前传播到指定层 objective(dst) # specify the optimization objective # 指定优化目标（默认为objective_L2优化） net.backward(start=end) # 反向传播到优化层 g = src.diff[0] # 输入图像梯度 # apply normalized ascent step to the input image # 对输入图像应用归一化上升步长 src.data[:] += step_size/np.abs(g).mean() * g src.data[0] = np.roll(np.roll(src.data[0], -ox, -1), -oy, -2) # unshift image # 还原抖动 if clip: bias = net.transformer.mean['data'] src.data[:] = np.clip(src.data, -bias, 255-bias) Next we implement an ascent through different scales. We call these scales “octaves”. 接下来，我们通过不同的尺度实现上升。 我们称这些尺度为“octaves”。iter_n是迭代次数，octave_n是尺度缩放次数，octave_scale是尺度缩放比例。1234567891011121314151617181920212223242526272829303132333435363738# 默认参数：迭代10次，缩放4次，缩放比1.4# 即原始图像迭代10次，长宽缩小1.4倍后再次迭代，一共进行4轮（包括原始尺寸）def deepdream(net, base_img, iter_n=10, octave_n=4, octave_scale=1.4, end='inception_4c/output', clip=True, **step_params): # prepare base images for all octaves # 准备数据，生成octave_n个数据 octaves = [preprocess(net, base_img)] for i in xrange(octave_n-1): octaves.append(nd.zoom(octaves[-1], (1, 1.0/octave_scale,1.0/octave_scale), order=1)) src = net.blobs['data'] # np.zeros_like(a): 依据给定数组(a)的形状和类型返回一个新的元素全部为1的数组。 detail = np.zeros_like(octaves[-1]) # allocate image for network-produced details for octave, octave_base in enumerate(octaves[::-1]): h, w = octave_base.shape[-2:] if octave &gt; 0: # upscale details from the previous octave h1, w1 = detail.shape[-2:] detail = nd.zoom(detail, (1, 1.0*h/h1,1.0*w/w1), order=1) src.reshape(1,3,h,w) # resize the network's input image size src.data[0] = octave_base+detail for i in xrange(iter_n): make_step(net, end=end, clip=clip, **step_params) # visualization vis = deprocess(net, src.data[0]) if not clip: # adjust image contrast if clipping is disabled vis = vis*(255.0/np.percentile(vis, 99.98)) showarray(vis) print octave, i, end, vis.shape clear_output(wait=True) # extract details produced on the current octave detail = src.data[0]-octave_base # returning the resulting image return deprocess(net, src.data[0]) 开始做梦123# 打开并显示图片img = np.float32(PIL.Image.open('sky1024px.jpg'))showarray(img) 1_=deepdream(net, img)# 运行 一共会得到4（尺度）*10（迭代）=40张图片。更改结束层会改变结果，如：1_=deepdream(net, img, end='inception_3b/5x5_reduce')# 更改结束层 具体的层可以参考配置文件，googlenet还是比较复杂的。而且结束的越晚，就更能从图像识别出现实物体。如：_=deepdream(net, img, end=&#39;inception_4e/output&#39;)左边的云已经可以看出是一只狗的脸了。 前方高能！上面一步就得到了奇怪的结果，如果把这样的输出再作为输入放到网络里呢？结果比较精神污染： 这是经过大概5轮反复输入得到的结果。官方迭代了100次，丧心病狂啊= = 控制做梦如果调整了我们的优化目标，就可以控制我们想要的结果。比如我们想把原始图像往另一张图像上靠近，可以定义一个指向性的优化目标：12345678def objective_guide(dst): x = dst.data[0].copy() y = guide_features ch = x.shape[0] x = x.reshape(ch,-1) y = y.reshape(ch,-1) A = x.T.dot(y) # compute the matrix of dot-products with guide features dst.diff[0].reshape(ch,-1)[:] = y[:,A.argmax(1)] # select ones that match best guide_features是需要预先提取的目标特征：1234567end = 'inception_3b/output'h, w = guide.shape[:2]src, dst = net.blobs['data'], net.blobs[end]src.reshape(1,3,h,w)src.data[0] = preprocess(net, guide)net.forward(end=end)guide_features = dst.data[0].copy() 好了，运行！1_=deepdream(net, img, end=end, objective=objective_guide) 有了花的感觉啊~ 总结正着传播过来是分类，反着过去是生成，用深度学习产生艺术作品似乎是一个很有意思的方向。而目前也有较为成熟甚至是商业化的项目了，比如deepart。如果你能够承受一定的精神污染，建议挑战一下Nightmare，这是YOLO大神的另一个作品。 deepdream-github]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>deepdream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe-python interface 学习-网络训练、部署、测试]]></title>
    <url>%2F2017%2F01%2F08%2Fpycaffe-interface2%2F</url>
    <content type="text"><![CDATA[继续python接口的学习。剩下还有solver、deploy文件的生成和模型的测试。 网络训练solver文件生成其实我觉得用python生成solver并不如直接写个配置文件，它不像net配置一样有很多重复的东西。对于一下的solver配置文件：12345678910111213141516base_lr: 0.001display: 782gamma: 0.1lr_policy: “step”max_iter: 78200 #训练样本迭代次数=max_iter/782(训练完一次全部样本的迭代数)momentum: 0.9snapshot: 7820snapshot_prefix: "snapshot"solver_mode: GPUsolver_type: SGDstepsize: 26067 test_interval: 782 #test_interval=训练样本数(50000)/batch_size(train:64)test_iter: 313 #test_iter=测试样本数(10000)/batch_size(test:32)test_net: "/home/xxx/data/val.prototxt"train_net: "/home/xxx/data/proto/train.prototxt"weight_decay: 0.0005 可以用以下方式实现生成：1234567891011121314151617181920212223242526from caffe.proto import caffe_pb2s = caffe_pb2.SolverParameter()path='/home/xxx/data/'solver_file=path+'solver1.prototxt's.train_net = path+'train.prototxt's.test_net.append(path+'val.prototxt')s.test_interval = 782 s.test_iter.append(313) #这里用的是append，码风不太一样s.max_iter = 78200 s.base_lr = 0.001 s.momentum = 0.9s.weight_decay = 5e-4s.lr_policy = 'step's.stepsize=26067s.gamma = 0.1s.display = 782s.snapshot = 7820s.snapshot_prefix = 'shapshot's.type = “SGD”s.solver_mode = caffe_pb2.SolverParameter.GPUwith open(solver_file, 'w') as f: f.write(str(s)) 并没有简单多少。需要注意的是有些参数需要计算得到： test_interval：假设我们有50000个训练样本，batch_size为64，即每批次处理64个样本，那么需要迭代50000/64=782次才处理完一次全部的样本。我们把处理完一次所有的样本，称之为一代，即epoch。所以，这里的test_interval设置为782，即处理完一次所有的训练数据后，才去进行测试。如果我们想训练100代，则需要设置max_iter为78200. test_iter：同理，如果有10000个测试样本，batch_size设为32，那么需要迭代10000/32=313次才完整地测试完一次，所以设置test_iter为313. lr_rate：学习率变化规律我们设置为随着迭代次数的增加，慢慢变低。总共迭代78200次，我们将变化lr_rate三次，所以stepsize设置为78200/3=26067，即每迭代26067次，我们就降低一次学习率。 模型训练完整按照定义的网络和solver去训练，就像命令行一样：12solver = caffe.SGDSolver('/home/xxx/solver.prototxt')solver.solve() 不过也可以分得更细一些，比如先加载模型：1solver = caffe.get_solver('models/bvlc_reference_caffenet/solver.prototxt') 这里用的是.get_solver，默认按照SGD方法求解。向前传播一次网络，即从输入层到loss层，计算net.blobs[k].data。1solver.net.forward() # train net 反向传播一次网络，即从loss层到输入层，计算net.blobs[k].diff and net.params[k][j].diff。1solver.net.backward() 如果需要一次完整的计算，正向、反向、更新权重（net.params[k][j].data），可以使用1solver.step(1) 改变数字进行多次计算。 网络部署部署即生成一个deploy文件，用于下面的模型测试。这里既可以用python，也可以直接修改net文件。123456789101112131415161718192021222324252627from caffe import layers as L,params as P,to_protoroot='/home/xxx/'deploy=root+'mnist/deploy.prototxt' #文件保存路径def create_deploy(): #少了第一层，data层 conv1=L.Convolution(bottom='data', kernel_size=5, stride=1,num_output=20, pad=0,weight_filler=dict(type='xavier')) pool1=L.Pooling(conv1, pool=P.Pooling.MAX, kernel_size=2, stride=2) conv2=L.Convolution(pool1, kernel_size=5, stride=1,num_output=50, pad=0,weight_filler=dict(type='xavier')) pool2=L.Pooling(conv2, pool=P.Pooling.MAX, kernel_size=2, stride=2) fc3=L.InnerProduct(pool2, num_output=500,weight_filler=dict(type='xavier')) relu3=L.ReLU(fc3, in_place=True) fc4 = L.InnerProduct(relu3, num_output=10,weight_filler=dict(type='xavier')) #最后没有accuracy层，但有一个Softmax层 prob=L.Softmax(fc4) return to_proto(prob)def write_deploy(): with open(deploy, 'w') as f: f.write('name:"Lenet"\n') f.write('input:"data"\n') f.write('input_dim:1\n') f.write('input_dim:3\n') f.write('input_dim:28\n') f.write('input_dim:28\n') f.write(str(create_deploy()))if __name__ == '__main__': write_deploy() 如果自己修改net，需要修改数据输入：123456layer &#123; name: "data" type: "Input" top: "data" input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 100 dim: 100 &#125; &#125;&#125; 并且增加一个softmax，对于原来的softmaxwithloss直接换掉就行。 网络测试训练好之后得到模型，实际使用是需要用模型进行预测。这时需要用到deploy文件和caffemodel。123456789101112131415161718192021222324252627282930#coding=utf-8import caffeimport numpy as nproot='/home/xxx/' #根目录deploy=root + 'mnist/deploy.prototxt' #deploy文件caffe_model=root + 'mnist/lenet_iter_9380.caffemodel' #训练好的 caffemodelimg=root+'mnist/test/5/00008.png' #随机找的一张待测图片labels_filename = root + 'mnist/test/labels.txt' #类别名称文件，将数字标签转换回类别名称net = caffe.Net(deploy,caffe_model,caffe.TEST) #加载model和network#图片预处理设置transformer = caffe.io.Transformer(&#123;'data': net.blobs['data'].data.shape&#125;) #设定图片的shape格式(1,3,28,28)transformer.set_transpose('data', (2,0,1)) #改变维度的顺序，由原始图片(28,28,3)变为(3,28,28)#transformer.set_mean('data', np.load(mean_file).mean(1).mean(1)) #减去均值，前面训练模型时没有减均值，这儿就不用transformer.set_raw_scale('data', 255) # 缩放到【0，255】之间transformer.set_channel_swap('data', (2,1,0)) #交换通道，将图片由RGB变为BGRim=caffe.io.load_image(img) #加载图片net.blobs['data'].data[...] = transformer.preprocess('data',im) #执行上面设置的图片预处理操作，并将图片载入到blob中#执行测试out = net.forward()labels = np.loadtxt(labels_filename, str, delimiter='\t') #读取类别名称文件prob= net.blobs['Softmax1'].data[0].flatten() #取出最后一层（Softmax）属于某个类别的概率值，并打印print proborder=prob.argsort()[-1] #将概率值排序，取出最大值所在的序号 print 'the class is:',labels[order] #将该序号转换成对应的类别名称，并打印 总结利用python接口，对网络的具体参数能够有更全面的认识和理解。不过也有几点需要注意： 数据格式的转换caffe的数据blob shape是NCHW，通道数在前。而python图像处理时shape是HW*C，通道数在后。因此需要转换一下。 图片显示与保存由于没有图形界面，很方便的jupyter notebook不能使用，只好保存图片查看。 caffe的python接口学习（2）：生成solver文件caffe的python接口学习（5）：生成deploy文件caffe的python接口学习（6）：用训练好的模型（caffemodel）来分类新的图片Deep learning tutorial on Caffe technology : basic commands, Python and C++ code.Multilabel classification on PASCAL using python data-layers]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe-python interface 学习-网络定义详解]]></title>
    <url>%2F2017%2F01%2F06%2Fpycaffe-interface1%2F</url>
    <content type="text"><![CDATA[之前用的都是caffe的命令行接口，单独训练还行，不过看里面层的参数、数据还是很麻烦的。特别是这周实验遇到了比较大的问题，命令行无能为力，还是要好好看看python接口。 python 接口编译这个一般在编译caffe时都会顺带完成，如果遇到ImportError: No module named caffe，可能是没有编译或者没有添加到路径。编译可以在根目录下make pycaffe，目录是/caffe/python。将caffe/python的路径添加到用户环境变量~/.bashrc中：1export PYTHONPATH=/home/xxx/caffe/python 然后输入sudo ldconfig确认。当然我的服务器没有管理员权限，这时可以每次手动添加目录，见下文。编译时要在Makefile.config中修改有关路径，除此之外，1WITH_PYTHON_LAYER := 1 也是需要注意的一点，这在f-rcnn中也提到过。 python 接口调用123456import syssys.path.append('/home/xxx/caffe/python')#手动添加路径import caffeimport numpy as npfrom skimage import ioimport matplotlib.pyplot as plt 以上可以直接复制好，每次都加上。 运行模式小设置123caffe.set_mode_cpu()#设置为cpu模式caffe.set_device(0)#gpu号caffe.set_mode_gpu()#gpu模式 定义网络下面是一个例子：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-"""Spyder Editor"""from caffe import layers as L,params as P,to_protopath='/home/xxx/data/' #保存数据和配置文件的路径train_lmdb=path+'train_db' #训练数据LMDB文件的位置val_lmdb=path+'val_db' #验证数据LMDB文件的位置mean_file=path+'mean.binaryproto' #均值文件的位置train_proto=path+'train.prototxt' #生成的训练配置文件保存的位置val_proto=path+'val.prototxt' #生成的验证配置文件保存的位置#编写一个函数，用于生成网络def create_net(lmdb,batch_size,include_acc=False): #创建第一层：数据层。向上传递两类数据：图片数据和对应的标签 data, label = L.Data(source=lmdb, backend=P.Data.LMDB, batch_size=batch_size, ntop=2, transform_param=dict(crop_size=40,mean_file=mean_file,mirror=True)) #创建第二屋：卷积层 conv1=L.Convolution(data, kernel_size=5, stride=1,num_output=16, pad=2,weight_filler=dict(type='xavier')) #创建激活函数层 relu1=L.ReLU(conv1, in_place=True) #创建池化层 pool1=L.Pooling(relu1, pool=P.Pooling.MAX, kernel_size=3, stride=2) conv2=L.Convolution(pool1, kernel_size=3, stride=1,num_output=32, pad=1,weight_filler=dict(type='xavier')) relu2=L.ReLU(conv2, in_place=True) pool2=L.Pooling(relu2, pool=P.Pooling.MAX, kernel_size=3, stride=2) #创建一个全连接层 fc3=L.InnerProduct(pool2, num_output=1024,weight_filler=dict(type='xavier')) relu3=L.ReLU(fc3, in_place=True) #创建一个dropout层 drop3 = L.Dropout(relu3, in_place=True) fc4 = L.InnerProduct(drop3, num_output=10,weight_filler=dict(type='xavier')) #创建一个softmax层 loss = L.SoftmaxWithLoss(fc4, label) if include_acc: #在训练阶段，不需要accuracy层，但是在验证阶段，是需要的 acc = L.Accuracy(fc4, label) return to_proto(loss, acc) else: return to_proto(loss) def write_net(): #将以上的设置写入到prototxt文件 with open(train_proto, 'w') as f: f.write(str(create_net(train_lmdb,batch_size=64))) #写入配置文件 with open(val_proto, 'w') as f: f.write(str(create_net(val_lmdb,batch_size=32, include_acc=True))) if __name__ == '__main__': write_net() 上面的代码，我们一开始就import了两个包，一个是layers，另一个是params。layers里面包含了Caffe所以内置的层（比如卷积，ReLU等），而params则包含了各种枚举值。网上很少找到函数详解，自己凭着理解总结一下吧： 数据层12345678910data,label=L.Data( source=lmdb, #数据源，训练数据LMDB文件的位置 backend=P.Data.LMDB, #数据类型，本文是lmdb batch_size=batch_size, #batch大小 ntop=2, #输出数量，本文是data和label，所以是2 transform_param=dict(crop_size=40, #crop大小 mean_file=mean_file, #均值文件 mirror=True #镜像操作 ) ) 卷积层12345678conv1=L.Convolution( data, #数据流入（即从数据层得到的data） kernel_size=5, #卷积核大小 stride=1, #步长 num_output=16, #输出 pad=2, #填零 weight_filler=dict(type='xavier') #权重初始化方式'xavier' ) 激活层、dropout层12345678relu1=L.ReLU( conv1, #数据流入（即从卷积层得到的conv1） in_place=True #in_place ，就地运算，节省存储开销 )drop3=L.Dropout( relu3, #数据流入（即从激活层得到的relu3） in_place=True #in_place ，就地运算，节省存储开销 ) 池化层123456pool1=L.Pooling( relu1, #数据流入（即从激活层得到的relu1） pool=P.Pooling.MAX, #池化方式：最大池化 kernel_size=3, #池化核大小 stride=2 #步长 ) 全连接层12345fc3=L.InnerProduct( pool2, #数据流入（即从池化层得到的pool2） num_output=1024, #全连接输出数目 weight_filler=dict(type='xavier') #权重初始化方式'xavier' ) SoftmaxWithLoss层1234loss = L.SoftmaxWithLoss( fc4, #数据流入（即从全连接层得到的fc4） label #数据流入（即从数据层得到的label） ) Accuracy层12345678if include_acc: #在训练阶段，不需要accuracy层，但是在验证阶段，是需要的 acc = L.Accuracy( fc4, label ) return to_proto(loss, acc)else: return to_proto(loss) 总结上面那种是一层一层往上累加的，最后返回了最后一层。当然如果直接建一个caffe.NetSpec()，会有一个整体的把握：123456def mynet(lmdb, batch_size): n = caffe.NetSpec() ################### n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb, transform_param=dict(scale=1./255), ntop=2) ################### return n.to_proto() 方法的参数中的lmdb是指Caffe支持的数据库的一种，叫lmdb，我们传入数据库的路径即可。而n=caffe.NetSpec()是获取Caffe的一个Net，我们只需不断的填充这个n，最后面把n输出到文件。在填充的时候要记得加上n.。各层的具体参数可以参考caffe.proto。总的来说，这种方式生成prototxt要简单一些，代码量比直接写要小，而且层的输入输出清晰：输入是第一个参数，输出是返回值。不过也要注意不同参数的数据格式，如dict(type=&#39;xavier&#39;)。 caffe的python接口学习（1）：生成配置文件Deep learning tutorial on Caffe technology : basic commands, Python and C++ code.Caffe学习4-利用caffe.proto自定义自己的网络]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2016-2017 新年，新开始]]></title>
    <url>%2F2017%2F01%2F01%2F2017-newyear%2F</url>
    <content type="text"><![CDATA[今天是2016的最后一天，并没有过的轰轰烈烈，似乎也有些对不起这样重要的一年。 一生中重要的事情往往不多，大部分都是处在一种“量变”的状态，“质变”不是说来就来的，而是需要有这样的一个机会。机会如何把握呢？有句话是“机会总是给有准备的人”，似乎就是在说“量变”的重要性。但是我可能还是理解错了。 首先，重要的事情之所以重要，不仅仅体现在这一件事情上，而是会接连影响到之后的事情，决定着事物发展方向。在学业的这段时间里，中考决定了高中生活，高考决定了大学生活，找工作决定了工作生涯……几年的生活工作往往就决定于一两天的选择。 第二，“量变”到底决定的是什么？每天好好学习，学习成绩就会有所提升，反映到升学上，就是选择的更多，即有更多更好的资源和前景，未来的生活状况的平均水平会有所提高。再好的学校也会有成绩、能力不足的人，再差的学校也会有大神级别的人物，但不同学校的分布水平不同。所以能去好一点的学校肯定不回去差的。 第三， 机会是什么？机会等于有更多的选择空间吗？假如我足够优秀，全世界的学校专业我都可以挑选，我就是成功的吗？或许只说对了一半。这只代表我有能力上学，却没有回答为什么学的问题。这个最重要的问题常常被忽视掉。因为这个问题的答案只有每个人自己知道，别人是不会关心的，或者说别人只是站在别人的角度去考虑。当别人一次次展示自己的成功时，往往会淡忘掉自己的目标和理想。每个人还是不一样的。 实际上有很多东西没有考虑清楚就上路了，却发现已经没法调头。2016就是这样一个重要的一年，重要的不在于成功地拥有了很多选择，而在于应该去如何选择。 二十多年来，经历了无数事情，唯独选择做得少，这也导致我在这方面的能力不足，特别是选择之后才明白一些事情，此时就晚了。因此信息是很重要的，特别是在信息不对称时，很容易处于被动。而当自己不知所措时，不能听从他人，而要问问自己，问一问过去的自己，究竟所要的是什么。 还有一点就是争取，办不成的事要再试试，不要总把自己的需求和愿望当作是在麻烦别人。同样的，别人拜托自己所不想做的事情，也不要勉强去做，要学会拒绝。 走得越远，越发现自己曾经的思维、方法是跟不上大环境的，还好离真正的社会也有一段距离。希望能慢慢提升自己，成为一个成熟的人。]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>新年</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12月观影]]></title>
    <url>%2F2016%2F12%2F28%2F12%E6%9C%88%E8%A7%82%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[在12月看了三场电影，一次在成都，两次在北京，算是频率比较高的了。分别是：《你的名字。》《三少爷的剑》《长城》。其实我很少看国产电影，第二部是师兄请客，第三部是圣诞节准备看电影但是实在没有什么评价很好的电影了才看的。印象中上次的国产电影是去年的跨年，看的《老炮儿》。可惜这两次观影又让我对国产失望了一分。 先说说三少爷吧。不吐槽人物装扮、演技的话，从剧情上，燕十三可能是最“正常”的一个人了。全剧围绕一个大备胎，备胎还被妹子误杀，备胎又把妹子杀了，男主一脸懵逼只好把仰慕自己基友杀了，最后和青楼女子生活在了一起（误……燕十三一心想打败三少爷，却在即将离世的前几周教伪装成平民（退隐江湖）的三少爷剑法，这好像是最大的幽默了。除了教剑法，还教做人处事，让三少爷明白了“哦原来不杀人也能做成事啊”。三少爷最无语的地方（也是同学原著党跑出电影院的地方），就是直到后来杀了苗子全村的慕容秋荻问谢晓峰“你还愿意再娶我一次吗？”谢晓峰说“嗯。”男主真是让人喜欢不起来啊~ 《长城》比三少爷还好一点，毕竟是做了尝试。 《长城》是由中国电影股份有限公司、乐视影业、传奇影业、环球影业联合出品，由中国导演张艺谋执导，马特·达蒙、景甜、佩德罗·帕斯卡、威廉·达福、刘德华、张涵予等联合主演的奇幻动作片。 这阵容真是中西融合，得知主题是抵抗饕餮，联想到了《独立日》和《星河战队》，这就是把传统的在太空打外星人改到了在古代打怪兽嘛。是把西方个人英雄主义和爱国主义思想（？）的一次结合。其实这两者在这类电影里面也是相通的，最后的任务往往只需要几个主角，他们成功了国家、世界才有救。具体这部电影，的确缺乏了亮点，也不知道是否是因为面对国际市场，做出来有点四不像，我也不想过多评价，3分/5分也差不多了。 神奇的是，这两天关于电影有了这样的新闻： 《摆渡人》《长城》等电影票房不如人意，豆瓣、猫眼要不要背锅？“张艺谋已死。”看完《长城》后，“亵渎电影”12月15日发了这么一条微博，他当时没有预料的是这条评论引发的后续效应将远远超过其控制范围。当天乐视影业 CEO 张昭就毫不客气地转发对骂，并以乐视影业官方名义发出了警告函，称将采取法律手段。今天人民日报客户端发文，以此为例批判“蓄意恶评伤害电影产业”。 相比之下是今年中国电影市场惨淡。如果只是这样，失去的可能不只是票房了，还有信心。]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>电影</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cs231n笔记1]]></title>
    <url>%2F2016%2F12%2F21%2Fcs231n%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[斯坦福深度学习与机器视觉课程cs231n，感觉挺不错的，顺便记下来一些零碎的点，不过具体内容还是要参考笔记、视频。不过网易云课堂的视频还是有些问题的。 研究历史 一开始是对猫的视觉进行研究，发现有如下神奇的特点： 对于整个图像，猫的视觉基础神经元没有被激活。 在切换图像时，神经元被激活。 因此研究人员认为神经元对简单形状、边缘有反应。 David Marr提出视觉是分层的。 1234567st=&gt;start: 输入图像e=&gt;end: 结束op1=&gt;operation: 边缘信息op2=&gt;operation: 2.5D图像op3=&gt;operation: 3D图像st-&gt;op1-&gt;op2-&gt;op3 90年代感知分组，将图像分为不同区域。1999，SIFT特征，图像识别、匹配。2001，Face Detection，没有进行3D建模。 数据集： PASCAL 20类 IMAGENET 22K类，14M图 2011年及之前图像分类识别基本靠SIFT+SVM。其竞赛2012年出现CNN，AlexNet，7层layer夺得冠军。之后2014：GoogleNet，VGG；2015：MSRA深度残差，均基于CNN。 kNN分类器两图像距离的定义。 L1距离：像素间绝对值再求和 L2距离：像素间差的平方求和再开方 超参数：无法在训练中优化，如距离定义、k值。采用单一验证、交叉验证方法。 kNN流程： 计算已知类别数据中图像（train set）与需要预测点图像（test set）距离； 按照距离依次排序； 选取与需要预测图像距离最小的前k个； 确定前k个中每种类别出现的次数； 将出现次数最多的类别作为预测类别。 损失 SVM损失公式：$j\neq y_i$是指对于第$i$类，只对其他类进行求和运算；sj是第j类得分结果；s{y_i}是本类的得分结果；$\Delta$是常数，一般取1。这是某一类的损失，总损失可以求平均。 image cat score car score frog score losses CAT 3.2 5.1 -1.7 2.9 CAR 1.3 4.9 2.0 0 FROG 2.2 2.5 -3.1 10.9 正则化 正则化项避免过拟合，考虑更多输入。 L1正则：w权重绝对值之和 L2正则：w权重平方和 Softmax损失公式：计算过程： item cat car frog score 3.2 5.1 -1.7 exp 24.5 164.0 0.18 normalize 0.13 0.87 0 $L_{cat}=-log(0.13)=0.89$当初始时（w很小），L=-log(1/N)=log(N)。 CS231n Convolutional Neural Networks for Visual RecognitionCS231n官方笔记授权翻译总集篇发布斯坦福CS231n—深度学习与计算机视觉]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>cs231n</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文阅读：Hyper-class Augmented and Regularized Deep Learning for Fine-grained Image Classification]]></title>
    <url>%2F2016%2F12%2F20%2FHyper-class-Augmented%2F</url>
    <content type="text"><![CDATA[介绍 难点： 精细粒度的标记数据的获取要昂贵得多（通常需要专业领域）; 存在大的类内（intra-class）和小类间（inter-class）方差。 目前训练策略：在已有模型上预训练CNN并在小规模数据集上fine-tune。 本文工作： 使用hyper-class来增强数据，从网络上搜索hyper-class-labeled的数据，形成多类学习任务。 公式化精细识别模型和hyper-class识别模型，通过挖掘二者的关系提升识别率。 增强数据： super-class，包括一系列的精细类别。 factor-type hyper-classes 因子类型。提出超类增强和正则化深度学习（hyper-class augmented and regularized deep learning）。学习框架与神经网络与多任务学习密切相关。这个想法是通过允许他们共享神经网络的相同特征层来联合训练多个相关任务。 超类增强和正则化深度学习 解决第一个难点，用由一些超类标记的大量辅助图像来增强细粒度数据。第二个难点，提出新的CNN模型。 超类数据增强 super-class，包括一系列的精细类别。 factor-type hyper-classes 因子类型。 对于给定的细粒度（fine-grained class）类，图像可能有不同的视角视图，即因子类型超类（factor-type hyper-classes）。这和super-type hyper-class 与 fine-grained classes完全不同。一个fine-grained class只能归属于一个单一的super-type class。比如fine-grained class “吉娃娃”属于super-type class“dog”，并不属于“cat”。然而对于汽车数据，其 fine-grained class可以有不同的视角，因此不必归为同一个单独的hyper-class。从生成的角度来看，可以通过首先生成其视图（hyper-class）然后生成给定的视图来生成汽车图像的fine-grained class。这也是我们下一小节描述的模型的概率基础。由于这种类型的超类可以被认为是图像的隐藏因素，这种类型称为factor-type hyper-class。super-type和factor-type 两种hyper-class之间的关键区别是：super-type是由fine-grained类隐含的，而factor-type对于给定的fine-grained类是未知的。factor-type另一个示例为人脸的不同表情（愉快，愤怒，微笑等），每个人都有不同表情的多张图像。 正则化学习模型 给定细粒度训练图像（fine-grainedtraining images）和辅助超类标记图像（auxiliary hyper-class labeled images），一个直接的策略是通过共享共同的特征和学习分类器来训练多任务CNN。在多任务学习中，hyper-class类和fine-grained类的标签集是不相交的，我们不用hyper-class标签来标记fine-grained的数据。 Factor-type Hyper-class Regularized Learning 给定图像x，识别结果为y的概率为：Pr(v|x)是任何 factor-type hyper-class v的概率，并且Pr(y|v,x)指定给定factor-type hyper-class和输入图像x的任何fine-grained class的概率。使用softmax函数对factor-type hyper-class概率进行建模，h(x)表示x的高层特征，即：其中uv表示hyper-class分类模型的权值，Pr(y=c|v,x)如下计算：其中w{v,c}表示factor-specific精细分类模型的权值，此时预测公式为：虽然我们的模型在混合模型中有其根，但是值得注意的是，不像大多数以前的混合模型，处理Pr(v|x)作为自由参数，我们将其制定为一个鉴别模型。它是 hyper-class增强图像，允许我们准确地学习{uv}。 然后我们可以记录Dt中的细粒度识别数据的负对数似然性和Da中的数据的hyper-class识别：为了激励非平凡正则化，我们注意到factor-specific权重w_{v,c}应该捕获与对应的factor-type hyper-class分类器uv类似的高水平factor-related特征。为此在w{v,c}和uv间引入正则化：为了理解正则化，在我们的汽车识别示例中，原始的fine-grained 数据不能够学习每视角类别分类器w{v,c}，因为没有办法推断视角hyper-class。但现在我们可以在hyper-class增强数据上训练视点分类器uv，因此正则化负责将知识传递到每个视角类别分类器，从而帮助模糊fine-grained任务中的类内方差。引入w’{v,c}=w_{v,c}-u_v，则上式化简为：$Pr(y=c|x)$由下式给出：可以看出，fine-grained 分类器与factor-type hyper-class分类器共享相同的分量$u_v$。 因此，它将所提出的模型连接到传统的浅层多任务学习中使用的权重共享。 Super-type hyper-class regularized learningsuper-type hyper-class正则化深度学习的唯一区别在于$Pr(y|v,x)$，它可以简单地建模因为super-type hyper-class $v_c$由fine-grained标签c隐含地表示。正则化为： 统一的深度CNN 使用hyper-class增强数据和多任务正则化学习技术，我们达到统一的深CNN框架，如图所示：其示了优化问题： Xie S, Yang T, Wang X, et al. Hyper-class augmented and regularized deep learning for fine-grained image classification[C]// IEEE Conference on Computer Vision and Pattern Recognition. 2015.车型识别“Hyper-class Augmented and Regularized Deep Learning for Fine-grained Image Classification”]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>fine-grained</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe自定义层]]></title>
    <url>%2F2016%2F12%2F19%2Fcaffe%E8%87%AA%E5%AE%9A%E4%B9%89%E5%B1%82%2F</url>
    <content type="text"><![CDATA[以后可能要在caffe内做一个判断的层，先学一下怎么添加新层。 developing new layer 开发一个新层 添加一个层的类声明到：include/caffe/layers/your_layer.hpp。 包括type的内联实现方法覆盖virtual inline const char* type() const { return &quot;YourLayerName&quot;; }，将YourLayerName替换为你的层名称。 实现{*} Blobs()方法来指定blob数量要求； 参阅/caffe/include/caffe/layers.hpp以使用内联{*} Blobs()方法强制执行top和bottom Blob计数。 如果你只实现CPU代码，则省略* _gpu声明。 在src/caffe/layers/your_layer.cpp中实现层。 （可选）用于一次性初始化的LayerSetUp：读取参数，固定大小的分配等。 Reshape用于计算top blob的大小，分配缓冲区以及取决于bottom blob的形状的任何其它工作。 Forward_cpu用于层的计算 Backward_cpu用于其反向梯度传播（可选 - 图层可以是仅向前传播） （可选）在layers/your_layer.cu中实现GPU版本Forward_gpu和Backward_gpu。 如果需要，在proto/caffe.proto中声明参数，使用（然后增加）”next available layer-specific ID”(“下一个可用的特定于层的ID”)在需要的message LayerParameter之上。 使用layer_factory.hpp中提供的宏在cpp文件中实例化并注册层。 假设有一个新层MyAwesomeLayer，可以使用以下命令实现它： 12INSTANTIATE_CLASS(MyAwesomeLayer);REGISTER_LAYER_CLASS(MyAwesome); 注意，应该将注册代码放在自己的cpp文件中，因此实现层是自包含的。 或者，如果您的图层有多个engines，也可以注册Creator。 示例见：caffe/layer_factory.cpp中的GetConvolutionLayer。 在test/test_your_layer.cpp中写入测试。 使用test/test_gradient_check_util.hpp来检查Forward和Backward。 仅向前传播层 如果写一个只包含在测试网络中的层，可不必编写反向传递。 可以在include/caffe/your_layer.hpp中编写一个Backward_cpu（或Backward_gpu）的内联实现以及您的图层的定义，如下所示：123virtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) &#123; NOT_IMPLEMENTED;&#125; NOT_IMPLEMENTED宏（在common.hpp中定义）会抛出一个错误日志“尚未实现”。 例如，查看准确度层（accuracy_layer.hpp）和阈值层（threshold_layer.hpp）定义。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python文件命名小脚本]]></title>
    <url>%2F2016%2F12%2F12%2Fpython%E6%96%87%E4%BB%B6%E5%91%BD%E5%90%8D%E5%B0%8F%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[写个文件命名的python程序，复（yu）习一下python。程序写得应该不是很好T T。。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import os;import shutil;from PIL import Image;##输入为图片路径##命名方式为统一位数数字递增##单一文件格式查找##输出为：## result.txt 文件名，是否为完整图片## \full 完整图片库## \part 不完整图片库path="D:\mydatabase";#设置工作目录os.chdir(path)#更改工作目录f=open("result.txt","w");#打开输出到的txt文件def rename(begin,weishu):#批量重命名，begin：起始数。weishu：数字位数 count=begin; filelist=os.listdir(path)#该文件夹下所有的文件（包括文件夹） for files in filelist:#遍历所有文件 Olddir=os.path.join(path,files);#原来的文件路径 addstr="";#补增0 if os.path.isdir(Olddir):#如果是文件夹则跳过 continue; addwei=weishu-len(str(count));#计算需要填多少0 for i in range(addwei): addstr+="0";#增加到所需位数 filename=os.path.splitext(files)[0];#文件名 filetype=os.path.splitext(files)[1];#文件扩展名 if filetype!=".jpg":#如果不为jpg文件则跳过 continue; string=str(count);#将计数值转为字符 newfilename=addstr+string+filetype;#合成最终文件名 f.write(newfilename+" ");#输出文件名到文件 Newdir=os.path.join(path,newfilename);#新的文件路径 os.rename(Olddir,Newdir);#重命名 classify(newfilename)#进行分类操作 count+=1;#计数加一 print(newfilename);#打印当前处理文件名 f.close(); def classify(filename):#分类函数，filename：所需分类文件名 img = Image.open(filename)#打开所要分类的图片 imgsize=img.size;#图像大小 full=1;#是否完整，1为完整 w=imgsize[0]; h=imgsize[1]; rate=float(w)/float(h);#长宽比 if rate&lt;0.67 or rate&gt;1.5 or w&lt;200 or h&lt;200: full=0; if full: if not os.path.isdir("full"):#是否存在full目录 os.mkdir("full");#新建full目录 Newdir0=os.path.join(path+"\\full",filename); shutil.copyfile(filename,Newdir0);#复制文件 f.write("1"+"\n");#输出是否完整标记 else: if not os.path.isdir("part"):#是否存在part目录 os.mkdir("part");#新建part目录 Newdir0=os.path.join(path+"\\part",filename); shutil.copyfile(filename,Newdir0);#复制文件 f.write("0"+"\n");#输出是否完整标记 rename(0,6);#运行]]></content>
      <categories>
        <category>Code</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO：You Only Look Once 论文阅读]]></title>
    <url>%2F2016%2F12%2F11%2FYOLO%EF%BC%9AYou-Only-Look-Once-%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[You Only Look Once: Unified, Real-Time Object Detection YOLO官网论文阅读笔记：You Only Look Once: Unified, Real-Time Object Detection 简介与特点YOLO是今年CVPR上提出的一种目标检测方法，其速度达到了45fps（YOLO v2 达到了67fps），完全可以处理视频。其框架是直接利用CNN的全局特征预测每个位置可能的目标，比RCNN先检测多个RP再CNN快了很多。 实际测试中，检测提供的dog.jpg图片（768*576）使用了0.043s，GPU是K40，也到了20fps以上。其大致流程为： 缩放图片到448x448； 运行单个CNN； 通过模型的置信度阈值得到检测结果。 除了速度快这一特点，还有： 背景误差小YOLO在预测时考虑全局图像，与滑动或region proposal只提出图像中一块区域不同。因此更能得到分类目标附近区域信息， 与快速R-CNN相比，YOLO的背景误差数量少于一半。 学习对象的可概括表示当训练对象为自然图像，而测试图像为艺术作品时，有很好的检测率。 不足精度相比于最先进的检测系统，但YOLO v2 已经达到了76.8的mAP。 统一检测YOLO将分离的组件检测器统一到一个神经网络中。网络使用整幅图的特征来预测bounding box，同时进行分类，网络全局地考虑整个图像和图像中的所有对象。 首先把图像分成$S \times S$个网格，如果对象的中心落入网格单元中，则该网格单元负责检测该对象。每个网格单元预测这些框的$B$个bounding box和置信度得分。 这些置信度分数反映了box包含某种对象的自信程度，以及它对框预测的box的准确程度。定义置信度为： $$Pr(Object)*IOU_{pred}^{truth}$$ 如果该网格中没有目标，则Pr项值应该为0，整体为0，否则为1。每个bounding box包括5个预测值：x，y，w，h和置信度。$(x,y)$坐标表示相对于网格单元的边界的框的中心。每一块需要预测的值是bounding box相对于该块中心的偏移，以及相对长宽，类别。每个网格单元还预测$C$条件类概率$Pr(Class_i|Object)$。 这些概率以网格单元包含对象为条件，即 最终预测结果为$S\times S\times (B * 5+C)$的张量。 网络设计模型以CNN实现，网络的初始卷积层从图像中提取特征，而完全连接的层预测输出概率和坐标。网络有24卷积层，其次是2完全连接的层。如图所示：输出为$7 \times 7 \times 30$的张量。每个grid有30维，这30维中，8维是回归box的坐标，2维是box的confidence，还有20维是类别。 其中坐标的x,y用对应网格的offset归一化到0-1之间，w,h用图像的width和height归一化到0-1之间。 训练 最终层预测类概率和边界框坐标。 通过图像宽度和高度将边界框宽度和高度归一化，使得它们落在0和1之间。我们将边界框x和y坐标参数化为特定网格单元位置的偏移，使得它们也在0和1之间 。 最后一层使用线性激活函数（linear activation function），其他层使用leaky rectified linear activation： $$\phi (x)=\begin{cases}x, &amp; \text{if $x&gt;0$}\\ 0.1x, &amp; \text{otherwise}\end{cases}$$ 优化了模型输出中的平方误差。使用平方误差，因为它很容易优化，但它不完全符合我们的最大化平均精度的目标。 It weights localization error equally with classification error，即8维的localization error和20维的classification error 同样重要，这可能不是理想的。 此外，在每个图像中，许多网格单元不包含任何对象。 这将这些网格单元的“置信度”为零，通常压倒包含对象的网格的梯度。 这可能导致模型不稳定，导致训练早期发散。为了弥补这一点，增加了边界框坐标预测的损失，并减少了对不包含对象的框的置信预测的损失。 使用两个参数： $$\lambda_{coord}=5$$ $$\lambda_{noobj}=0.5$$ 来完成这个。通过这样的因子设置，更重视8维的坐标预测，即给这些localization error损失前面赋予更大的loss weight, 更小的classification error 。对不同大小的box预测中，相比于大box预测偏一点，小box预测偏一点肯定更不能被忍受的。而sum-square error loss中对同样的偏移loss是一样。 预测边界框宽度和高度的平方根，而不是宽度和高度。在训练期间，我们只需要一个bounding box预测器负责每个对象（原本每个网格单元会有B个预测）。 我们将一个与ground truth相比的最高IOU的预测器指定为“负责”。loss function：其中$1i^{obj}$表示如果对象出现在单元格i中，$1{ij}^{obj}$表示网络单元格i中的第j个bounding box预测器对于该预测是“负责”的。如果对象存在于该网格单元中，则损失函数仅惩罚分类误差（因此前面讨论的条件类概率）。 它也只惩罚边界框坐标误差，如果该预测器是对ground truth “负责”的。即： 只有当某个网格中有object的时候才对classification error进行惩罚。 只有当某个box predictor对某个ground truth box负责的时候，才会对box的coordinate error进行惩罚，而对哪个ground truth box负责就看其预测值和ground truth box的IoU是不是在那个cell的所有box中最大。 测试结果如表：特别的，作者还进行了艺术作品中的检测识别，以测试其泛化性能。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
        <tag>YOLO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[愿饮一盏口嚼酒]]></title>
    <url>%2F2016%2F12%2F05%2F%E6%84%BF%E9%A5%AE%E4%B8%80%E7%9B%8F%E5%8F%A3%E5%9A%BC%E9%85%92%2F</url>
    <content type="text"><![CDATA[0 愿饮一盏口嚼酒，可唤三载牵丝梦。 1单单就这部电影来说，并不是一个悲剧，当时我也并没有哭，这样的结局，真好。 重要的人，不能忘记的人，不想忘记的人 是的，这是人，而并不是一个名字。用笔写在书上，记在日记里，写在你我手上，名字是毫无意义的。就像它出现在遇难者名单中，只是一个冰冷的名字罢了。幸好在你手上写的是“喜欢你”啊。这样才会有相互吸引的动力吧。三叶为什么能够割去头发赶赴东京，泷又如何会独自跑到荒凉的山谷，三年前已经有了答案。 2一条短短的绳结系住了二人。 聚拢，成形，捻转，回绕，时而返回，暂歇，再联结。这就是组纽。这就是时间。这就是产灵（musubi） “产灵”可能没有很好的表达出它应有的含义，但我想每个人都知道这是一种什么样的东西。这是祖母所说的，而祖母与三叶四叶经常在做一些绳结。不同颜色的丝线纵横交错，一旦成节，就难以分开。我们文化中类似的正如中国结，比这样一个绳带要复杂多了。 等待就是它本身的目的。不一定等到甚么，只要等，连系就在。——梁文道 名字忘了一次又一次，而结不散。 3我最感兴趣的是口嚼酒，从一开始制作的过程我就被吸引了：少女嚼下米饭再吐出，经过自然发酵形成酒。而承装米的盒子，同样是用红色的绳线所系住。不知道为什么我觉得这一过程很美。这代表了三叶最重要的东西，是她的“一半”（灵魂的一部分，或者说是神明与现实的纽带）。或许这是陨石撞击后三叶唯一能够留下来的东西吧，而泷喝下了它才得以再次回到过去。 水也好，米也罢，还有酒，什么东西进入身体的过程，也叫作产灵（musubi）。进入身体的东西，会和魂相结合。所以今天的奉纳，是宫水一脉传承百年，让人和神灵相联结的重要的传统。 喝下口嚼酒是进入身体的结合过程，那么制作口嚼酒，便是如同系结一般。三年前三叶除了给了泷自己头上发带的绳结，能够让二人相遇相识，更是“准备”了灵魂的绳结。泷和三叶能以这样的一种方式结合，真的很好。 4喜欢只有相互才是美好的，很羡慕这样的人，也为他们祝福。可是如何才能满足这个条件呢，交换身体是一个好的选择吗？能够喜欢别人并去追寻是一个很重要的技能，但另一方面，知道别人喜欢自己，自己也能喜欢对方却是可遇不可求的。就像三叶能从乡下来到东京，就可以正好碰到了列车上的泷，这种可能性有多少呢？而从嘈杂的列车上，泷又能记得住一个只有一面、一带之缘的人的名字吗？或许很多人都觉得这只是电影中剧情的需要吧。 可我确相信，的确会有这样的事情发生，但是你永远不可能提前预知，因为真正能够触动内心，变成不想忘记的人还是少数，大多数只是匆匆的过客。或许等到一两年后，甚至七八年后，对往事重新回忆，才发现原来结早已牢牢系好。 祖母说她们家族一直都有这种“交换身体”的梦，三叶父亲母亲是否也交换过呢？我感觉可能没有，因为他之前很少理解三叶的变化，虽然说最后父亲还是相信了女儿进行了演习，这可能只是一种信任。如此看来交换身体也并非是一个达到相互喜欢好的选择啊。三叶帮泷去和前辈约会，自己却不知为何在镜子前落下了眼泪；泷与前辈身在一起，心却不在：电梯里努力不让自己碰到前辈，在展览馆看到糸守町的图片，伫立许久……因为他们已经知道了，谁是 重要的人，不能忘记的人，不想忘记的人。 5君の名は.这句话没有用问号，心中已经有了答案，看似是最重要的元素，却又是无足轻重的代号。五年后，我们知道了泷已经毕业，希望成为一个即使城市突然消失，也能能够让城市留下些什么的建筑师。而更重要的是三叶的故事，从乡下来到城市，应该比泷更辛苦一些。最后泷从阶梯又下到上，三叶由上至下，相遇无言，却又猛然回头相问，可能是对他们最好的结局吧。 End?仔细想想，哭和笑一样只是感情的表达而已，即使是喜剧，我们被泷和三叶的追寻过程所感动，为最终的相遇而喝彩，都是可以用这种方式表达的。那么我，或许是没有到最后哭或笑的时候吧。 写于2016.12.3 17:00，成都]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>电影</tag>
        <tag>你的名字。</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一次选举]]></title>
    <url>%2F2016%2F11%2F29%2F%E7%AC%AC%E4%B8%80%E6%AC%A1%E9%80%89%E4%B8%BE%2F</url>
    <content type="text"><![CDATA[好像从初中课本上就已经知道每个人拥有选举权和被选举权，每个成年人都有选举的权利。可是至今没有见过选票是什么样的，似乎枉做了一位中国公民。但昨天突如其来的一份选举通知让我有了这样一个机会，我也可以当家作主一次了。一同发来的还有一份候选人资料，上面列了四名候选人以及其工作经历，没有像班级里面的选举，至少应该有个竞选宣言吧。看了一遍自己毫无感觉，完全不知道应该投给谁。如果说上面附上每名候选人的照片，我可能就会以貌取人选择长得不错的，可惜别人肯定看透了这个小心思，四名女候选人都没有照片，那么该如何评选呢？ 投票时间是上午9:30到10:40，为了减少绕路，出一次门干完所有事，我选择临近结束投票，之后正好赶上午饭。投票地点在学院的小房间，做个小活动开个小会都在这里。两名工作人员，一个还拿着手机应该是在拍照录像。这突然就显得正式严肃了很多。首先要填一个选民证，是自己的姓名年龄之类的。可是工作人员也没有查验我的身份，仅凭自己填就可以投票，似乎不是很合理，万一有敌对势力破坏选举，或者选票造假呢？班上其他需要选举的人好像有的没有来，完全可以顶替投票嘛。在这之后终于拿到了庄严的一票：之后呢，也要投好庄严一票。……可面前只有四个名字，从昨天见到到今天投票还不过一天，班干部竞选也要经过几个月呢。没办法，既然来了还是要投的，看哪个顺眼选哪个吧。首先选了一个名字有三个字的（其他都是两个字），之后又把第一个和最后一个选了，有始有终。那么第二个就无法再被选，也只好这样。打上圆圈，把选票交还给工作人员，我也就离开了。 吃饭的路上，发现出太阳了真是美好的一天——也是第一次行使自己选举权的一天。然而我突然感到有些对不起第二个候选者，万一她才是有政见、有能力的人呢？这种感觉很奇怪，明明行使了自己的权利，却感觉和没有一样，唯一值得纪念的或许只是见到了选票？在没有了解候选人的情况下投了票，或许还不如弃权呢。同学没有去，说结果肯定是内定的，我却不是很赞同。我相信投出的选票会真正被统计并选出得票较多的人，不过不同的选举结果会为社会带来多大的改变，却很难得知。但是既然作为了选民，经历了选举，我可以对别人说中国是有民主的，因为是我选出了人大代表。不过如果有人继续问下去，我可能就无法回答了，或许需要等到我成为候选人的那一天才会有答案吧。]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>选举</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine learning- Trends, perspectives, and prospects]]></title>
    <url>%2F2016%2F11%2F26%2FMachine-learning-Trends-perspectives-and-prospects%2F</url>
    <content type="text"><![CDATA[论文阅读：Machine learning: Trends, perspectives, and prospects绪论 机器学习强调让电脑通过学习自动地提高自己。其发展基于： 新的学习算法和理论 可用数据增多和计算成本变低 可以看到，机器学习的这几年的流行是多方面的因素，第二点的作用甚至更大一些：作为机器学习“燃料”的数据不仅提高了准确性，更避免了过拟合，提升泛化性能；计算速度的大幅提高，才让九十年代末进入瓶颈的算法得以真正运行，并且应用到较为复杂的图像领域，使深层网络成为可能。 其关注问题在于： 如何构建一个让机器通过经验（experience）学习从而得到提升？ 什么是掌控所有学习系统基本统计计算理论？(What are the fundamental statistical-computational-information-theoretic laws that govern all learning systems, including computers,humans, and organizations?) 第一个问题较为容易理解，这是一个很实际的问题。比如我们可以定义损失函数来评价模型目前的偏差，并想方设法减小这些偏差，我们的模型也就得到了提升。这种反馈的思想也早已在电子、控制行业中采用，只不过用途不尽相同：机器学习的目的是预测甚至是生成，即输入未知数据来得到分析之后的属性，或通过分析产生新的数据；而电子控制中的目的是稳定，在干扰的条件下将所控制的值控制到我们想要的值上。第二个问题很难回答。目前在人脑的机理尚未完全了解的情况下，我们已经开始尝试通过纯计算来达到学习的效果。可能我们做的只是不断地优化，但效果已经显现。 通过二十年的发展，机器学习已经在很多方面得到了应用，原文也举了很多例子。 学习问题可以被定义为当通过某种类型的训练经验来执行某个任务时改善一些性能度量的问题。 增强性能减小误差是学习的目标，改变误差的定义往往会达到不同的效果，比如对label true 而output false 的结果（未检出）加大惩罚，就会达到“宁可错杀一千不可放过一个”的效果。当然这只是有客观结果的学习，对于无客观结果，如融合两幅画的风格生成新的一幅画，很难用人的判断去决定误差大小。 许多算法关注于函数的近似问题。 传统回归问题也是一种函数的近似，而当我们我们把图片看作是函数输入$x$，分类结果看作是输出$y$，那么网络可以看作一种较为复杂的函数$f(x)$（简单说是经过了很多层的矩阵运算）。传统我们可以根据公式，使用不同模型（多项式、高斯函数）对输入输出进行回归拟合，达到预测效果。而如今数据量太大，很难得到解析解，这就需要不断地改变参数从而使误差减小。 无论什么学习算法，一个关键的科学和实践目标是在理论上描述特定学习算法的能力和任何给定学习问题的固有困难：算法如何准确地从特定类型和大小的训练数据学习？算法对于其建模假设中的误差或训练数据中的误差有多鲁棒性？给定一个给定量的训练数据的学习问题，是否有可能设计一个成功的算法或这个学习问题根本难以处理？ 也正是因为采用不断迭代更改参数提高性能的原因，一旦模型确定，我们很难再去更改学习后模型内的参数。输入的数据也不同，得到的模型结果很难是相同的。这样一切得到的结果就是数学上的概率。 Drivers of machine-learning progress 过去十年，网络和移动计算系统收集和传输大量数据的能力迅速增长，这种现象通常被称为“大数据”。收集这些数据的科学家和工程师经常转向机器学习，并从这些数据集获得有用的预测和决策。事实上，数据的绝对尺寸（sheer size）使得必须开发出可扩展的程序，同时考虑计算和统计，但问题不仅仅是现代数据集的大小；它是许多这些数据的粒状（granular），个性化的性质。 大数据差不多也是和机器学习同时兴起的，这是十年中数据指数型上升的必然结果。同样数据也从集约化的数据库（由工程师指定数据的项目、输入的格式），变成了发散的个性化的内容，每个人都可以是数据的制造者。这对数据的处理和分析带来了一定的困难。 Core methods and recent progress下面是核心方法和目前进展 The most widely used machine-learning methods are supervised learning methods 目前看来还是监督学习是使用最广泛的。这也就是上面说的函数拟合问题了：通过学习映射f（x）形成它们的预测，其为每个输入x（或给定x的y上的概率分布）产生输出y。当然存在许多不同形式的映射f，包括决策树，决策树，逻辑回归，支持向量机，神经网络，内核机器和贝叶斯分类器。不过这些不是重点，重点在于深度学习。 深度网络是阈值单元（threshold units）的多层网络，每个网络计算其输入的一些简单的参数化函数。 目前的深度网络在利用GPU的并行计算下，参数已经达到了数十亿，真是一个庞大的数字。。不过效果是越来越智能了，下图是两个例子： 尽管深度学习的大量实际成功来自用于发现这种表示的监督学习方法，但是还努力开发不需要标记训练数据的深度学习算法。一般问题被称为无监督学习，机器学习研究的第二范例。广泛地，无监督学习通常涉及在关于数据的结构性质（例如，代数，组合或概率）的假设下对未标记的数据的分析。 非监督学习是接下来的一个研究重点，Yann LeCun 在CMU的演讲上也特别提到了这一点（演讲内容）。未来的机器学习不仅要能学会，还要能通过学习到的进行创造，如生成对抗网络，视频预测等。 回归到论文中，第三个主要的机器学习例子是强化学习（ reinforcement learning ）。这里训练对象与外部环境有很多的互动。基本原理为： 如果Agent的某个行为策略导致环境正的奖赏(强化信号)，那么Agent以后产生这个行为策略的趋势便会加强。Agent的目标是在每个离散状态发现最优策略以使期望的折扣奖赏和最大。 Emerging trends 一个主要趋势是对机器学习算法操作的环境的日益关注。词语“环境”在这里部分地指计算架构；而经典的机器学习系统涉及在单个机器上运行的单个程序，现在通常将机器学习系统部署在包括数千或十万个处理器。 文中的例子是购物推荐系统，这是一个很实用并且应用范围很广的系统。对于每个人可以独立地学习其购物习惯，而对于一类人也可以有一个整体的学习和分类。大小不同层次和类别，可以让机器学习更有针对性。小的可以部署在个人客户端，大的可以在服务器，这样也充分利用了资源。 词语“环境”还指数据源，其范围从可能具有隐私或所有权关注的一组人，到可能对机器学习系统具有某些要求的分析员或决策者（例如 ，其输出是可视化的），以及围绕系统部署的社会，法律或政治框架。 环境还可以包括其他机器学习系统或其他代理，并且系统的整体集合可以是合作的或敌对的。 广义地说，环境向学习算法提供各种资源并对这些资源施加约束。 机器学习研究人员越来越正式化这些关系，旨在设计在各种环境中可证明有效的算法，并明确允许用户表达和控制资源之间的权衡。 这里的环境是更大的环境，甚至超出了算法的范围。 之后是分布式的学习系统。学习是一项很耗费资源的计算活动，单个区域的计算资源不足时，采用分布式计算会大大提高效率。这里需要解决的是通信的问题了。 最终目标是除了精度要求之外还能够向机器学习系统提供时间和空间预算，系统找到允许实现这种要求的操作点。 Opportunities and challenges目前的机器学习方法，是我们在如人类和其他动物，组织，经济和生物进化中观察到的学习类型。机器学习由简到难，这导致一些研究人员开始探索如何构建计算机终身学习者或永无止境的学习的问题。或者说通过学习能否找到一种最佳的学习方法从而自我学习。这样的话计算机是否就拥有了自我进化的能力呢。与自然学习系统类比的另一个方面提出了基于团队，混合主动学习的想法。 当然目前很大的问题在于数据来源与隐私。真正的大数据往往只存在于少数的几家大型互联网公司，而这些数据的来源就是网上每名用户。或许你正在使用免费的网盘服务，随时同步的照片为你带来了方便。但这些照片对于机器学习来说是最好的原料，而这些公司是否会在用户不知情的情况下拿来使用分析呢？从用户个人来看可能很难接受，但每一张单独的照片对于机器学习无关紧要，只有大量整体的数据才能达到最好的效果，比如非监督学习。由此带来的社会问题也是值得我们思考。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>论文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[from wordpress to hexo 搬迁成功]]></title>
    <url>%2F2016%2F11%2F18%2Ffrom-wordpress-to-hexo-%E6%90%AC%E8%BF%81%E6%88%90%E5%8A%9F%2F</url>
    <content type="text"><![CDATA[搬迁成功！之前网站建立在树莓派服务器上，很大的问题在于没有固定的IP，经常找不到服务器。。 经过一段时间的探索，发现github.io真是建站神器，免费的空间还没有什么限制。唯一一点可能是在PKU更新内网无法访问……这也导致一开始的尝试一直处于失败状态。（另外git clone coding也失败，这个不是内网吗？） 注意的问题当然在部署时还是有一定的问题出现： Deployer not found 当键入hexo d时会出现 1RROR Deployer not found: 这是因为git的部署器没有安装，需要安装后： 1npm install hexo-deployer-git --save 再执行。 当然遗憾的是还是出错： 123456789101112131415 NFO Deploying: gitINFO Setting up Git deployment...'git' FATAL Something's wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.htmlError: spawn git ENOENT at notFoundError (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:11:11) at verifyENOENT (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:46:16) at ChildProcess.cp.emit (F:\myhexo\node_modules\cross-spawn\lib \enoent.js:33:19) at Process.ChildProcess._handle.onexit (internal/child_process.js:215:12)FATAL spawn git ENOENTError: spawn git ENOENT at notFoundError (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:11:11) at verifyENOENT (F:\myhexo\node_modules\cross-spawn\lib\enoent.js:46:16) at ChildProcess.cp.emit (F:\myhexo\node_modules\cross-spawn\lib \enoent.js:33:19) at Process.ChildProcess._handle.onexit (internal/child_process.js:215:12) 跟当初wordpress一样总会留个问题啊。。 注意站点设置文件与主题设置文件的区别 这个在Next的设置中其实也有提示，然而我并没有很注意= =||一开始多说的评论设置在站点设置文件更改没有问题，但菜单始终失败，最后发现是应该在主题设置文件更改。 下一步之前wordpress的文章基本上都迁移过来了，但是图片就比较麻烦了，需要重新整理，抽空弄一下吧。Hexo生成的是静态网页，有一点不好是非常依赖hexo程序的生成，这样就不能随时随地发了（当是发微博吗？）……总之小站重新复活，好好干~ 感谢Hexo，感谢Next主题对本博客的大力支持。]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe-faster-rcnn demo测试]]></title>
    <url>%2F2016%2F11%2F12%2FCaffe-faster-rcnn%20demo%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[RCNN是目前detection中较新且准确度较高的方法，充分发挥了CNN分类的优势，但速度并不快，从而产生了fast rcnn和faster rcnn来解决这个问题。本文使用py-faster-rcnn对该方法做一初步测试。 rbgirshick/py-faster-rcnn 环境准备软件环境 Caffe Python 一般来说这些我们都已经有所接触，但仍有一些需要注意的地方： 要使用rbgirshick/py-faster-rcnn中的caffe编译一次，其caffe在rbgirshick/py-faster-rcnn/caffe-fast-rcnn @ 0dcd397中。因为这里面有一些专门为f-rcnn写的层，具体区别可以在caffe.proto中查看，如增加了ROIPoolingParameter、SmoothL1LossParameter等参数。 编译时一定要增加对Python层（Python layers）的支持。具体需要打开Makefile.config，找到： 12# In your Makefile.config, make sure to have this line uncommentedWITH_PYTHON_LAYER := 1 将其改为1，否则运行时会出错，提示没有对应的layer。 硬件要求小的网络用Titan, K20, K40这些就可以，显存3G以上。大的可能需要K40,11G以上显存，当然这些往往个人无法搭建起来。 安装（DEMO） 编译Cython模块 12 cd $FRCN_ROOT/libmake $FRCN_ROOT为你的FRCNN根目录，下同。 编译Caffe 和 pycaffe 123456cd $FRCN_ROOT/caffe-fast-rcnn # Now follow the Caffe installation instructions here: # http://caffe.berkeleyvision.org/installation.html # If you&apos;re experienced with Caffe and have all of the requirements installed # and your Makefile.config in place, then simply do:make -j8 &amp;&amp; make pycaffe -j8是指8核编译，更快一些。 下载预计算的R-CNN检测器12cd $FRCN_ROOT./data/scripts/fetch_faster_rcnn_models.sh 这个模型解压出来750M，下载的话大概695M，而且很慢。。为了方便大家，我把模型上传到了百度云，faster_rcnn_models， 密码：gbpo。 运行这一步就很简单了，12cd $FRCN_ROOT./tools/demo.py 当然权限不足直接运行py也可以。这个运行是需要在图像界面下进行的，否则会报错。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>fast-rcnn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（11）python的数据可视化]]></title>
    <url>%2F2016%2F11%2F09%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%8811%EF%BC%89python%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[caffe本身没有可视化的工具，一般需要配合python或matlab实现数据的可视化，在实践本文之前要先把caffe python编译好。另外有的服务器只有shell，没有可视化的界面，只好先把每一层的数据先保存成图片格式，再进行显示。 Caffe学习系列(14)：初识数据可视化Python and/or MATLAB Caffe (optional) 载入数据12345678910import numpy as npimport caffefrom skimage import iocaffe_root='/home/XXX/caffe/'import os,sysos.chdir(caffe_root)sys.path.insert(0,caffe_root+'python')im = caffe.io.load_image('examples/images/cat.jpg')io.imsave('cat.jpg',im)print im.shape 这里用到了skimage 这个库，caffe用于读取图像的函数caffe.io.load_image也是用的这个，具体可以在python/caffe/io.py中查看。之后我们也用这个库进行图像的保存。以上的程序较为简单，读取了示例图片，为了验证是否正确又另存为了副本。最后输出的shape为：（360,480,3）。 载入卷积模型1net = caffe.Net('examples/net_surgery/conv.prototxt', caffe.TEST) 123456789101112131415161718192021222324252627# Simple single-layer network to showcase editing model parameters.name: "convolution"layer &#123; name: "data" type: "Input" top: "data" input_param &#123; shape: &#123; dim: 1 dim: 3 dim: 100 dim: 100 &#125; &#125;&#125;layer &#123; name: "conv" type: "Convolution" bottom: "data" top: "conv" convolution_param &#123; num_output: 16 kernel_size: 5 stride: 1 weight_filler &#123; type: "gaussian" std: 0.01 &#125; bias_filler &#123; type: "constant" value: 0 &#125; &#125;&#125; 载入的是示例中简单的卷积模型，但在shape上有所修改：第二个dim由1改为3，代表三通道输入；同时num_output改为了16，增加了滤波器的个数。 数据格式处理12345im_input=im[np.newaxis,:,:,:].transpose(0,3,1,2)print "data-blobs:",im_input.shape#print "datashape:",net.blobs['data'].data.shapenet.blobs['data'].reshape(*im_input.shape)net.blobs['data'].data[...] = im_input 图片的输入规格和caffe的blob规格并不相同。图片的维度为（360,480,3），而blob的4维数组要求通道数在前，因此需要改变顺序，并且由于仅有一张图片，需要增加一维代表图片序号，该维值为0即可。因此im_input=im[np.newaxis,:,:,:].transpose(0,3,1,2)先增加了一个维度，后改变了维的顺序，使其与输入要求相同。之后改变blobs数据层的维度，使之与图像大小相同（这一步感觉会让网络配置文件中input_param：shape的维度改变，可能是为了方便程序的扩展，没有直接改配置文件）。最后把图像数据输入到blob。 保存图像为了方便调用，可以写一个保存图片的函数：1234567891011def save_data(data,name,padsize=1, padval=0): data -= data.min() data /= data.max() # force the number of filters to be square n = int(np.ceil(np.sqrt(data.shape[0]))) padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3) data = np.pad(data, padding, mode='constant', constant_values=(padval, padval)) # tile the filters into an image data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1))) data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:]) io.imsave(name,data) 保存图片首先进行了归一化操作，之后为了美观生成一个方形模板，再把图片依次放上去。测试这一段可以看一下原图多通道的每一通道分量：1save_data(net.blobs['data'].data[0],'origin images.jpg') 卷积层输出123456net.forward()print "data-blobs:",net.blobs['data'].data.shapeprint "conv-blobs:",net.blobs['conv'].data.shapeprint "weight-blobs:",net.params['conv'][0].data.shapesave_data(net.params['conv'][0].data[:,0],'conv weights(filter).jpg')save_data(net.blobs['conv'].data[0],'post-conv images.jpg') 经过一次向前计算，得到了卷积后的结果和初始的卷积核值，打印他们的大小分别为： data-blobs: (1, 3, 360, 480) conv-blobs: (1, 16, 356, 476) weight-blobs: (16, 3, 5, 5) 最后保存成了两个图片文件：]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（10）数据转换img2db]]></title>
    <url>%2F2016%2F11%2F08%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%8810%EF%BC%89%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2img2db%2F</url>
    <content type="text"><![CDATA[在处理图像时，我们已拥有的图像往往是常用的jpg、png格式，但在caffe中，输入的数据类型常是lmdb或leveldb，因此我们需要对原始数据进行转换。 Caffe学习系列(11)：图像数据转换成db（leveldb/lmdb)文件 convert_imageset在caffe中，提供了一个用于格式转换的文件：convert_imageset.cpp，存放在根目录下的tools文件夹下。编译之后，生成对应的可执行文件放在 build/tools/ 下面，这个文件的作用就是用于将图片文件转换成caffe框架中能直接使用的db文件。而windows平台下，如果用vs编译会在build/x64/debug中生成convert_imageset.exe。 使用方法该文件的使用格式： convert_imageset [FLAGS] ROOTFOLDER/ LISTFILE DB_NAME 而数据集推荐的是imagenet。对于具体的参数包含有： FLAGS：图片转换参数。 ROOTFOLDER/：图片的绝对路径，从系统根目录开始。 LISTFILE：图片文件清单文件，为txt格式，一行有一张图片。 DB_NAME：生成db文件的存放目录。 其中图片文件清单比较麻烦，因此可以使用脚本文件读取图片并保存为txt。 图片文件清单的生成本文以caffe程序中自带的图片为例，进行讲解，图片目录是 example/images/, 两张图片，一张为cat.jpg, 另一张为fish_bike.jpg，表示两个类别。我们创建一个sh脚本文件，调用linux命令来生成图片清单： # sudo vi examples/images/create_filelist.sh 编辑这个文件,输入下面的代码并保存：123456789# /usr/bin/env shDATA=examples/imagesecho "Create train.txt..."rm -rf $DATA/train.txtfind $DATA -name *cat.jpg | cut -d '/' -f3 | sed "s/$/ 1/"&gt;&gt;$DATA/train.txtfind $DATA -name *bike.jpg | cut -d '/' -f3 | sed "s/$/ 2/"&gt;&gt;$DATA/tmp.txtcat $DATA/tmp.txt&gt;&gt;$DATA/train.txtrm -rf $DATA/tmp.txtecho "Done.." 这个脚本文件中，用到了rm,find, cut, sed,cat等linux命令。 rm: 删除文件 find: 寻找文件 cut: 截取路径 sed: 在每行的最后面加上标注。本例中将找到的cat.jpg文件加入标注为1，找到的bike.jpg文件加入标注为2 cat: 将两个类别合并在一个文件里。 最终生成如下的一个train.txt文件： cat.jpg 1 fish-bike.jpg 2 当然，图片很少的时候，手动编写这个列表清单文件就行了。但图片很多的情况，就需要用脚本文件来自动生成了。在以后的实际应用中，还需要生成相应的val.txt和test.txt文件，方法是一样的。生成的这个train.txt文件，就可以作为第三个参数，直接使用了。 FLAGS参数设置接下来，我们来了解一下FLAGS这个参数组，有些什么内容： gray: 是否以灰度图的方式打开图片。程序调用opencv库中的imread()函数来打开图片，默认为false shuffle: 是否随机打乱图片顺序。默认为false backend:需要转换成的db文件格式，可选为leveldb或lmdb,默认为lmdb resize_width/resize_height: 改变图片的大小。在运行中，要求所有图片的尺寸一致，因此需要改变图片大小。 程序调用opencv库的resize（）函数来对图片放大缩小，默认为0，不改变 check_size: 检查所有的数据是否有相同的尺寸。默认为false,不检查 encoded: 是否将原图片编码放入最终的数据中，默认为false encode_type: 与前一个参数对应，将图片编码为哪一个格式：‘png’,’jpg’…… 好了，知道这些参数后，我们就可以调用命令来生成最终的lmdb格式数据了。 转换脚本的编写由于参数比较多，因此我们可以编写一个sh脚本来执行命令。首先，创建sh脚本文件： # sudo vi examples/images/create_lmdb.sh 编辑，输入下面的代码并保存：123456#!/usr/bin/en shDATA=examples/imagesrm -rf $DATA/img_train_lmdbbuild/tools/convert_imageset --shuffle \--resize_height=256 --resize_width=256 \/home/xxx/caffe/examples/images/ $DATA/train.txt $DATA/img_train_lmdb 设置参数-shuffle,打乱图片顺序。设置参数-resize_height和-resize_width将所有图片尺寸都变为256*256。/home/xxx/caffe/examples/images/ 为图片保存的绝对路径。最后，运行这个脚本文件： # sudo sh examples/images/create_lmdb.sh 就会在examples/images/ 目录下生成一个名为 img_train_lmdb的文件夹，里面的文件就是我们需要的db文件了。 针对于windows环境，可以使用bat快速运行： D:/caffe-master/Build/x64/Release/convert_imageset --shuffle --resize_height=256 --resize_width=256 D:/caffe-master/examples/images/ D:/caffe-master/examples/images/train.txt D:/caffe-master/examples/images/img_train_lmdb]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（9）LeNet在Caffe上的使用]]></title>
    <url>%2F2016%2F11%2F08%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%889%EF%BC%89LeNet%E5%9C%A8Caffe%E4%B8%8A%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[使用官网例程训练LeNet。 Training LeNet on MNIST with Caffe 准备数据Caffe程序的运行要注意需命令行要在Caffe的根目录下。 cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh 依次运行，会在caffe\examples\mnist下得到两个目录mnist_train_lmdb, 和 mnist_test_lmdb，作为训练和测试集。 定义MNIST网络Caffe上的LeNet并不是传统的LeNet-5，在参数上还是有所不同的。以\caffe\examples\mnist\lenet_train_test.prototxt 为例（本地文件与官网上的教程也有所区别），介绍一下如何定义网络。首先是定义网络名称：1name: "LeNet" 数据层利用我们已经生成的MNIST数据，把数据输入到网络：12345678910111213141516171819202122232425262728293031323334layer &#123; name: "mnist" type: "Data" top: "data" top: "label" include &#123; phase: TRAIN &#125; transform_param &#123; scale: 0.00390625 &#125; data_param &#123; source: "examples/mnist/mnist_train_lmdb" batch_size: 64 backend: LMDB &#125;&#125;layer &#123; name: "mnist" type: "Data" top: "data" top: "label" include &#123; phase: TEST &#125; transform_param &#123; scale: 0.00390625 &#125; data_param &#123; source: "examples/mnist/mnist_test_lmdb" batch_size: 100 backend: LMDB &#125;&#125; 具体来说，本层名为：”mnist”，类型为”data”，输出到两个blob：”data””label”。下面用到了之前所说的include，包含TRAIN与TEST，表示该层是在训练还是测试时调用，其区别在于输入的不同数据集（见data_param）。transform_param用于输入数据的缩放，使之在$[0,1]$内，其中$0.00390625=1/256$。 卷积层本网络中，有两个卷积层，第一层为：1234567891011121314151617181920212223layer &#123; name: "conv1" type: "Convolution" bottom: "data" top: "conv1" param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; convolution_param &#123; num_output: 20 kernel_size: 5 stride: 1 weight_filler &#123; type: "xavier" &#125; bias_filler &#123; type: "constant" &#125; &#125;&#125; 这一层使用数据层的数据作为输入，生成”conv1”层，具体产生20个通道的输出，卷积核大小为5，卷积步长为1。先后两个lr_mults是对本层可学习参数的速率调整，权重学习率与solver中的学习率相同，而偏置学习率为其两倍，这往往导致更好的手链率。权重初始化使用”xavier”方式，偏置初始化为0。第二个卷积层在池化层1后，输出到池化层2，参数除了输出个数（num_output）改为50，其余的相同。 池化层1234567891011layer &#123; name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125; 第一个池化层表示采用最大池化的方法，进行大小为2，步长为2的非重叠池化。第二个池化层与第一个完全相同，其输入为卷积层2，输出到全连接层1. 全连接层123456789101112131415161718192021layer &#123; name: "ip1" type: "InnerProduct" bottom: "pool2" top: "ip1" param &#123; lr_mult: 1 &#125; param &#123; lr_mult: 2 &#125; inner_product_param &#123; num_output: 500 weight_filler &#123; type: "xavier" &#125; bias_filler &#123; type: "constant" &#125; &#125;&#125; 全连接层与卷积层的写法非常相似，ip1层产生500个输出。在激活层后，还有一个全连接层，用于最后的输出分类，因此有10个输出。 激活层123456layer &#123; name: "relu1" type: "ReLU" bottom: "ip1" top: "ip1"&#125; ReLU是一个元素操作，因此可以使用原地操作（in-place operations）用于节省空间。其实就是top与bottom的名字相同。当然其他的层不能使用重复的blob名称。 损失层最后是损失层：1234567layer &#123; name: "loss" type: "SoftmaxWithLoss" bottom: "ip2" bottom: "label" top: "loss"&#125; softmax_loss层同时实现softmax和多项对数损失（这可以节省时间并提高数值稳定性）。输入为预测的输出和label，并且没有输出（向后的输出）。它计算损失函数，并且反向传播相对于ip2的梯度。 Accuracy准确率层这一层是用于在测试中返回准确率使用的：12345678910layer &#123; name: "accuracy" type: "Accuracy" bottom: "ip2" bottom: "label" top: "accuracy" include &#123; phase: TEST &#125;&#125; 与loss相似，但要注明phase: TEST。 定义MNIST 求解器求解器文件路径为： $CAFFE_ROOT/examples/mnist/lenet_solver.prototxt: 12345678910111213141516171819202122232425# The train/test net protocol buffer definitionnet: "examples/mnist/lenet_train_test.prototxt"# test_iter specifies how many forward passes the test should carry out.# In the case of MNIST, we have test batch size 100 and 100 test iterations,# covering the full 10,000 testing images.test_iter: 100# Carry out testing every 500 training iterations.test_interval: 500# The base learning rate, momentum and the weight decay of the network.base_lr: 0.01momentum: 0.9weight_decay: 0.0005# The learning rate policylr_policy: "inv"gamma: 0.0001power: 0.75# Display every 100 iterationsdisplay: 100# The maximum number of iterationsmax_iter: 10000# snapshot intermediate resultssnapshot: 5000snapshot_prefix: "examples/mnist/lenet"# solver mode: CPU or GPUsolver_mode: GPU 这些参数见上一篇：caffe学习（8）Solver 配置详解。 训练并测试模型简单的话可以直接运行： cd $CAFFE_ROOT ./examples/mnist/train_lenet.sh 即运行： ./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt 也就是我们上面的求解器配置文件。首先出现的是我们打开的solver文件，之后打开网络模型：lenet_train_test.prototxt，初始化网络参数。 I1108 16:08:29.103813 46285 layer_factory.hpp:77] Creating layer mnist I1108 16:08:29.104310 46285 net.cpp:100] Creating Layer mnist I1108 16:08:29.104336 46285 net.cpp:408] mnist -&gt; data I1108 16:08:29.104374 46285 net.cpp:408] mnist -&gt; label I1108 16:08:29.107558 46328 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb 不过仔细看的话会发现初始化了两遍网络，其实是因为我们同时在训练和测试，这两个网络的区别就是测试有”accuracy”层，训练没有。这些信息告诉了层之间的连接、输入、输出关系。结束后正式开始训练： I1108 16:08:29.156116 46285 net.cpp:283] Network initialization done. I1108 16:08:29.156206 46285 solver.cpp:60] Solver scaffolding done. I1108 16:08:29.156466 46285 caffe.cpp:251] Starting Optimization I1108 16:08:29.156500 46285 solver.cpp:279] Solving LeNet I1108 16:08:29.156512 46285 solver.cpp:280] Learning Rate Policy: inv I1108 16:08:29.158172 46285 solver.cpp:337] Iteration 0, Testing net (#0) I1108 16:08:31.021287 46285 solver.cpp:404] Test net output #0: accuracy = 0.0933 I1108 16:08:31.021385 46285 solver.cpp:404] Test net output #1: loss = 2.36349 (* 1 = 2.36349 loss) 可以看到，初始化参数后测试模型，准确率有9.33%，比10%还低一些。基于参数设置，我们每迭代100次输出loss 信息，每迭代500次测试模型，输出accuracy 信息： I1108 16:08:46.974346 46285 solver.cpp:337] Iteration 500, Testing net (#0) I1108 16:08:48.808943 46285 solver.cpp:404] Test net output #0: accuracy = 0.9767 I1108 16:08:48.809048 46285 solver.cpp:404] Test net output #1: loss = 0.068445 (* 1 = 0.068445 loss) I1108 16:08:48.823623 46285 solver.cpp:228] Iteration 500, loss = 0.0609579 I1108 16:08:48.823714 46285 solver.cpp:244] Train net output #0: loss = 0.0609579 (* 1 = 0.0609579 loss) I1108 16:08:48.823740 46285 sgd_solver.cpp:106] Iteration 500, lr = 0.0192814 可以发现输出500次后准确率已经达到了97.67%。达到训练次数后（这里减少了训练次数），得到最终结果： I1108 16:09:04.727638 46285 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel I1108 16:09:04.754024 46285 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate I1108 16:09:04.770093 46285 solver.cpp:317] Iteration 1000, loss = 0.0819535 I1108 16:09:04.770177 46285 solver.cpp:337] Iteration 1000, Testing net (#0) I1108 16:09:06.607952 46285 solver.cpp:404] Test net output #0: accuracy = 0.9844 I1108 16:09:06.608042 46285 solver.cpp:404] Test net output #1: loss = 0.0491373 (* 1 = 0.0491373 loss) I1108 16:09:06.608055 46285 solver.cpp:322] Optimization Done. I1108 16:09:06.608064 46285 caffe.cpp:254] Optimization Done. 得到了两个文件：lenet_iter_1000.caffemodel和lenet_iter_1000.solverstate。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>LeNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（8）Solver 配置详解]]></title>
    <url>%2F2016%2F11%2F08%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%888%EF%BC%89Solver%20%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Solver是求解学习模型的核心配置文件，网络确定后，solver就决定了学习的效果。本文结合caffe.proto和网上资料，对solver配置进行学习。 SolverCaffe学习系列(7)：solver及其配置，denny402 Solver在caffe中的定义通常的solver文件与net文件相互关联，同样的net我们往往使用不同的solver尝试得到最好的效果，其运行代码为： caffe train --solver=*_slover.prototxt 关于solver的一切，都在caffe.proto文件中message SolverParameter 这一部分。 网络文件源12345678910// Proto filename for the train net, possibly combined with one or more// test nets.optional string net = 24;// Inline train net param, possibly combined with one or more test nets.optional NetParameter net_param = 25;optional string train_net = 1; // Proto filename for the train net.repeated string test_net = 2; // Proto filenames for the test nets.optional NetParameter train_net_param = 21; // Inline train net params.repeated NetParameter test_net_param = 22; // Inline test net params. 这是最开始的部分，需要说明net文件的位置。在这四个train_net_param, train_net, net_param, net字段中至少需要出现一个，当出现多个时，就会按着(1) test_net_param, (2) test_net, (3) net_param/net 的顺序依次求解。必须为每个test_net指定一个test_iter。还可以为每个test_net指定test_level和/或test_stage。注意的是：文件的路径要从caffe的根目录开始，其它的所有配置都是这样。可以看到这几行的标签序号并不是顺序的，也说明caffe在不断地修改，下一个可用的序号是41。 网络状态123456789// The states for the train/test nets. Must be unspecified or// specified once per net.//// By default, all states will have solver = true;// train_state will have phase = TRAIN,// and all test_state's will have phase = TEST.// Other defaults are set according to the NetState defaults.optional NetState train_state = 26;repeated NetState test_state = 27; 网络状态必须是未指定的或者只能在一个网络中指定一次。关于NetState，其定义为：123456789message NetState &#123; optional Phase phase = 1 [default = TEST]; optional int32 level = 2 [default = 0]; repeated string stage = 3;&#125;enum Phase &#123; TRAIN = 0; TEST = 1;&#125; 迭代器12// The number of iterations for each test net.repeated int32 test_iter = 3; 首先是test_iter，这需要与test layer中的batch_size结合起来理解。mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。因此test_iter设置为100。执行完一次全部数据，称之为一个epoch。123456// The number of iterations between two testing phases.optional int32 test_interval = 4 [default = 0];optional bool test_compute_loss = 19 [default = false];// If true, run an initial test pass before the first iteration,// ensuring memory availability and printing the starting value of the loss.optional bool test_initialization = 32 [default = true]; test_interval是指测试间隔，每训练test_interval次，进行一次测试。同时test_compute_loss可以选择是否计算loss。test_initialization是指在第一次迭代前，计算初始的loss以确保内存可用。 123456789optional float base_lr = 5; // The base learning rate// the number of iterations between displaying info. If display = 0, no info// will be displayed.optional int32 display = 6;// Display the loss averaged over the last average_loss iterationsoptional int32 average_loss = 33 [default = 1];optional int32 max_iter = 7; // the maximum number of iterations// accumulate gradients over `iter_size` x `batch_size` instancesoptional int32 iter_size = 36 [default = 1]; base_lr指基础的学习率；display是信息显示间隔，迭代一定次数显示一次信息。average_loss用于显示在上次average_loss迭代中的平均损失。max_iter是最大迭代次数，需要合适设置达到精度、震荡的平衡。iter_size是迭代器大小，梯度的计算是通过iter_size x batch_size决定的。 学习策略 123456789101112optional string lr_policy = 8;optional float gamma = 9; // The parameter to compute the learning rate.optional float power = 10; // The parameter to compute the learning rate.optional float momentum = 11; // The momentum value.optional float weight_decay = 12; // The weight decay.// regularization types supported: L1 and L2// controlled by weight_decayoptional string regularization_type = 29 [default = "L2"];// the stepsize for learning rate policy "step"optional int32 stepsize = 13;// the stepsize for learning rate policy "multistep"repeated int32 stepvalue = 34; 只要是梯度下降法来求解优化，都会有一个学习率，也叫步长。base_lr用于设置基础学习率，在迭代的过程中，可以对基础学习率进行调整。怎么样进行调整，就是调整的策略，由lr_policy来设置。caffe提供了多种policy： fixed: 总是返回base_lr（学习率不变） step: 返回 base_lr * gamma ^ (floor(iter / step))还需要设置stepsize参数以确定step，iter表示当前迭代次数。 exp: 返回base_lr * gamma ^ iter， iter为当前迭代次数 inv: 如果设置为inv,还需要设置一个power, 返回base_lr (1 + gamma iter) ^ (- power) multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据stepvalue值变化。 poly: 学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power) sigmoid: 学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))。 multistep示例：1234567891011base_lr: 0.01momentum: 0.9weight_decay: 0.0005# The learning rate policylr_policy: "multistep"gamma: 0.9stepvalue: 5000stepvalue: 7000stepvalue: 8000stepvalue: 9000stepvalue: 9500 之后有momentum，上次梯度更新的权重；weight_decay权重衰减，防止过拟合；regularization_type正则化方式。 clip_gradients1optional float clip_gradients = 35 [default = -1]; 参数梯度的实际L2范数较大时，将clip_gradients设置为&gt; = 0，以将参数梯度剪切到该L2范数。具体作用还不是很理解。 snapshot快照12345678910optional int32 snapshot = 14 [default = 0]; // The snapshot intervaloptional string snapshot_prefix = 15; // The prefix for the snapshot.// whether to snapshot diff in the results or not. Snapshotting diff will help// debugging but the final protocol buffer size will be much larger.optional bool snapshot_diff = 16 [default = false];enum SnapshotFormat &#123; HDF5 = 0; BINARYPROTO = 1;&#125;optional SnapshotFormat snapshot_format = 37 [default = BINARYPROTO]; 快照可以将训练出来的model和solver状态进行保存，snapshot用于设置训练多少次后进行保存，默认为0，不保存。snapshot_prefix设置保存路径。还可以设置snapshot_diff，是否保存梯度值，保存有利于调试，但需要较大空间存储，默认为false，不保存。也可以设置snapshot_format，保存的类型。有两种选择：HDF5 和BINARYPROTO ，默认为BINARYPROTO。 运行模式 1234567891011enum SolverMode &#123; CPU = 0; GPU = 1;&#125;optional SolverMode solver_mode = 17 [default = GPU];// the device_id will that be used in GPU mode. Use device_id = 0 in default.optional int32 device_id = 18 [default = 0];// If non-negative, the seed with which the Solver will initialize the Caffe// random number generator -- useful for reproducible results. Otherwise,// (and by default) initialize using a seed derived from the system clock.optional int64 random_seed = 20 [default = -1]; 设置CPU或GPU模式，在GPU下还可以指定使用哪一块GPU运行。random_seed用于初始生成随机数种子。 Solver类型 1234567891011// type of the solveroptional string type = 40 [default = "SGD"];// numerical stability for RMSProp, AdaGrad and AdaDelta and Adamoptional float delta = 31 [default = 1e-8];// parameters for the Adam solveroptional float momentum2 = 39 [default = 0.999];// RMSProp decay value// MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)optional float rms_decay = 38; type是solver的类型，目前有SGD、NESTEROV、ADAGRAD、RMSPROP、ADADELTA、ADAM = 5这六类。之后的一些是这些类型的特有参数，根据需要设置。 杂项 123456// If true, print information about the state of the net that may help with// debugging learning problems.optional bool debug_info = 23 [default = false];// If false, don't save a snapshot after training finishes.optional bool snapshot_after_train = 28 [default = true]; debug_info用于输出调试信息。snapshot_after_train用于训练后是否输出快照。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Google Protocol Buffer 学习]]></title>
    <url>%2F2016%2F11%2F07%2FGoogle%20Protocol%20Buffer%20%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[Caffe上有很多使用了Google Protocol Buffer的东西，从网上来看，这“是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，很适合做数据存储或 RPC 数据交换格式。它可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式”。作为caffe模型定义的数据格式，看懂caffe.proto对caffe的理解会有很大帮助。 Google ProtobufGoogle Protocol Buffer 的使用和原理，刘 明 小例子我们首先要在.proto文件中定义协议缓冲区消息类型（protocol buffer message types），来指定要序列化的信息的结构。下面是官网的一个小例子，定义了一个人的信息： message Person { required string name = 1; required int32 id = 2; optional string email = 3; enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } message PhoneNumber { required string number = 1; optional PhoneType type = 2 [default = HOME]; } repeated PhoneNumber phone = 4; } 在 protobuf 的术语中，结构化数据被称为 Message，message中有不同成员。proto 文件非常类似 java 或者 C 语言的数据定义，string、int32这种类型我们已经见得多了。支持类型包括数字（整数或浮点）, 布尔值,，字符串，原始字节（raw bytes），或者是其他的message类型 (如上例) 。除了这些类型，其前后多了一些“修饰”。类型前是field rules，有可选（optional）、必填（required）、重复（repeated）三种。后面是message中的ID，同一message下ID随成员递增。 定义一个消息类型上面的例子其实已经定义了一个较为复杂的message。而对于一个完整的proto文件，在文件前还应该加上 syntax = &quot;proto2&quot;;//说明语法是2还是3 package caffe; //包名，通常与文件名相同 首先是field rules， Specifying Field Rules required：本字段一个massage必须只有一个。 optional：本字段可以有0个或1个。 repeated：本字段可以重复任何次，并保留顺序。 其中必填字段在使用中需要小心，特别是从必填改到可选时，读取时可能认为这个字段是不完整的。 Assigning Tags 定义消息中的每一个字段都有唯一的编号标签ID，用于消息二进制标识，并且在使用后不应该变。由于编码的原因，值在1到15范围的编号需要一个字节编码，包括标识号与字段类型。在16到2047范围的标签用两个字节。这对于数据储存大小有很大的联系，因此频繁出现的标签成员应该使编号尽可能小。其原因在于一种Varint的编码。其标签范围是1到$2^{29}-1$（536,870,911），同时除了中间的19000到19999，这是为协议缓冲区（Protocol Buffers）实现保留的。 Reserved Fields 保留字段，是对于被删除或者注释的字段进行保留， 如果以后加载相同.proto的旧版本，这可能会导致严重的问题。确保不会发生这种情况的一种方法是指定保留已删除字段的字段标签。协议缓冲区编译器将报告任何未来的用户是否尝试使用这些字段标识符。 message Foo { reserved 2, 15, 9 to 11; reserved &quot;foo&quot;, &quot;bar&quot;; } 不能在同一保留语句中混合字段名和标识号。 编译.proto文件使用写好的proto就可以用编译器将文件编译为目标语言了。在protobuf V3.0网站上可以下载 Protobuf V3.0的源代码，V2.6版本在网页上比较靠后，更新到2.6.1。然后解压编译安装便可以使用它了。从caffe文件上看用的还是2的语法。2和3的区别可以从网上搜到，如Google Protobuf 3版本介绍。其实变化也是不少，比如只保留repeated标记数组类型，optional和required都被去掉了；字段default标记不能使用了。 坑爹的是服务器没有权限装不了，于是只好在自家电脑上用windows版的。主要是用于验证， package lm; message helloworld { required int32 id = 1; // ID required string str = 2; // str optional int32 opt = 3; //optional field } 用命令行执行 protoc -I=. --cpp_out=. lm.helloworld.proto 得到了两个文件：lm.helloworld.pb.h ， 定义了 C++ 类的头文件lm.helloworld.pb.cc ， C++ 类的实现文件。有了这两个文件，之后我们想读写都可以用类操作实现了。 读写数据数据写到磁盘代码12345678910111213141516171819 #include "lm.helloworld.pb.h"… int main(void) &#123; lm::helloworld msg1; msg1.set_id(101); msg1.set_str(“hello”); // Write the new address book back to disk. fstream output("./log", ios::out | ios::trunc | ios::binary); if (!msg1.SerializeToOstream(&amp;output)) &#123; cerr &lt;&lt; "Failed to write msg." &lt;&lt; endl; return -1; &#125; return 0; &#125; 在代码中，其实重要的只是前三行，定义了helloworld类的对象，设置id的值，设置str的值。最后用SerializeToOstream输出到文件流。 读取数据代码 12345678910111213141516171819202122 #include "lm.helloworld.pb.h" … void ListMsg(const lm::helloworld &amp; msg) &#123; cout &lt;&lt; msg.id() &lt;&lt; endl; cout &lt;&lt; msg.str() &lt;&lt; endl; &#125; int main(int argc, char* argv[]) &#123; lm::helloworld msg1; &#123; fstream input("./log", ios::in | ios::binary); if (!msg1.ParseFromIstream(&amp;input)) &#123; cerr &lt;&lt; "Failed to parse address book." &lt;&lt; endl; return -1; &#125; &#125; ListMsg(msg1); … &#125; 在读取代码中，声明类 helloworld 的对象 msg1，然后利用 ParseFromIstream 从一个 fstream 流中读取信息并反序列化。此后，ListMsg 中采用 get 方法读取消息的内部信息，并进行打印输出操作。 分别运行后得到如下结果： &gt;writer &gt;reader 101 Hello 验证了程序。 Peotocol Buffer 编码在标签中说到了Varint，现在再结合编码讲一下，主要是参考了引用2的网页。Varint 中的每个 byte 的最高位 bit 有特殊的含义，如果该位为 1，表示后续的 byte 也是该数字的一部分，如果该位为 0，则结束。其他的 7 个 bit 都用来表示数字。因此小于 128 的数字都可以用一个 byte 表示。大于 128 的数字，比如 300，会用两个字节来表示：1010 1100 0000 0010。下图演示了 Google Protocol Buffer 如何解析两个 bytes。注意到最终计算前将两个 byte 的位置相互交换过一次，这是因为 Google Protocol Buffer 字节序采用 little-endian(小端在前) 的方式。消息经过序列化后会成为一个二进制数据流，该流中的数据为一系列的 Key-Value 对。如下图所示：采用这种 Key-Pair 结构无需使用分隔符来分割不同的 Field。对于可选的 Field，如果消息中不存在该 field，那么在最终的 Message Buffer 中就没有该 field，这些特性都有助于节约消息本身的大小。假如生成如下的消息： Test1.id = 10; Test1.str = “hello”； 则最终的 Message Buffer 中有两个 Key-Value 对，一个对应消息中的 id；另一个对应 str。Key 用来标识具体的 field，在解包的时候，Protocol Buffer 根据 Key 就可以知道相应的 Value 应该对应于消息中的哪一个 field。Key 的定义如下： (field_number &lt;&lt; 3) | wire_type //&lt;&lt;是左移运算 可以看到 Key 由两部分组成。第一部分是 field_number，比如消息 lm.helloworld 中 field id 的 field_number 为 1。第二部分为 wire_type。表示 Value 的传输类型。其中wire_type有如下几种：在我们的例子当中，field id 所采用的数据类型为 int32，因此对应的 wire type 为 0。细心的读者或许会看到在 Type 0 所能表示的数据类型中有 int32 和 sint32 这两个非常类似的数据类型。Google Protocol Buffer 区别它们的主要意图也是为了减少 encoding 后的字节数。在计算机内，一个负数一般会被表示为一个很大的整数，因为计算机定义负数的符号位为数字的最高位。如果采用 Varint 表示一个负数，那么一定需要 5 个 byte。为此 Google Protocol Buffer 定义了 sint32 这种类型，采用 zigzag 编码。Zigzag 编码用无符号数来表示有符号数字，正数和负数交错，这就是 zigzag 这个词的含义了。具体编码如图所示：使用 zigzag 编码，绝对值小的数字，无论正负都可以采用较少的 byte 来表示，充分利用了 Varint 这种技术。其他的数据类型，比如字符串等则采用类似数据库中的 varchar 的表示方法，即用一个 varint 表示长度，然后将其余部分紧跟在这个长度部分之后即可。总之，Protocol Buffer的编码确费尽心机，效果当然也不错，特别是与常用的XML相比，包括解包的速度。 了解了一下Google Protocol Buffer，算是一些课外知识了。]]></content>
      <categories>
        <category>学习</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>proto</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（7）损失层、通用层]]></title>
    <url>%2F2016%2F11%2F07%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%887%EF%BC%89%E6%8D%9F%E5%A4%B1%E5%B1%82%E3%80%81%E9%80%9A%E7%94%A8%E5%B1%82%2F</url>
    <content type="text"><![CDATA[Caffe LayersCaffe学习系列(5)：其它常用层及参数，denny402 损失层Loss Layers 损失通过将输出与目标进行比较，并不断优化减小loss。 Softmax（with loss） 层类型：SoftmaxWithLoss 示例： 1234567 layer &#123; name: "loss" type: "SoftmaxWithLoss" bottom: "ip1" bottom: "label" top: "loss"&#125; 在概念上等同于softmax layer+多项对数损失层（multinomial logistic loss layer），但提供了更稳定的梯度。softmax只是输出每一类的概率，并没有与label做比较。 Sum-of-Squares / Euclidean 层类型：EuclideanLoss这是比较传统的求偏差的方法，$\frac 1 {2N} \sum_{i=1}^N | x^1_i - x^2_i |_2^2$，直接计算欧氏距离。 Hinge / Margin 层类型：HingeLoss 参数(HingeLossParameter hinge_loss_param)： 可选 norm [default L1]:应该是正则化方法，目前只有L1、L2。 输入： n c h * w Predictions预测值 n 1 1 * 1 Labels标签 输出：1 1 1 * 1 Computed Loss 示例 12345678910111213141516171819# L1 Norm L1正则layer &#123; name: "loss" type: "HingeLoss" bottom: "pred" bottom: "label"&#125;# L2 Norm L2正则layer &#123; name: "loss" type: "HingeLoss" bottom: "pred" bottom: "label" top: "loss" hinge_loss_param &#123; norm: L2 &#125;&#125; Hinge loss主要用于SVM。 Accuracy 层类型：Accuracy 示例 12345678910layer &#123; name: "accuracy" type: "Accuracy" bottom: "ip2" bottom: "label" top: "accuracy" include &#123; phase: TEST &#125;&#125; 只有test阶段才有，因此需要加入include参数。它实际上不是损失并且没有后退步骤。 通用层Common Layers Inner Product 层类型：InnerProduct 参数 (InnerProductParameter inner_product_param)： 必须参数 num_output (c_o):滤波器数量。 推荐参数 weight_filler [default type: ‘constant’ value: 0]：全职初始化方式、值。还可以选择”xavier”算法来进行初始化，也可以设置为”gaussian”。 可选参数 bias_filler [default type: ‘constant’ value: 0]：偏置初始化。 bias_term [default true]: 是否启用偏置项。 输入：n c_i h_i * w_i 输出：n c_o 1 * 1 示例： 123456789101112131415161718192021layer &#123; name: "fc8" type: "InnerProduct" # learning rate and decay multipliers for the weights param &#123; lr_mult: 1 decay_mult: 1 &#125; # learning rate and decay multipliers for the biases param &#123; lr_mult: 2 decay_mult: 0 &#125; inner_product_param &#123; num_output: 1000 weight_filler &#123; type: "gaussian" std: 0.01 &#125; bias_filler &#123; type: "constant" value: 0 &#125; &#125; bottom: "fc7" top: "fc8"&#125; Reshape 层类型：Reshape 参数 (ReshapeParameter reshape_param)： 可选参数： shape 输入：单独的blob 输出：变形后的blob 示例： 1234567891011121314layer &#123; name: "reshape" type: "Reshape" bottom: "input" top: "output" reshape_param &#123; shape &#123; dim: 0 # copy the dimension from below dim: 2 dim: 3 dim: -1 # infer it from the other dimensions &#125; &#125; &#125; 这一操作不改变数据，只改变维度，也没有在过程中拷贝数据。输出的尺寸有shape参数的值规定，正数是对应的维度，除此外还有两个特殊值： 0表示复制底层对应的维度。bottom第一维度值为2，top第一维度也是2。 -1表示从其他维度推断。为了保证数据总数不变，可以根据其他维数值计算。 特别的，当时用参数：reshape_param { shape { dim: 0 dim: -1 } }时，reshape层相当于flatten层，将n c h w的数据变为n (chw)。 Concatenation 层类型： Concat 参数 (ConcatParameter concat_param)： 可选参数 axis [default 1]: 0表示沿着数量（n），1表示沿着通道（C）。 输入：n_i c_i h * w 对于每个blob输入，i= 1 到 K。 输出： 当 axis = 0: (n_1 + n_2 + … + n_K) c_1 h * w, 所有的c_i应该相同。 当 axis = 1: n_1 (c_1 + c_2 + … + c_K) h * w, 所有的n_i 应该相同。 这个层把多个blob连接为一个blob。 层的学习暂时到这里。。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（6）激活层]]></title>
    <url>%2F2016%2F11%2F07%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%886%EF%BC%89%E6%BF%80%E6%B4%BB%E5%B1%82%2F</url>
    <content type="text"><![CDATA[激活（Activation）层又叫神经元（Neuron）层，最主要的是激活函数的设置。 Activation / Neuron LayersCaffe源码解析6：Neuron_Layer，楼燚航的blog 一般来说，这一层是元素级的运算符，从底部blob作为输入并产生一个相同大小的顶部blob： 输入：n c h * w 输出：n c h * w ReLU / Rectified-Linear and Leaky-ReLU 层类型：ReLU 参数(ReLUParameter relu_param)： 可选参数 negative_slope [default 0]: 用来指定负斜率部分的因子$\nu$。完整的函数表达式为：$y = \max(0, x) + \nu \min(0, x)$。反向传播的公式为$$\frac{\partial E}{\partial x} = \left{\begin{array}{lr}\nu \frac{\partial E}{\partial y} &amp; \mathrm{if} \; x \le 0 \\frac{\partial E}{\partial y} &amp; \mathrm{if} \; x &gt; 0\end{array} \right.$$ 示例（./models/bvlc_reference_caffenet/train_val.prototxt）： 123456789101112131415161718192021layer &#123; name: "relu1" type: "ReLU" bottom: "conv1" top: "conv1"&#125; ``` 支持in-place计算，bottom输入和top输出可以相同避免内存消耗。 **Sigmoid** - 层类型：Sigmoid - 示例( ./models/bvlc_reference_caffenet/train_val.prototxt)： ```pythonlayer &#123; name: "relu1" type: "ReLU" bottom: "conv1" top: "conv1"&#125; 激活函数表达式为$y = (1 + \exp(-x))^{-1}$，由于收敛速度问题现在用的不多了。 TanH、AbsVal、BNLL 层类型：TanH、AbsVal、BNLL 示例： 123456 layer &#123; name: "layer" bottom: "in" top: "out" type: "TanH"#"AbsVal"、“BNLL”官网上BNLL没有加双引号，应该是有误&#125; 分别是双曲正切函数、绝对值、binomial normal log likelihood（$f(x)=log(1 + e^x)$）的简称。 Power 层类型：Power 参数 (PowerParameter power_param)： 可选 power [default 1] scale [default 1] shift [default 0] 示例： 1234567891011 layer &#123; name: "layer" bottom: "in" top: "out" type: "Power" power_param &#123; power: 2 scale: 1 shift: 0 &#125;&#125; 幂运算函数为$f(x)= (shift + scale * x) ^ p$。 Caffe中的激活层还有很多，也有一些是加速的层。比如DropoutLayer现在是非常常用的一种网络层，只用在训练阶段，一般用在网络的全连接层中，可以减少网络的过拟合问题。具体的使用再具体看./src/caffe/layers/下的文件吧。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（5）视觉层]]></title>
    <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%885%EF%BC%89%E8%A7%86%E8%A7%89%E5%B1%82%2F</url>
    <content type="text"><![CDATA[上一篇是数据层，这一篇是视觉层（Vision Layers）。参考官网和网友博客。 Vision LayersCaffe学习系列(3)：视觉层（Vision Layers)及参数，denny402Caffe源码解析5：Conv_Layer，楼燚航的blog 视觉层通常将图像作为输入，产生其他图像作为输出。图像输入可以是灰度图（通道C=1），RGB图（通道C=3）。同样图像也具有二维的空间结构，其高度$h&gt;1$宽度$w&gt;1$。大多数视觉层通过对输入区域应用特定操作产生输出的相应区域，这里就有点像传统的数字图像处理的工作了。相比之下其他层常常忽略输入的空间结构，视其为具有$chw$维度的大向量。 卷积层Convolution 层类型：Convolution CPU实现：./src/caffe/layers/convolution_layer.cpp CUDA GPU实现： ./src/caffe/layers/convolution_layer.cu 参数 (ConvolutionParameter convolution_param)： 必须参数 num_output (c_o):卷积核（filter）的个数。 kernel_size (or kernel_h and kernel_w): 卷积核大小，非方阵用_h _w。 推荐参数 weight_filler [default type: ‘constant’ value: 0]：卷积核的初始化，默认为全0，可以用”xavier”算法来进行初始化，也可以设置为”gaussian”。 可选参数 bias_term [default true]:是否开启偏置项，默认为true, 开启。 pad (or pad_h and pad_w) [default 0]: 填零操作，默认为0，不填零。是对原图进行填零，使卷积核在图像边缘能够进行卷积操作，运算后和原图的尺寸相同。扩充的时候是左右、上下对称的，比如卷积核的大小为5*5，那么pad设置为2，则四个边缘都扩充2个像素，即宽度和高度都扩充了4个像素。 stride (or stride_h and stride_w) [default 1]: 卷积核的移动步长，默认为1。 group (g) [default 1]: 分组，默认为1组。如果大于1，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第i个输出分组只能与第i个输入分组进行连接。groups是代表filter 组的个数。引入gruop主要是为了选择性的连接卷基层的输入端和输出端的channels，否则参数会太多。 It was there to implement the grouped convolution in Alex Krizhevsky’s paper: when group=2, the first half of the filters are only connected to the first half of the input channels, and the second half only connected to the second half. 当group=2时，前半部分filter与输入的前半部分通道连接，后半部分filter与后半部分输入通道连接。 输入：n c_i h_i * w_i 输出：n c_o h_o w_o, where h_o = (h_i + 2 pad_h - kernel_h) / stride_h + 1 and w_o likewise。 示例 (./models/bvlc_reference_caffenet/train_val.prototxt) 1234567891011121314151617181920212223242526272829303132layer &#123; name: "conv1" type: "Convolution" bottom: "data" top: "conv1" # learning rate and decay multipliers for the # filters param &#123; lr_mult: 1 decay_mult: 1 &#125; # learning rate and decay multipliers for the biases param &#123; lr_mult: 2 decay_mult: 0 &#125; convolution_param &#123; num_output: 96 # learn 96 filters kernel_size: 11 # each filter is 11x11 stride: 4 # step 4 pixels between each filter # application weight_filler &#123; type: "gaussian" # initialize the filters from a Gaussian std: 0.01 # distribution with stdev 0.01 (default # mean: 0) &#125; bias_filler &#123; type: "constant" # initialize the biases to zero (0) value: 0 &#125; &#125;&#125; 卷积层将输入图像与一组可学习的滤波器进行卷积，每个在输出图像中产生一个特征图。 池化层Pooling Pooling 层一般在网络中是跟在Conv卷积层之后，做采样操作，其实是为了进一步缩小feature map，同时也能增大神经元的视野。 层类型：Pooling CPU实现：./src/caffe/layers/pooling_layer.cpp CUDA GPU实现：./src/caffe/layers/pooling_layer.cu 参数(PoolingParameter pooling_param)： 必须 kernel_size (or kernel_h and kernel_w)：池化核大小。 可选参数 pool [default MAX]: 池化方法，默认为MAX，还有 AVE, or STOCHASTIC。 pad (or pad_h and pad_w) [default 0]:填零。 stride (or stride_h and stride_w) [default 1]:步长。 输入：n c h_i * w_i 输出：n c h_o * w_o，h_o and w_o 与卷积层计算方法相同。 示例 ( ./models/bvlc_reference_caffenet/train_val.prototxt) 12345678910111213layer &#123; name: "pool1" type: "Pooling" bottom: "conv1" top: "pool1" pooling_param &#123; pool: MAX kernel_size: 3 # pool over a 3x3 region stride: 2 # step two pixels (in the # bottom blob) between pooling # regions &#125;&#125; 局部响应归一化层RNL 局部响应归一化层通过对局部输入区域进行归一化来执行一种“横向抑制”。具体作用感觉和特征缩放有点像，使梯度下降在所有方向上具有相同的曲率。而RNL这种方法的计算相比对每个神经元输入归一化要简单。 层类型：LRN CPU实现： ./src/caffe/layers/lrn_layer.cpp CUDA GPU实现：./src/caffe/layers/lrn_layer.cu 参数 (LRNParameter lrn_param)： 可选参数 local_size [default 5]:需要求和的通道数数目（对于跨通道LRN），或者是方形区域求和的变长（对于通道内LRN）。 alpha [default 1]: 比例参数。 beta [default 5]: 指数参数。 norm_region [default ACROSS_CHANNELS]:ACROSS_CHANNELS表示在相邻的通道间求和归一化，但没有空间延伸，即大小为local_size x 1 x 1；WITHIN_CHANNEL表示在一个通道内部特定的区域内进行求和归一化，其大小为：1 x local_size x local_size。每个输入值被除以$(1 + (\alpha/n) \sum_i x_i^2)^\beta$，$n$是每个局部区域的大小。 示例： 1234567891011layers &#123; name: "norm1" type: LRN bottom: "pool1" top: "norm1" lrn_param &#123; local_size: 5 alpha: 0.0001 beta: 0.75 &#125;&#125; Im2col Caffe中卷积操作需要先对数据进行im2col，再进行内积运算，如下图所示：]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（4）数据层]]></title>
    <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%884%EF%BC%89%E6%95%B0%E6%8D%AE%E5%B1%82%2F</url>
    <content type="text"><![CDATA[数据是学习的原料，参考官网和网友的资料，来看一下数据与数据层。 Data：Ins and OutsCaffe学习系列(2)：数据层及参数，denny402 数据：输入与输出 在Caffe中，数据是以Blobs流动的（见caffe学习（1）caffe模型三种结构）。数据层的输入输出便需要由其他格式与Blobs进行相互转换。一些常见的变换如平均减法（mean-subtraction）、特征缩放是通过data layer配置完成。新的输入类型需要开发新的数据层，网络的其余部分遵循Caffe层目录的模块化。下段加载了MNIST数据：1234567891011121314151617181920212223layer &#123; name: "mnist" # Data layer loads leveldb or lmdb storage DBs for high-throughput.加载leveldb 或 lmdb类型的数据实现高吞吐量 type: "Data" # the 1st top is the data itself: the name is only convention top: "data" # the 2nd top is the ground truth: the name is only convention top: "label" # the Data layer configuration data_param &#123; # path to the DB source: "examples/mnist/mnist_train_lmdb" # type of DB: LEVELDB or LMDB (LMDB supports concurrent reads) backend: LMDB # batch processing improves efficiency. batch_size: 64 &#125; # common data transformations transform_param &#123; # feature scaling coefficient: this maps the [0, 255] MNIST data to [0, 1] scale: 0.00390625 &#125;&#125; name: 表示该层的名称，可随意取，本层为”mnist”。 type: 层类型，如果是Data，表示数据来源于LevelDB或LMDB。根据数据的来源不同，数据层的类型也不同（后面会详细阐述）。一般在练习的时候，我们都是采用的LevelDB或LMDB数据，因此层类型设置为Data。 top或bottom: 每一层用bottom来输入数据，用top来输出数据。如果只有top没有bottom，则此层只有输出，没有输入。反之亦然。如果有多个 top或多个bottom，表示有多个blobs数据的输入和输出。 data 与 label: 在数据层中，至少有一个命名为data的top。如果有第二个top，一般命名为label。 这种(data,label)配对是分类模型所必需的。本例中第一个top是数据本身，第二个top是label（ground truth）（这些名字只是约定的）。 include: 一般训练的时候和测试的时候，模型的层是不一样的。该层（layer）是属于训练阶段的层，还是属于测试阶段的层，需要用include来指定。如果没有include参数，则表示该层既在训练模型中，又在测试模型中。（上例中没有出现） 123include &#123; phase: TRAIN #仅在训练中出现 &#125; Transformations: 数据的预处理，可以将数据变换到定义的范围内。如设置scale为0.00390625，实际上就是1/255, 即将输入数据由0-255归一化到0-1之间。除了缩放，还有其他的一些预处理操作： 12345678transform_param &#123; scale: 0.00390625 mean_file_size: "examples/cifar10/mean.binaryproto" # 用一个配置文件来进行均值操作 mirror: 1 # 1表示开启镜像，0表示关闭，也可用ture和false来表示 # 剪裁一个 227*227的图块，在训练阶段随机剪裁（random cropping），在测试阶段从中间裁剪 crop_size: 227 &#125; prefetching：预取，对于吞吐量数据层获取下一批数据，并在Net计算当前批处理时在后台准备。 具体的还需要分析data_param，data_param部分，就是根据数据的来源不同，来进行不同的设置。 数据来自于数据库（如LevelDB和LMDB）层类型（layer type）:Data 必须设置的参数：source: 包含数据库的目录名称batch_size: 每次处理的数据个数，如64 可选的参数：rand_skip: 在开始的时候，跳过一定数量的数据输入，通常对异步的SGD很有用（useful for asynchronous sgd）。backend: 选择是采用LevelDB还是LMDB, 默认是LevelDB.示例： 123data_param &#123; source: "examples/mnist/mnist_train_lmdb" batch_size: 64&#125; 数据来自于内存层类型：MemoryData 必须设置的参数：batch_size：每一次处理的数据个数，比如2channels：通道数height：高度width: 宽度即指定要从内存中读取的输入块的大小。存储器数据层直接从存储器读取数据，而不复制它。为了使用它，必须调用MemoryDataLayer :: Reset（C ++）或Net.set_input_arrays（Python），以便指定一个连续数据源（作为4D行主数组），一次读取一个批处理大小的块。示例： 1234567891011121314151617 layer &#123; top: "data" top: "label" name: "memory_data" type: "MemoryData" memory_data_param&#123; batch_size: 2 height: 100 width: 100 channels: 1 &#125; transform_param &#123; scale: 0.0078125 mean_file: "mean.proto" mirror: false &#125;&#125; 数据来自于HDF5(Input)层类型：HDF5Data 必须设置的参数：source: 读取的文件名称batch_size: 每一次处理的数据个数示例： 12345678910layer &#123; name: "data" type: "HDF5Data" top: "data" top: "label" hdf5_data_param &#123; source: "examples/hdf5_classification/data/train.txt" batch_size: 10 &#125;&#125; 数据输出到HDF5(Output)层类型：HDF5Data 必须设置的参数：file_name: 输出到的文件名称HDF5输出层执行与本节中其他层相反的功能：它将其输出blob写入磁盘。 数据来自于图片层类型：ImageData 必须设置的参数：source: 一个文本文件的名字，每一行给定一个图片文件的名称和标签（label）batch_size: 每一次处理的数据个数，即图片数 可选参数：rand_skip: 在开始的时候，跳过一定的数据输入。通常对异步的SGD很有用。shuffle: 随机打乱顺序，默认值为falsenew_height,new_width: 如果设置，则将图片进行resize示例：1234567891011121314151617layer &#123; name: "data" type: "ImageData" top: "data" top: "label" transform_param &#123; mirror: false crop_size: 227 mean_file: "data/ilsvrc12/imagenet_mean.binaryproto" &#125; image_data_param &#123; source: "examples/_temp/file_list.txt" batch_size: 50 new_height: 256 new_width: 256 &#125;&#125; 数据来源于Windows层类型：WindowData 必须设置的参数：source: 一个文本文件的名字batch_size: 每一次处理的数据个数，即图片数示例：1234567891011121314151617181920212223 layer &#123; name: "data" type: "WindowData" top: "data" top: "label" include &#123; phase: TRAIN &#125; transform_param &#123; mirror: true crop_size: 227 mean_file: "data/ilsvrc12/imagenet_mean.binaryproto" &#125; window_data_param &#123; source: "examples/finetune_pascal_detection/window_file_2007_trainval.txt" batch_size: 128 fg_threshold: 0.5 bg_threshold: 0.5 fg_fraction: 0.25 context_pad: 16 crop_mode: "warp" &#125;&#125; DummyDummyData用于调试，详见DummyDataParameter。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（3）接口]]></title>
    <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%883%EF%BC%89%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[接口Interfaces Interfaces Caffe提供丰富的接口，比如命令行，python，matlab。先说一下命令行 命令行 caffe命令及其参数解析，Single、Dog Caffe的程序位于caffe / build / tools，运行时可以在根目录执行./build/tools/caffe &lt;command&gt;&lt;args&gt;。其中&lt;command&gt;有四种： train：训练或finetune模型(model) test：测试模型 device_query：显示gpu信息 time：显示程序执行时间 其中的&lt;args&gt;参数有： -solver -gpu -snapshot -weights -model -sighup_effect -sigint_effect 训练traincaffe train可以从头开始学习模型、从已保存的快照中恢复学习或添加新数据进行fine-tunes。具体来说，所有训练需要通过-solver solver.prototxt参数进行求解器配置；恢复需要使用-snapshot model_iter_1000.solverstate参数来加载求解程序快照；fine-tunes微调需要模型初始化的-weights model.caffemodel参数。 12345678# train LeNet 训练LeNetcaffe train -solver examples/mnist/lenet_solver.prototxt# train on GPU 2 在特定的GPU上caffe train -solver examples/mnist/lenet_solver.prototxt -gpu 2# resume training from the half-way point snapshot 从快照恢复caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate# fine-tune CaffeNet model weights for style recognition 完整例子参阅examples/finetuning_on_flickr_style，仅调用可使用：caffe train -solver examples/finetuning_on_flickr_style/solver.prototxt -weights models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel 对于train的参数，功能为： -solver：必选，后跟一个protocol buffer类型(.prototxt)的文件，即模型的配置文件。 -gpu：可选，指定某一块GPU运行，-gpu all是所有运行： 1234# train on GPUs 0 &amp; 1 (doubling the batch size)caffe train -solver examples/mnist/lenet_solver.prototxt -gpu 0,1# train on all GPUs (multiplying batch size by number of devices)caffe train -solver examples/mnist/lenet_solver.prototxt -gpu all -snapshot：可选，从快照中恢复，设置快照可从solver配置中进行，保存为solverstate。 -weights：可选参数。用预先训练好的权重来fine-tuning模型，需要一个caffemodel，不能和-snapshot同时使用。 -iterations： 可选参数，迭代次数，默认为50。 如果在配置文件文件中没有设定迭代次数，则默认迭代50次。 -model：可选参数，定义在protocol buffer文件中的模型。也可以在solver配置文件中指定。 -sighup_effect：可选参数。用来设定当程序发生挂起事件时，执行的操作，可以设置为snapshot, stop或none, 默认为snapshot。 -sigint_effect： 可选参数。用来设定当程序发生键盘中止事件时（ctrl+c), 执行的操作，可以设置为snapshot, stop或none, 默认为stop。测试test测试时输出每个batch得分，最后返回平均值。test参数用在测试阶段，用于最终结果的输出，要模型配置文件中我们可以设定需要输入accuracy还是loss. 假设我们要在验证集中验证已经训练好的模型，就可以这样写123# score the learned LeNet model on the validation set as defined in the# model architeture lenet_train_test.prototxtcaffe test -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu 0 -iterations 100 意思是利用训练好了的权重（-weight)，输入到测试模型中(-model)，用编号为0的gpu(-gpu)测试100次(-iteration)。 时间timetime参数用来在屏幕上显示程序运行时间。如：123# (These example calls require you complete the LeNet / MNIST example first.)# time LeNet training on CPU for 10 iterationscaffe time -model examples/mnist/lenet_train_test.prototxt -iterations 10 这个例子用来在屏幕上显示lenet模型迭代10次所使用的时间。包括每次迭代的forward和backward所用的时间，也包括每层forward和backward所用的平均时间。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（2）前后传播，loss，solver]]></title>
    <url>%2F2016%2F11%2F06%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89%E5%89%8D%E5%90%8E%E4%BC%A0%E6%92%AD%EF%BC%8Closs%EF%BC%8Csolver%2F</url>
    <content type="text"><![CDATA[向前和向后传播 Forward and Backward 前后传播是Net的重要组成，如下图所示： 向前Forward通过给定的参数计算每层的值，就像函数一样top=f(bottom)。上图表示数据通过内积层输出，再由softmax给出损失。 向后Backward向后是计算loss的梯度，每层梯度通过自动微分来计算整个模型梯度，即反向传播。从这个图上可以看出，由loss开始，通过链式法则，不断求出结果对各层的导数。Net::Forward()和Net::Backward()是针对网络，Layer::Forward()和Layer::Backward()是针对每一层。同时也可以设置CPU、GPU模式。过程大概是：solver调用forward计算输出和loss，再生成梯度，并尝试更新权重减小loss。 损失Loss Loss Loss使loss变小，是学习中的一个目标。如上所说，loss是由forward计算而出。1234567layer &#123; name: "loss" type: "SoftmaxWithLoss" bottom: "pred" bottom: "label" top: "loss"&#125; 这一段就是上面流程图最后一层loss的表达。 Loss weights一般的loss只是最后一层才有，其他层只是中间计算，不过每层都可以通过增加loss_weight: &lt;float&gt;到该层生成的每个顶（top）层中。对于后缀有loss的层都隐含着loss_weight: 1（对第一个top，其他的loss_weight: 0）。因此上面代码也等价于在最后加上loss_weight: 1。然而任何能反向传播的层都可以赋予非零loss_weight，最终的loss由网络上各层loss权重求得1234loss := 0for layer in layers: for top, loss_weight in layer.tops, layer.loss_weights: loss += loss_weight * sum(top) Solver Solver 分类Solver通过forward和backward形成参数更新，从而改善loss。包括： Stochastic Gradient Descent (type: “SGD”), AdaDelta (type: “AdaDelta”), Adaptive Gradient (type: “AdaGrad”), Adam (type: “Adam”), Nesterov’s Accelerated Gradient (type: “Nesterov”) and RMSprop (type: “RMSProp”) 方法对于数据集$D$，优化目标是使整个$|D|$的平均loss最小，即：$$L(W) = \frac{1}{|D|} \sum_i^{|D|} f_W\left(X^{(i)}\right) + \lambda r(W)$$其中$f_W\left(X^{(i)}\right)$是对于数据$X^{(i)}$输入的损失，$r(W)$是加权$\lambda$的权重$W$正则项。通常用于学习的数据量$|D|$很大，在学习时常常将其分为很多大小为$N$的batch，其中$N&lt;&lt;|D|$，可以将原式中的$|D|$换为$N$。$$L(W) \approx \frac{1}{N} \sum_i^N f_W\left(X^{(i)}\right) + \lambda r(W)$$模型向前计算$f_W$，向后返回梯度$\nabla f_W$。参数的更新$\Delta W$由solver从$\nabla f_W$得到，正则化梯度每种方法得到的不同。具体每种solver，网上讲的很多，这里就不讲了。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe学习（1）caffe模型三种结构]]></title>
    <url>%2F2016%2F11%2F05%2Fcaffe%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89caffe%E6%A8%A1%E5%9E%8B%E4%B8%89%E7%A7%8D%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[caffe模型三种结构 自己写的然而CSDN出bug了，绑定三方账户原来的博客无法编辑，只好转发过来 Blobs, Layers, and Nets: anatomy of a Caffe modelBlob：存储和传递（communication）blob是数据存储和传输的包装，并且还在底层提供CPU和GPU之间的同步能力。Blob提供了保存数据的统一存储器接口； 例如图像批次，模型参数和用于优化的导数。 在数学上，blob是以C连续方式（C-contiguous fashion）存储的N维数组。 关于C连续方式，stackoverflow有一个解释。该方式主要与Fortran和Matlab相反，是一种以行为主顺序（Row-major order）的存储方式，简单的说就是把一行存完，再存下一行，把第一个通道（channel）的所有行写完后写完再写下一个通道。例如对一批（batches）图像，用4维blob存储，表示为number N（数据批量大小） x channel K（通道、维度特征） x height H （高）x width W（宽），对于索引 (n, k, h, w) 的物理地址就是：((n K + k) H + h) W + w，注意区分大小写，大写是总的，小写是索引值。对于图像是4D的，当然也可以不用4D。具体参数需要根据层类型和数据大小配置。 blob使用了两块存储区域，为data（数据）和diff（网络梯度），实际值可以存储在CPU或GPU上，访问也可以不变（const）访问或者可变（mutable）访问。 const Dtype cpu_data() const; Dtype* mutable_cpu_data(); 同理可得GPU上diff类型数据操作。官网上有一个example，展示了数据在CPU和GPU上流动操作。Layer计算和连接Layer包括很多计算方法，如Vision Layers：Convolution、Pooling、LRNLoss Layers：Softmax、Sum-of-Squares既然作为计算，就有输入输出，输入从底部（bottom）获取，并通过顶部（top）连接输出。每层须有三个关键计算：setup, forward, and backward。setup：初始化层和连接。forward：底部向顶部计算。backward：给定梯度，从top计算传回bottom。A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.（是说存在layer中吗）forward和backward也分为CPU和GPU两个版本。 If you do not implement a GPU version, the layer will fall back to the CPU functions as a backup option. This may come handy if you would like to do quick experiments, although it may come with additional data transfer cost 这里好像是说使用GPU会因为数据需要从CPU复制到GPU上，因此会有数据传输成本，但GPU跑的还是快一些，所以是quick experiments。Net定义和操作Net由Layer组成（The net is a set of layers connected in a computation graph有向无环图）。模型初始化由Net :: Init（）处理：主要是创建blob和layer，并调用layer里的setup，同时会输出INFO。模型格式模型定义在.prototxt文件中，训练好的模型在model目录下.binaryproto格式的文件中，模型的格式由caffe.proto定义。]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe运行MNIST | example]]></title>
    <url>%2F2016%2F10%2F09%2Fcaffe-e8-bf-90-e8-a1-8cmnist-example%2F</url>
    <content type="text"><![CDATA[参考教程测试一下caffe。 1.获取数据包。 cd $CAFFE_ROOT ./data/mnist/get_mnist.sh ./examples/mnist/create_mnist.sh 官网上是这样的，caffe_root是根目录，看网上有人说必须在根目录下运行，否则会出错，具体没有验证。。但是windows平台下应该是没有wget的，需要自己下载一下。得到四个文件，测试与训练，样本与标签。 第二句是个坑，直接执行的话会提示找不到convert_mnist_data.bin（好像是这个）。这个环境还是linux下的，windows下编译出来的是exe，和caffe在一起。用法是 convert_mnist_data [FLAGS] input_image_file input_label_file output_db_file执行两次将mnist date转化为可用的lmdb格式的文件。并将新生成的2个文件mnist-train-lmdb 和 mnist-test-lmdb放于create_mnist.sh同目录下。 2.测试 参数文件用的是 ./build/tools/caffe train –solver=examples/mnist/lenet_solver.prototxt mnist_test_lmdb mnist_train_lmdb 两个文件夹需要放在\examples\mnist。如果像我没用GPU，还需要在.prototxt里面更改solver_mode为 CPU。 感觉跑了一个半小时吧，终于跑完了。生成了四个文件……数据都训练好之后，接下来就是如何将模型应用到实际数据了（记录的博客）： ./build/tools/caffe.bin test -model=examples/mnist/lenet_train_test.prototxt -weights=examples/mnist/lenet_iter_10000.caffemodel -gpu=0 如果没有GPU则使用 ./build/tools/caffe.bin test -model=examples/mnist/lenet_train_test.prototxt -weights=examples/mnist/lenet_iter_10000.caffemodel看起来准确率很高。 Training LeNet on MNIST with Caffe]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>mnist</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Caffe | Installation caffe 安装]]></title>
    <url>%2F2016%2F10%2F05%2Fcaffe-installation-caffe-e5-ae-89-e8-a3-85%2F</url>
    <content type="text"><![CDATA[尝试下caffe在windows的安装。 本本用的还是A卡，干脆禁掉CUDA和cuDNN吧，换新的重新安装再配置。 &lt;CpuOnlyBuild&gt;true&lt;/CpuOnlyBuild&gt;&lt;UseCuDNN&gt;false&lt;/UseCuDNN&gt;&lt;PythonSupport&gt;true&lt;/PythonSupport&gt;之后是python，之前装过又删掉了，这次再装一遍吧。windows下推荐的是Miniconda环境，里面集成了python2.7。运行 conda install --yes numpy scipy matplotlib scikit-image pip pip install protobuf 是在Miniconda2文件夹中cmd里面运行的，会自动下载安装一些库，本机下载了250M左右，第一次下载到第二个就卡住了QAQ，然后关了之后再输说之前存在任务触发保护了需要解锁，输入以下命令解锁。 conda clean –lock虚拟环境创建操作参考了conda简单使用，一开始一直在python里面输命令，好蠢= =miniconda只是一个方便配置python的虚拟环境，里面python可以根据需要再加自己要的库。这个步骤应该是在前面的库安装完了之后再进行，否则新安装的库好像不会在老的环境里出现。删除原有环境： conda remove –name 此处是环境名字 –all其他没有怎么设置了，直接用vs2013打开编译…… 但是感觉过了一年，卡在了这里： nuget好像也是一个管理库的插件，会自动下载各种东西，然而下载速度简直无语。。感觉过了几个小时下载好了，多了900MB东西。然后build……又是一堆错误。 首先是libcaffe.lib无法生成，其中两个小问题，找不到layer_factory.h和pyconfig.h，分别在caffe目录中和python目录中。之后找不到python27.lib，也在python中libs里，但是还会有一堆warning，不管了。。 最后终于都build完成，Debug版的出来1.8G，release版的提示nuget超时，不太清楚原因。python部分把生成出来的拷到python目录\lib\site-packages，import一下应该就可以了。 测试的话用的生成出来的test_all.exe，一开始跑得还快，后面的单项都能到40多秒，果然学习不是轻松的事情= =出现一些错误，好像是找不到测试图片，所以程序上应该没有错了。 哎，编译不易，还须多珍惜。 来源： Caffe | Installation]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>caffe</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[出发|关服]]></title>
    <url>%2F2016%2F09%2F21%2Fe5-87-ba-e5-8f-91-e5-85-b3-e6-9c-8d%2F</url>
    <content type="text"><![CDATA[明天出发，跑个三角形，树莓派可以休息啦。]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[机会还是会留给有准备的人啊~]]></title>
    <url>%2F2016%2F09%2F19%2Fe6-9c-ba-e4-bc-9a-e8-bf-98-e6-98-af-e4-bc-9a-e7-95-99-e7-bb-99-e6-9c-89-e5-87-86-e5-a4-87-e7-9a-84-e4-ba-ba-e5-95-8a%2F</url>
    <content type="text"><![CDATA[这段时间这么盲目、慌张，也好久没有这样的感觉了，可能习惯了按部就班的生活…… 简单说学到了几点： 1.计划可能真的赶不上变化，事情没有发生就不能确定，也要留后手。 2.多准备，不能把鸡蛋装一个篮子里。 3.要有一定的紧张感，居安思危。 &nbsp; PS：树莓派工作了好几天没休息，不知道有没有事。。 过几天如果出远门就关了]]></content>
      <categories>
        <category>感</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[从flash到手游]]></title>
    <url>%2F2016%2F09%2F13%2Fe4-bb-8eflash-e5-88-b0-e6-89-8b-e6-b8-b8%2F</url>
    <content type="text"><![CDATA[近日室友又重新怀旧，玩起了金庸群侠传这个flash游戏。老实说我对这个游戏印象并不是很深，因为一没读过金庸，二来游戏确实很复杂，系统、任务众多，不是简单的休闲游戏。在我看来，这样一个出色的rpg游戏，可以算是flash游戏的巅峰了。如果是一个pc游戏，可能反而没有这么大的影响力。 03、04年，大概是小学三四年级的时候，电脑已经比较普遍了，特别是在我们小学已经开始上微机课了。更重要的一点是宽带取代了拨号上网，再也没有了奇奇怪怪叽叽喳喳的拨号声音，速度也能到几百k。有了网络物质基础，就需要寻求网络娱乐内容。在家里，梦幻西游、大话西游成了大家的首选（至少是男生吧），回家总是会打开玩一会儿。神奇的是我没有在这两个游戏上充过点卡，要不是自己就玩个免费的阶段（10级前吧），要不就玩同学的号。网吧更多的可能是cs、流星蝴蝶剑的天下，但是我也一次没有去过。而在学校，flash游戏成了班上所有同学的选择，优点显而易见：无需安装打开就玩；体积小，最多加载个一分钟；类型多，打开4399这类网站，光游戏分类就琳琅满目，每个人都能找到自己所需要的。于是微机课上完自由活动的时间，也就成了班上flash游戏的时间。flash游戏分类里面有一个双人小游戏，这是我和小伙伴最喜欢的。常常和小伙伴两个人挤在一张电脑桌前，还好当时手小，键盘放四只手也没问题，愉快的游戏时间就开始了。这可能就是小时候的“开黑”吧~ 那时我们突然意识到，除了打球、踢球这种运动，原来还有游戏这个虚拟的空间能够让我们站在一起，或是并肩作战，或是相互厮杀。因此本质上，游戏应该和其他的运动项目没有什么区别。胜利有时也不是必须的，不是常说友谊第一比赛第二吗？人与人之间的关系才是重要的。 不过这么多年过去了，无论是游戏，还是玩游戏的人，似乎都变了很多。 flash游戏受限于平台，难以做得更好，移动端随着智能手机逐渐发力，手游如雨后春笋不断出现。金庸群侠传原作者半瓶神仙醋，也亲自投身于手游开发。然而手游看似更为休闲，但实际上往里面投钱的并不少。这又是为什么呢？ 或许是曾经的关系搞反了。现在网络更方便，人们可以从网上认识到更多的人，甚至早于在现实中相识。因此想要得到对方的认同的话，游戏中的一堆数字是最好的证明。而最简单的方法就是充值……这么看来，游戏更是一种炫耀、渴望被认同的一种手段。当初和现实朋友一起奋战的感觉变得更少了。 但是另一方面，游戏业也在这种氛围中钱越赚越多。半瓶神仙醋的微博，还停留在去年，其中靠前的一条是给自己的手游《冒险王2》宣传，而金庸群侠传4好像已经无限跳票，做免费游戏当然不能是他的义务，玩家自然不能强迫。毕竟如今35岁，可能再也追不回之前的梦了。我想我可能也是如此吧，十年后又能记得起如今的梦吗？还是定一个小目标：十年后还能记得起十年前身边的人，和与这些人一起的故事吧。 &nbsp; 自慰诗 半瓶神仙醋， 生于冀中， 学于湘潭， 混迹于京师。 苟活二十有五， 孑然一身， 惨淡经营， 做此游戏， 聊已慰藉。 不求闻达于诸侯， 不甘堕落于俗事。 虽勤勉学习， 艰苦奋斗， 仍然一事无成， 可叹造化弄人也. （此为2006年金庸群侠传2中所作）]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初次评论无需审核]]></title>
    <url>%2F2016%2F09%2F08%2Fe5-88-9d-e6-ac-a1-e8-af-84-e8-ae-ba-e6-97-a0-e9-9c-80-e5-ae-a1-e6-a0-b8%2F</url>
    <content type="text"><![CDATA[应该是这个设置吧，以后大家评论也不用经过审核了，我审起来也麻烦。。 PS：检查卫生说我桌子乱不是一次两次了，这次为了收拾桌子就把树莓派收起来了，今天服务器就没开= =]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[祝你有美好的一天'√']]></title>
    <url>%2F2016%2F09%2F07%2Fe7-a5-9d-e4-bd-a0-e6-9c-89-e7-be-8e-e5-a5-bd-e7-9a-84-e4-b8-80-e5-a4-a9-e2-88-9a%2F</url>
    <content type="text"><![CDATA[如果你有幸和我到ktv，那么一定不会错过一首歌：Have a nice day。 虽然嗓音不行，但是瞎喊喊就很爽了，唱歌不就是为了表达宣泄感情吗？ 不过说到这首歌，不得不提这首歌的乐队：Bon Jovi。 最初接触他们是小学在某电视节目上，放的CS的视频，BGM就是It’s my life，这也是我认为是最被国人所知的一首歌。视频里精彩的击杀镜头被这首歌衬托得十分酷炫，不过当时可能更多的激起了玩游戏的欲望…… 这首歌始终被埋在记忆深处，直到初中开始学习英语，英语老师让我们学唱一首英文歌。一开始选的是You are my sunshine，反光镜乐队版，很短很简单= =会唱之后很没有成就感，就想再试试其他歌，不知怎么，It’s my life就悄悄在脑中响起……最后在课上老师让我演唱一曲，我也不知道最后唱的哪一个，但是五音不全是肯定的。 唱歌受挫，听歌总不会如此。我便更多地去了解这个乐队，Have a nice day、Bounce、Always等一首首加入了我的mp3。至少在初中、高中这段紧张的学习生涯中给了我很多动力。 Oh, if there’s one thing I hang onto that gets me through the night I ain’t gonna do what I don’t want to; I’m gonna live my life Shining like a diamond, rolling with the dice Standing on the ledge, I show the wind how to fly When the world gets in my face, I say Have a nice day！ 《Have a nice day》 初高中听这种歌是为了提神，因为每天辛苦的学习可能都不是我们所希望做的，或者说”live my life”。但我们可以把这个过程当作”live my life”的准备，“考上大学就能live my life”了。事实如此吗？大学里好像也只是按着课程，为了成绩、排名而努力，量化到了一个数字，这就是“life”吗？是自己的“life”还是别人规划好的“life”呢？可能我只是把别人交给的任务努力做的优秀罢了。 [caption id=”attachment_39” align=”aligncenter” width=”300”] ‘√’[/caption] 不过我也希望有一天能够找到自己的“life”，因此从初二开始，这个专辑封面一直作为了我的头像，希望Have a nice day，也希望live my life。PS：今年Bon Jovi又发了新专，This House Is Not for Sale，毕竟62年出生的人了。]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>bon jovi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[发微博啦~ Joy是小一一]]></title>
    <url>%2F2016%2F09%2F03%2Fe5-8f-91-e5-be-ae-e5-8d-9a-e5-95-a6-joy-e6-98-af-e5-b0-8f-e4-b8-80-e4-b8-80%2F</url>
    <content type="text"><![CDATA[如果再加上if weibo then wordpress会不会无限循环啊。。[笑cry] (via Weibo http://ift.tt/2c0xa0b)]]></content>
      <categories>
        <category>微博</category>
      </categories>
      <tags>
        <tag>IFTTT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微博的朋友们你们好吗]]></title>
    <url>%2F2016%2F09%2F03%2Fe5-be-ae-e5-8d-9a-e7-9a-84-e6-9c-8b-e5-8f-8b-e4-bb-ac-e4-bd-a0-e4-bb-ac-e5-a5-bd-e5-90-97%2F</url>
    <content type="text"><![CDATA[把wordpress和weibo连起来了，这么说我发一条文章就会自动发一条微博咯？ 测试一下]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
      <tags>
        <tag>IFTTT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Real or Not]]></title>
    <url>%2F2016%2F09%2F03%2Freal-or-not%2F</url>
    <content type="text"><![CDATA[美剧有个特点，就是有点长，还拖成好几季，比如之前看的南方公园，目前估计都要二十季了吧。入坑还是要谨慎。 然而我还是经不住诱惑开始看Rick and Morty了……不得不说脑洞太大，动画的形式充分展现了无边的想象力。 比如虚拟（Rick and Morty (e4)）和梦境（Rick and Morty (e2) ）的两集，分别讲述二人在人工制造的虚拟城市和多层梦境（有点像盗梦空间）的冒险。这种经历往往会让人对虚拟和现实产生一定的疑问。 当然很多人都开始了这种思考，特别是人们逐渐了解人体、生命之间的秘密和联系之后，发现很多都是可以解释和人工产生的。最疯狂的可能是“缸中之脑”这个概念。这是希拉里·普特南（Hilary Putnam）1981年在他的《理性，真理与历史》（Reason, Truth, and History）一书中，阐述的假想。艺术作品也对这种设定情有独钟，频频出现于各种科幻剧里，个人感觉《黑客帝国》是最著名的。 那么问题来了，如果有一天发现世界并不是真实的，应该怎么办？似乎这比今天吃什么还要难以抉择。 即使是虚拟的世界，但通过努力完成了一定的工作量，在这个世界中有所贡献，似乎也是不能轻言放弃的，就像minecraft中一个个宏伟的工程。同样与他人之间的关系也很重要，不过如果得知其他所有的人都是虚拟出来，只是AI的话一定很难以接受。哪个选择才是正确的，或许并不重要，而在虚拟中和真实中度过一生又有什么不同呢？ 如果这种事情发生在我身上，具体应该如何选择，的确是需要好好考虑一番。但是让我用一句话总结的话，还是选择一个更需要我的世界吧。]]></content>
      <categories>
        <category>感</category>
      </categories>
      <tags>
        <tag>瑞克和莫蒂</tag>
        <tag>Rick and Morty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[去健身吧~]]></title>
    <url>%2F2016%2F09%2F01%2Fe5-8e-bb-e5-81-a5-e8-ba-ab-e5-90-a7%2F</url>
    <content type="text"><![CDATA[去健身吧~ （其实去年我也是这么说的） 最初的原因其实是看了一部JoJo的动漫，人物的画风大概是这样的： &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 以及这样 感觉好强啊= =，对于习惯正常日漫萌系画风的我还是有一定的冲击（特别是这些羞耻？的造型。。）。不过就剧情来讲，差不多每一场战斗都是靠的脑子取胜的，看起来还是很有意思。 于是我也开始了跟风办了健身卡，大二下学期尽量保证每周至少去了一次……发现没什么用。。。 现在有了时间（才不是看了新的动漫），尽量养成健身的习惯吧。而且这办卡还是去了三次才办成，来之不易，更应该好好珍惜。 那么一起加油吧~ 最后！！！！！《99》完整版出了！！！！不来听一下吗？ [audio mp3=”http://yanjoy.win/wp-content/uploads/MOB-CHOIR-99.mp3&quot;][/audio]]]></content>
      <categories>
        <category>闲</category>
      </categories>
      <tags>
        <tag>健身</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[上传文件限制解除]]></title>
    <url>%2F2016%2F08%2F31%2Fe4-b8-8a-e4-bc-a0-e6-96-87-e4-bb-b6-e9-99-90-e5-88-b6-e8-a7-a3-e9-99-a4%2F</url>
    <content type="text"><![CDATA[想添加首歌，然而打开添加媒体…… “最大上传文件大小：2MB。” ？？？什么年代了这种限制简直不够看啊！ 还好大家都是这么想的 先是参考wpyou.com，三种方法都尝试了结果都不行，特别是第二个找不到php.ini，自己在根目录创建了一个。但是眉头一皱感觉事情没有那么简单 查找资料发现限制大小的其实是php，那么php.ini应该在php目录中。参考冰莫言php安装路径，果然找到了，然而不止一个： 这里面3、4个目录都有php.ini。。所以都改了吧= =，把upload_max_filesize 由2M改到64M。心满意足~~~~ 干完之后发现果然增加了！！！！增到8M了！！ ？？？？跟说好的不一样啊！！！莫非是大意了？ 再看一下文章，果然我只改了upload，还有post_max_size 没改。一看post_max_size=8M，对这个数字很熟悉啊，看来上传媒体文件不能大于post大小限制。 而且jb51.net上有更详细的说明，顺便改了wp-config.php的内容。 一切完工，成功了~ PS：早知道改这么麻烦就把限制再放开些了 [playlist ids=”74”]]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
      <tags>
        <tag>wordpress</tag>
        <tag>文件大小限制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树莓派搭建wordpress|心路历程（雾]]></title>
    <url>%2F2016%2F08%2F30%2Fe6-a0-91-e8-8e-93-e6-b4-be-e6-90-ad-e5-bb-bawordpress-e5-bf-83-e8-b7-af-e5-8e-86-e7-a8-8b-ef-bc-88-e9-9b-be%2F</url>
    <content type="text"><![CDATA[项目这几个月都没有动，两个树莓派就放那里不断吃灰= = 为了废物利用（？）还是随便建个小站玩玩吧。 虽然说wp是个很傻嗨的东西，但现在还没有完全搞定。。 首先参考的是果壳及其引文，采用的是nginx。照着做到最后一步，wp-admin/install.php 打不开QAQ是一片空白。。 之后删了nginx换apache，然而……还是这样 联想到第四步和实际情况的不符 在/var/www目录下新建一个index.php文件： $ sudo nano /var/www/index.php 在这个文件里只需写入一行： 保存并退出编辑。删除该目录下的index.html文件，再次用浏览器打开Apache服务器的默认起始页面，应该能够看到PHP的配置信息。 感觉可能是index没放对地方。/www 里面还有个 /html，放进这里才能显示。。 这么说我把wordpress放在/www 里面没有卵用，放到/www/html 里面才行。 结果肯定是成功了（要不就没有这些东西了） 可是更新功能全跪（无法创建目录），另外无法从wp上传文件（显示上传成功在服务器上找不到） 网上所有的方法其实都一样——给权限。但是疯狂给权限之后还是不行啊~~~ 上传服务器有人说改一下uploadspath，改成/wp-content/uploads 就可以了，实践后貌似没用，就干脆改成全路径/usr/share/wordprees 也加上竟然可以了（玄学 所以忙了大概一天，解决了这些东西 感觉自己什么都不会 用树莓派建LAMP+WordPress服务器树莓派Raspberry Pi讨论区技术论坛云汉电子社区来源： 用树莓派建LAMP+WordPress服务器树莓派Raspberry Pi讨论区技术论坛_云汉电子社区_ &nbsp; PS：拿这个当第一篇博文貌似并不文艺，写博客的初衷是啥呢？重拾文艺撩妹]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
      <tags>
        <tag>wordpress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小站开工]]></title>
    <url>%2F2016%2F08%2F29%2Fe5-b0-8f-e7-ab-99-e5-bc-80-e5-b7-a5%2F</url>
    <content type="text"><![CDATA[差不多干了一天，只剩下插件和主题两片乌云]]></content>
      <categories>
        <category>网站维护</category>
      </categories>
  </entry>
</search>
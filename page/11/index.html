<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '7.1.0',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Default~">
<meta property="og:type" content="website">
<meta property="og:title" content="小一一的小站">
<meta property="og:url" content="https://joyfyan.github.io/page/11/index.html">
<meta property="og:site_name" content="小一一的小站">
<meta property="og:description" content="Default~">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="小一一的小站">
<meta name="twitter:description" content="Default~">





  
  
  <link rel="canonical" href="https://joyfyan.github.io/page/11/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>小一一的小站</title>
  






  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?59765bbe651ba5fb69dc22dd74eb6ba7";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>







  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小一一的小站</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Blog of Yan Joy</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/08/caffe学习（9）LeNet在Caffe上的使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/08/caffe学习（9）LeNet在Caffe上的使用/" class="post-title-link" itemprop="url">caffe学习（9）LeNet在Caffe上的使用</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-08 15:57:00" itemprop="dateCreated datePublished" datetime="2016-11-08T15:57:00+08:00">2016-11-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:30:12" itemprop="dateModified" datetime="2016-11-18T15:30:12+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/08/caffe学习（9）LeNet在Caffe上的使用/" class="leancloud_visitors" data-flag-title="caffe学习（9）LeNet在Caffe上的使用">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用官网例程训练LeNet。</p>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/gathered/examples/mnist.html" target="_blank" rel="noopener">Training LeNet on MNIST with Caffe</a></p>
</blockquote>
<hr>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><p>Caffe程序的运行要注意需命令行要在Caffe的根目录下。</p>
<pre><code>cd $CAFFE_ROOT
./data/mnist/get_mnist.sh
./examples/mnist/create_mnist.sh
</code></pre><p>依次运行，会在caffe\examples\mnist下得到两个目录mnist_train_lmdb, 和 mnist_test_lmdb，作为训练和测试集。</p>
<hr>
<h2 id="定义MNIST网络"><a href="#定义MNIST网络" class="headerlink" title="定义MNIST网络"></a>定义MNIST网络</h2><p>Caffe上的LeNet并不是传统的<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">LeNet-5</a>，在参数上还是有所不同的。以\caffe\examples\mnist\lenet_train_test.prototxt 为例（本地文件与官网上的教程也有所区别），介绍一下如何定义网络。首先是定义网络名称：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">name: <span class="string">"LeNet"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="数据层"><a href="#数据层" class="headerlink" title="数据层"></a>数据层</h3><p>利用我们已经生成的MNIST数据，把数据输入到网络：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  type: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TRAIN</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: <span class="number">0.00390625</span></span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: <span class="string">"examples/mnist/mnist_train_lmdb"</span></span><br><span class="line">    batch_size: <span class="number">64</span></span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  type: <span class="string">"Data"</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">  transform_param &#123;</span><br><span class="line">    scale: <span class="number">0.00390625</span></span><br><span class="line">  &#125;</span><br><span class="line">  data_param &#123;</span><br><span class="line">    source: <span class="string">"examples/mnist/mnist_test_lmdb"</span></span><br><span class="line">    batch_size: <span class="number">100</span></span><br><span class="line">    backend: LMDB</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>具体来说，本层名为：”mnist”，类型为”data”，输出到两个blob：”data””label”。下面用到了之前所说的include，包含TRAIN与TEST，表示该层是在训练还是测试时调用，其区别在于输入的不同数据集（见data_param）。transform_param用于输入数据的缩放，使之在$[0,1]$内，其中$0.00390625=1/256$。</p>
<h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>本网络中，有两个卷积层，第一层为：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"conv1"</span></span><br><span class="line">  type: <span class="string">"Convolution"</span></span><br><span class="line">  bottom: <span class="string">"data"</span></span><br><span class="line">  top: <span class="string">"conv1"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">2</span></span><br><span class="line">  &#125;</span><br><span class="line">  convolution_param &#123;</span><br><span class="line">    num_output: <span class="number">20</span></span><br><span class="line">    kernel_size: <span class="number">5</span></span><br><span class="line">    stride: <span class="number">1</span></span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>这一层使用数据层的数据作为输入，生成”conv1”层，具体产生20个通道的输出，卷积核大小为5，卷积步长为1。先后两个lr_mults是对本层可学习参数的速率调整，权重学习率与solver中的学习率相同，而偏置学习率为其两倍，这往往导致更好的手链率。权重初始化使用”xavier”方式，偏置初始化为0。<br>第二个卷积层在池化层1后，输出到池化层2，参数除了输出个数（num_output）改为50，其余的相同。</p>
<h3 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"pool1"</span></span><br><span class="line">  type: <span class="string">"Pooling"</span></span><br><span class="line">  bottom: <span class="string">"conv1"</span></span><br><span class="line">  top: <span class="string">"pool1"</span></span><br><span class="line">  pooling_param &#123;</span><br><span class="line">    pool: MAX</span><br><span class="line">    kernel_size: <span class="number">2</span></span><br><span class="line">    stride: <span class="number">2</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>第一个池化层表示采用最大池化的方法，进行大小为2，步长为2的非重叠池化。第二个池化层与第一个完全相同，其输入为卷积层2，输出到全连接层1.</p>
<h3 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"ip1"</span></span><br><span class="line">  type: <span class="string">"InnerProduct"</span></span><br><span class="line">  bottom: <span class="string">"pool2"</span></span><br><span class="line">  top: <span class="string">"ip1"</span></span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line">  param &#123;</span><br><span class="line">    lr_mult: <span class="number">2</span></span><br><span class="line">  &#125;</span><br><span class="line">  inner_product_param &#123;</span><br><span class="line">    num_output: <span class="number">500</span></span><br><span class="line">    weight_filler &#123;</span><br><span class="line">      type: <span class="string">"xavier"</span></span><br><span class="line">    &#125;</span><br><span class="line">    bias_filler &#123;</span><br><span class="line">      type: <span class="string">"constant"</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>全连接层与卷积层的写法非常相似，ip1层产生500个输出。<br>在激活层后，还有一个全连接层，用于最后的输出分类，因此有10个输出。</p>
<h3 id="激活层"><a href="#激活层" class="headerlink" title="激活层"></a>激活层</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"relu1"</span></span><br><span class="line">  type: <span class="string">"ReLU"</span></span><br><span class="line">  bottom: <span class="string">"ip1"</span></span><br><span class="line">  top: <span class="string">"ip1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>ReLU是一个元素操作，因此可以使用原地操作（in-place operations）用于节省空间。其实就是top与bottom的名字相同。当然其他的层不能使用重复的blob名称。</p>
<h3 id="损失层"><a href="#损失层" class="headerlink" title="损失层"></a>损失层</h3><p>最后是损失层：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  type: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">  bottom: <span class="string">"ip2"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>softmax_loss层同时实现softmax和多项对数损失（这可以节省时间并提高数值稳定性）。输入为预测的输出和label，并且没有输出（向后的输出）。它计算损失函数，并且反向传播相对于ip2的梯度。</p>
<h3 id="Accuracy准确率层"><a href="#Accuracy准确率层" class="headerlink" title="Accuracy准确率层"></a>Accuracy准确率层</h3><p>这一层是用于在测试中返回准确率使用的：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"accuracy"</span></span><br><span class="line">  type: <span class="string">"Accuracy"</span></span><br><span class="line">  bottom: <span class="string">"ip2"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"accuracy"</span></span><br><span class="line">  include &#123;</span><br><span class="line">    phase: TEST</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>与loss相似，但要注明<code>phase: TEST</code>。</p>
<hr>
<h2 id="定义MNIST-求解器"><a href="#定义MNIST-求解器" class="headerlink" title="定义MNIST 求解器"></a>定义MNIST 求解器</h2><p>求解器文件路径为：</p>
<pre><code>$CAFFE_ROOT/examples/mnist/lenet_solver.prototxt:
</code></pre><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># The train/test net protocol buffer definition</span><br><span class="line">net: <span class="string">"examples/mnist/lenet_train_test.prototxt"</span></span><br><span class="line"># test_iter specifies how many forward passes the test should carry out.</span><br><span class="line"># In the <span class="keyword">case</span> of MNIST, we have test batch size <span class="number">100</span> <span class="keyword">and</span> <span class="number">100</span> test iterations,</span><br><span class="line"><span class="meta"># covering the full 10,000 testing images.</span></span><br><span class="line">test_iter: <span class="number">100</span></span><br><span class="line"># Carry out testing every <span class="number">500</span> training iterations.</span><br><span class="line">test_interval: <span class="number">500</span></span><br><span class="line"># The base learning rate, momentum <span class="keyword">and</span> the weight decay of the network.</span><br><span class="line">base_lr: <span class="number">0.01</span></span><br><span class="line">momentum: <span class="number">0.9</span></span><br><span class="line">weight_decay: <span class="number">0.0005</span></span><br><span class="line"># The learning rate policy</span><br><span class="line">lr_policy: <span class="string">"inv"</span></span><br><span class="line">gamma: <span class="number">0.0001</span></span><br><span class="line">power: <span class="number">0.75</span></span><br><span class="line"># Display every <span class="number">100</span> iterations</span><br><span class="line">display: <span class="number">100</span></span><br><span class="line"># The maximum number of iterations</span><br><span class="line">max_iter: <span class="number">10000</span></span><br><span class="line"><span class="meta"># snapshot intermediate results</span></span><br><span class="line">snapshot: <span class="number">5000</span></span><br><span class="line">snapshot_prefix: <span class="string">"examples/mnist/lenet"</span></span><br><span class="line"><span class="meta"># solver mode: CPU or GPU</span></span><br><span class="line">solver_mode: GPU</span><br></pre></td></tr></table></figure>
<p>这些参数见上一篇：<a href="http://blog.csdn.net/yan_joy/article/details/53079185" target="_blank" rel="noopener">caffe学习（8）Solver 配置详解</a>。</p>
<hr>
<h2 id="训练并测试模型"><a href="#训练并测试模型" class="headerlink" title="训练并测试模型"></a>训练并测试模型</h2><p>简单的话可以直接运行：</p>
<pre><code>cd $CAFFE_ROOT
./examples/mnist/train_lenet.sh
</code></pre><p>即运行：</p>
<pre><code>./build/tools/caffe train --solver=examples/mnist/lenet_solver.prototxt
</code></pre><p>也就是我们上面的求解器配置文件。<br>首先出现的是我们打开的solver文件，之后打开网络模型：lenet_train_test.prototxt，初始化网络参数。</p>
<pre><code>I1108 16:08:29.103813 46285 layer_factory.hpp:77] Creating layer mnist
I1108 16:08:29.104310 46285 net.cpp:100] Creating Layer mnist
I1108 16:08:29.104336 46285 net.cpp:408] mnist -&gt; data
I1108 16:08:29.104374 46285 net.cpp:408] mnist -&gt; label
I1108 16:08:29.107558 46328 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
</code></pre><p>不过仔细看的话会发现初始化了两遍网络，其实是因为我们同时在训练和测试，这两个网络的区别就是测试有”accuracy”层，训练没有。这些信息告诉了层之间的连接、输入、输出关系。结束后正式开始训练：</p>
<pre><code>I1108 16:08:29.156116 46285 net.cpp:283] Network initialization done.
I1108 16:08:29.156206 46285 solver.cpp:60] Solver scaffolding done.
I1108 16:08:29.156466 46285 caffe.cpp:251] Starting Optimization
I1108 16:08:29.156500 46285 solver.cpp:279] Solving LeNet
I1108 16:08:29.156512 46285 solver.cpp:280] Learning Rate Policy: inv
I1108 16:08:29.158172 46285 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 16:08:31.021287 46285 solver.cpp:404]     Test net output #0: accuracy = 0.0933
I1108 16:08:31.021385 46285 solver.cpp:404]     Test net output #1: loss = 2.36349 (* 1 = 2.36349 loss)
</code></pre><p>可以看到，初始化参数后测试模型，准确率有9.33%，比10%还低一些。基于参数设置，我们每迭代100次输出loss 信息，每迭代500次测试模型，输出accuracy 信息：</p>
<pre><code>I1108 16:08:46.974346 46285 solver.cpp:337] Iteration 500, Testing net (#0)
I1108 16:08:48.808943 46285 solver.cpp:404]     Test net output #0: accuracy = 0.9767
I1108 16:08:48.809048 46285 solver.cpp:404]     Test net output #1: loss = 0.068445 (* 1 = 0.068445 loss)
I1108 16:08:48.823623 46285 solver.cpp:228] Iteration 500, loss = 0.0609579
I1108 16:08:48.823714 46285 solver.cpp:244]     Train net output #0: loss = 0.0609579 (* 1 = 0.0609579 loss)
I1108 16:08:48.823740 46285 sgd_solver.cpp:106] Iteration 500, lr = 0.0192814
</code></pre><p>可以发现输出500次后准确率已经达到了97.67%。<br>达到训练次数后（这里减少了训练次数），得到最终结果：</p>
<pre><code>I1108 16:09:04.727638 46285 solver.cpp:454] Snapshotting to binary proto file examples/mnist/lenet_iter_1000.caffemodel
I1108 16:09:04.754024 46285 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_1000.solverstate
I1108 16:09:04.770093 46285 solver.cpp:317] Iteration 1000, loss = 0.0819535
I1108 16:09:04.770177 46285 solver.cpp:337] Iteration 1000, Testing net (#0)
I1108 16:09:06.607952 46285 solver.cpp:404]     Test net output #0: accuracy = 0.9844
I1108 16:09:06.608042 46285 solver.cpp:404]     Test net output #1: loss = 0.0491373 (* 1 = 0.0491373 loss)
I1108 16:09:06.608055 46285 solver.cpp:322] Optimization Done.
I1108 16:09:06.608064 46285 caffe.cpp:254] Optimization Done.
</code></pre><p>得到了两个文件：lenet_iter_1000.caffemodel和lenet_iter_1000.solverstate。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/08/caffe学习（8）Solver 配置详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/08/caffe学习（8）Solver 配置详解/" class="post-title-link" itemprop="url">caffe学习（8）Solver 配置详解</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-08 09:45:00" itemprop="dateCreated datePublished" datetime="2016-11-08T09:45:00+08:00">2016-11-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:29:24" itemprop="dateModified" datetime="2016-11-18T15:29:24+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/08/caffe学习（8）Solver 配置详解/" class="leancloud_visitors" data-flag-title="caffe学习（8）Solver 配置详解">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Solver是求解学习模型的核心配置文件，网络确定后，solver就决定了学习的效果。本文结合caffe.proto和网上资料，对solver配置进行学习。</p>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/tutorial/solver.html" target="_blank" rel="noopener">Solver</a><br><a href="http://www.cnblogs.com/denny402/p/5074049.html" target="_blank" rel="noopener">Caffe学习系列(7)：solver及其配置，denny402</a></p>
</blockquote>
<hr>
<h2 id="Solver在caffe中的定义"><a href="#Solver在caffe中的定义" class="headerlink" title="Solver在caffe中的定义"></a>Solver在caffe中的定义</h2><p>通常的solver文件与net文件相互关联，同样的net我们往往使用不同的solver尝试得到最好的效果，其运行代码为：</p>
<pre><code>caffe train --solver=*_slover.prototxt
</code></pre><p>关于solver的一切，都在caffe.proto文件中message SolverParameter 这一部分。</p>
<p><strong>网络文件源</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Proto filename for the train net, possibly combined with one or more</span></span><br><span class="line"><span class="comment">// test nets.</span></span><br><span class="line">optional <span class="built_in">string</span> net = <span class="number">24</span>;</span><br><span class="line"><span class="comment">// Inline train net param, possibly combined with one or more test nets.</span></span><br><span class="line">optional NetParameter net_param = <span class="number">25</span>;</span><br><span class="line"></span><br><span class="line">optional <span class="built_in">string</span> train_net = <span class="number">1</span>; <span class="comment">// Proto filename for the train net.</span></span><br><span class="line">repeated <span class="built_in">string</span> test_net = <span class="number">2</span>; <span class="comment">// Proto filenames for the test nets.</span></span><br><span class="line">optional NetParameter train_net_param = <span class="number">21</span>; <span class="comment">// Inline train net params.</span></span><br><span class="line">repeated NetParameter test_net_param = <span class="number">22</span>; <span class="comment">// Inline test net params.</span></span><br></pre></td></tr></table></figure></p>
<p>这是最开始的部分，需要说明net文件的位置。在这四个train_net_param, train_net, net_param, net字段中至少需要出现一个，当出现多个时，就会按着(1) test_net_param, (2) test_net, (3) net_param/net 的顺序依次求解。必须为每个test_net指定一个test_iter。还可以为每个test_net指定test_level和/或test_stage。注意的是：文件的路径要从caffe的根目录开始，其它的所有配置都是这样。<br>可以看到这几行的标签序号并不是顺序的，也说明caffe在不断地修改，下一个可用的序号是41。</p>
<p><strong>网络状态</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The states for the train/test nets. Must be unspecified or</span></span><br><span class="line"><span class="comment">// specified once per net.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// By default, all states will have solver = true;</span></span><br><span class="line"><span class="comment">// train_state will have phase = TRAIN,</span></span><br><span class="line"><span class="comment">// and all test_state's will have phase = TEST.</span></span><br><span class="line"><span class="comment">// Other defaults are set according to the NetState defaults.</span></span><br><span class="line">optional NetState train_state = <span class="number">26</span>;</span><br><span class="line">repeated NetState test_state = <span class="number">27</span>;</span><br></pre></td></tr></table></figure></p>
<p>网络状态必须是未指定的或者只能在一个网络中指定一次。<br>关于NetState，其定义为：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">message NetState &#123;</span><br><span class="line">  optional Phase phase = <span class="number">1</span> [<span class="keyword">default</span> = TEST];</span><br><span class="line">  optional int32 level = <span class="number">2</span> [<span class="keyword">default</span> = <span class="number">0</span>];</span><br><span class="line">  repeated <span class="built_in">string</span> stage = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">enum</span> Phase &#123;</span><br><span class="line">   TRAIN = <span class="number">0</span>;</span><br><span class="line">   TEST = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p><strong>迭代器</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The number of iterations for each test net.</span></span><br><span class="line">repeated int32 test_iter = <span class="number">3</span>;</span><br></pre></td></tr></table></figure></p>
<p>首先是<code>test_iter</code>，这需要与test layer中的batch_size结合起来理解。mnist数据中测试样本总数为10000，一次性执行全部数据效率很低，因此我们将测试数据分成几个批次来执行，每个批次的数量就是batch_size。假设我们设置batch_size为100，则需要迭代100次才能将10000个数据全部执行完。因此test_iter设置为100。执行完一次全部数据，称之为一个epoch。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// The number of iterations between two testing phases.</span></span><br><span class="line">optional int32 test_interval = <span class="number">4</span> [<span class="keyword">default</span> = <span class="number">0</span>];</span><br><span class="line">optional <span class="keyword">bool</span> test_compute_loss = <span class="number">19</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</span><br><span class="line"><span class="comment">// If true, run an initial test pass before the first iteration,</span></span><br><span class="line"><span class="comment">// ensuring memory availability and printing the starting value of the loss.</span></span><br><span class="line">optional <span class="keyword">bool</span> test_initialization = <span class="number">32</span> [<span class="keyword">default</span> = <span class="literal">true</span>];</span><br></pre></td></tr></table></figure></p>
<p><code>test_interval</code>是指测试间隔，每训练test_interval次，进行一次测试。同时<code>test_compute_loss</code>可以选择是否计算loss。<code>test_initialization</code>是指在第一次迭代前，计算初始的loss以确保内存可用。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">optional <span class="keyword">float</span> base_lr = <span class="number">5</span>; <span class="comment">// The base learning rate</span></span><br><span class="line"><span class="comment">// the number of iterations between displaying info. If display = 0, no info</span></span><br><span class="line"><span class="comment">// will be displayed.</span></span><br><span class="line">optional int32 display = <span class="number">6</span>;</span><br><span class="line"><span class="comment">// Display the loss averaged over the last average_loss iterations</span></span><br><span class="line">optional int32 average_loss = <span class="number">33</span> [<span class="keyword">default</span> = <span class="number">1</span>];</span><br><span class="line">optional int32 max_iter = <span class="number">7</span>; <span class="comment">// the maximum number of iterations</span></span><br><span class="line"><span class="comment">// accumulate gradients over `iter_size` x `batch_size` instances</span></span><br><span class="line">optional int32 iter_size = <span class="number">36</span> [<span class="keyword">default</span> = <span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<p><code>base_lr</code>指基础的学习率；<code>display</code>是信息显示间隔，迭代一定次数显示一次信息。<code>average_loss</code>用于显示在上次average_loss迭代中的平均损失。<code>max_iter</code>是最大迭代次数，需要合适设置达到精度、震荡的平衡。<code>iter_size</code>是迭代器大小，梯度的计算是通过<code>iter_size</code> x <code>batch_size</code>决定的。</p>
<p><strong>学习策略</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">optional <span class="built_in">string</span> lr_policy = <span class="number">8</span>;</span><br><span class="line">optional <span class="keyword">float</span> gamma = <span class="number">9</span>; <span class="comment">// The parameter to compute the learning rate.</span></span><br><span class="line">optional <span class="keyword">float</span> power = <span class="number">10</span>; <span class="comment">// The parameter to compute the learning rate.</span></span><br><span class="line">optional <span class="keyword">float</span> momentum = <span class="number">11</span>; <span class="comment">// The momentum value.</span></span><br><span class="line">optional <span class="keyword">float</span> weight_decay = <span class="number">12</span>; <span class="comment">// The weight decay.</span></span><br><span class="line"><span class="comment">// regularization types supported: L1 and L2</span></span><br><span class="line"><span class="comment">// controlled by weight_decay</span></span><br><span class="line">optional <span class="built_in">string</span> regularization_type = <span class="number">29</span> [<span class="keyword">default</span> = <span class="string">"L2"</span>];</span><br><span class="line"><span class="comment">// the stepsize for learning rate policy "step"</span></span><br><span class="line">optional int32 stepsize = <span class="number">13</span>;</span><br><span class="line"><span class="comment">// the stepsize for learning rate policy "multistep"</span></span><br><span class="line">repeated int32 stepvalue = <span class="number">34</span>;</span><br></pre></td></tr></table></figure>
<p>只要是梯度下降法来求解优化，都会有一个学习率，也叫步长。base_lr用于设置基础学习率，在迭代的过程中，可以对基础学习率进行调整。怎么样进行调整，就是调整的策略，由lr_policy来设置。caffe提供了多种policy：</p>
<ul>
<li>fixed: 总是返回base_lr（学习率不变）</li>
<li>step: 返回 base_lr * gamma ^ (floor(iter / step))<br>还需要设置stepsize参数以确定step，iter表示当前迭代次数。</li>
<li>exp: 返回base_lr * gamma ^ iter， iter为当前迭代次数</li>
<li>inv: 如果设置为inv,还需要设置一个power, 返回base_lr <em> (1 + gamma </em> iter) ^ (- power)</li>
<li>multistep: 如果设置为multistep,则还需要设置一个stepvalue。这个参数和step很相似，step是均匀等间隔变化，而multistep则是根据stepvalue值变化。</li>
<li>poly: 学习率进行多项式误差, 返回 base_lr (1 - iter/max_iter) ^ (power)</li>
<li>sigmoid: 学习率进行sigmod衰减，返回 base_lr ( 1/(1 + exp(-gamma * (iter - stepsize))))。</li>
</ul>
<p>multistep示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">base_lr: <span class="number">0.01</span></span><br><span class="line">momentum: <span class="number">0.9</span></span><br><span class="line">weight_decay: <span class="number">0.0005</span></span><br><span class="line"><span class="comment"># The learning rate policy</span></span><br><span class="line">lr_policy: <span class="string">"multistep"</span></span><br><span class="line">gamma: <span class="number">0.9</span></span><br><span class="line">stepvalue: <span class="number">5000</span></span><br><span class="line">stepvalue: <span class="number">7000</span></span><br><span class="line">stepvalue: <span class="number">8000</span></span><br><span class="line">stepvalue: <span class="number">9000</span></span><br><span class="line">stepvalue: <span class="number">9500</span></span><br></pre></td></tr></table></figure></p>
<p>之后有<code>momentum</code>，上次梯度更新的权重；<code>weight_decay</code>权重衰减，防止过拟合；<code>regularization_type</code>正则化方式。</p>
<p><strong>clip_gradients</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optional <span class="keyword">float</span> clip_gradients = <span class="number">35</span> [<span class="keyword">default</span> = <span class="number">-1</span>];</span><br></pre></td></tr></table></figure></p>
<p>参数梯度的实际L2范数较大时，将clip_gradients设置为&gt; = 0，以将参数梯度剪切到该L2范数。具体作用还不是很理解。</p>
<p><strong>snapshot快照</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">optional int32 snapshot = <span class="number">14</span> [<span class="keyword">default</span> = <span class="number">0</span>]; <span class="comment">// The snapshot interval</span></span><br><span class="line">optional <span class="built_in">string</span> snapshot_prefix = <span class="number">15</span>; <span class="comment">// The prefix for the snapshot.</span></span><br><span class="line"><span class="comment">// whether to snapshot diff in the results or not. Snapshotting diff will help</span></span><br><span class="line"><span class="comment">// debugging but the final protocol buffer size will be much larger.</span></span><br><span class="line">optional <span class="keyword">bool</span> snapshot_diff = <span class="number">16</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</span><br><span class="line"><span class="keyword">enum</span> SnapshotFormat &#123;</span><br><span class="line">  HDF5 = <span class="number">0</span>;</span><br><span class="line">  BINARYPROTO = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">optional SnapshotFormat snapshot_format = <span class="number">37</span> [<span class="keyword">default</span> = BINARYPROTO];</span><br></pre></td></tr></table></figure></p>
<p>快照可以将训练出来的model和solver状态进行保存，<code>snapshot</code>用于设置训练多少次后进行保存，默认为0，不保存。<code>snapshot_prefix</code>设置保存路径。还可以设置<code>snapshot_diff</code>，是否保存梯度值，保存有利于调试，但需要较大空间存储，默认为false，不保存。也可以设置<code>snapshot_format</code>，保存的类型。有两种选择：HDF5 和BINARYPROTO ，默认为BINARYPROTO。</p>
<p><strong>运行模式</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">enum</span> SolverMode &#123;</span><br><span class="line">  CPU = <span class="number">0</span>;</span><br><span class="line">  GPU = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">optional SolverMode solver_mode = <span class="number">17</span> [<span class="keyword">default</span> = GPU];</span><br><span class="line"><span class="comment">// the device_id will that be used in GPU mode. Use device_id = 0 in default.</span></span><br><span class="line">optional int32 device_id = <span class="number">18</span> [<span class="keyword">default</span> = <span class="number">0</span>];</span><br><span class="line"><span class="comment">// If non-negative, the seed with which the Solver will initialize the Caffe</span></span><br><span class="line"><span class="comment">// random number generator -- useful for reproducible results. Otherwise,</span></span><br><span class="line"><span class="comment">// (and by default) initialize using a seed derived from the system clock.</span></span><br><span class="line">optional int64 random_seed = <span class="number">20</span> [<span class="keyword">default</span> = <span class="number">-1</span>];</span><br></pre></td></tr></table></figure>
<p>设置CPU或GPU模式，在GPU下还可以指定使用哪一块GPU运行。<code>random_seed</code>用于初始生成随机数种子。</p>
<p><strong>Solver类型</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// type of the solver</span></span><br><span class="line">optional <span class="built_in">string</span> type = <span class="number">40</span> [<span class="keyword">default</span> = <span class="string">"SGD"</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// numerical stability for RMSProp, AdaGrad and AdaDelta and Adam</span></span><br><span class="line">optional <span class="keyword">float</span> delta = <span class="number">31</span> [<span class="keyword">default</span> = <span class="number">1e-8</span>];</span><br><span class="line"><span class="comment">// parameters for the Adam solver</span></span><br><span class="line">optional <span class="keyword">float</span> momentum2 = <span class="number">39</span> [<span class="keyword">default</span> = <span class="number">0.999</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// RMSProp decay value</span></span><br><span class="line"><span class="comment">// MeanSquare(t) = rms_decay*MeanSquare(t-1) + (1-rms_decay)*SquareGradient(t)</span></span><br><span class="line">optional <span class="keyword">float</span> rms_decay = <span class="number">38</span>;</span><br></pre></td></tr></table></figure>
<p><code>type</code>是solver的类型，目前有SGD、NESTEROV、ADAGRAD、RMSPROP、ADADELTA、ADAM = 5这六类。之后的一些是这些类型的特有参数，根据需要设置。</p>
<p><strong>杂项</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// If true, print information about the state of the net that may help with</span></span><br><span class="line"><span class="comment">// debugging learning problems.</span></span><br><span class="line">optional <span class="keyword">bool</span> debug_info = <span class="number">23</span> [<span class="keyword">default</span> = <span class="literal">false</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// If false, don't save a snapshot after training finishes.</span></span><br><span class="line">optional <span class="keyword">bool</span> snapshot_after_train = <span class="number">28</span> [<span class="keyword">default</span> = <span class="literal">true</span>];</span><br></pre></td></tr></table></figure>
<p><code>debug_info</code>用于输出调试信息。<code>snapshot_after_train</code>用于训练后是否输出快照。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/07/Google Protocol Buffer 学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/07/Google Protocol Buffer 学习/" class="post-title-link" itemprop="url">Google Protocol Buffer 学习</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-07 20:27:00" itemprop="dateCreated datePublished" datetime="2016-11-07T20:27:00+08:00">2016-11-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:50:04" itemprop="dateModified" datetime="2016-11-18T15:50:04+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/学习/" itemprop="url" rel="index"><span itemprop="name">学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/07/Google Protocol Buffer 学习/" class="leancloud_visitors" data-flag-title="Google Protocol Buffer 学习">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Caffe上有很多使用了Google Protocol Buffer的东西，从网上来看，这“是一种轻便高效的结构化数据存储格式，可以用于结构化数据串行化，很适合做数据存储或 RPC 数据交换格式。它可用于通讯协议、数据存储等领域的语言无关、平台无关、可扩展的序列化结构数据格式”。作为caffe模型定义的数据格式，看懂caffe.proto对caffe的理解会有很大帮助。</p>
<blockquote>
<p><a href="https://developers.google.com/protocol-buffers/docs/overview" target="_blank" rel="noopener">Google Protobuf</a><br><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-gpb/#ibm-pcon" target="_blank" rel="noopener">Google Protocol Buffer 的使用和原理，刘 明</a></p>
</blockquote>
<hr>
<h3 id="小例子"><a href="#小例子" class="headerlink" title="小例子"></a>小例子</h3><p>我们首先要在.proto文件中定义协议缓冲区消息类型（protocol buffer message types），来指定要序列化的信息的结构。下面是官网的一个小例子，定义了一个人的信息：</p>
<pre><code>message Person {
  required string name = 1;
  required int32 id = 2;
  optional string email = 3;

  enum PhoneType {
    MOBILE = 0;
    HOME = 1;
    WORK = 2;
  }

  message PhoneNumber {
    required string number = 1;
    optional PhoneType type = 2 [default = HOME];
  }

  repeated PhoneNumber phone = 4;
}
</code></pre><p>在 protobuf 的术语中，结构化数据被称为 Message，message中有不同成员。proto 文件非常类似 java 或者 C 语言的数据定义，<code>string</code>、<code>int32</code>这种类型我们已经见得多了。支持类型包括数字（整数或浮点）, 布尔值,，字符串，原始字节（raw bytes），或者是其他的message类型 (如上例) 。除了这些类型，其前后多了一些“修饰”。类型前是field rules，有可选（<code>optional</code>）、必填（<code>required</code>）、重复（<code>repeated</code>）三种。后面是message中的ID，同一message下ID随成员递增。</p>
<hr>
<h3 id="定义一个消息类型"><a href="#定义一个消息类型" class="headerlink" title="定义一个消息类型"></a>定义一个消息类型</h3><p>上面的例子其实已经定义了一个较为复杂的message。而对于一个完整的proto文件，在文件前还应该加上</p>
<pre><code>syntax = &quot;proto2&quot;;//说明语法是2还是3
package caffe;    //包名，通常与文件名相同
</code></pre><p>首先是field rules，</p>
<p><strong>Specifying Field Rules</strong></p>
<ul>
<li>required：本字段一个massage必须只有一个。</li>
<li>optional：本字段可以有0个或1个。</li>
<li>repeated：本字段可以重复任何次，并保留顺序。</li>
</ul>
<p>其中必填字段在使用中需要小心，特别是从必填改到可选时，读取时可能认为这个字段是不完整的。</p>
<p><strong>Assigning Tags</strong></p>
<p>定义消息中的每一个字段都有唯一的编号标签ID，用于消息二进制标识，并且在使用后不应该变。由于编码的原因，值在1到15范围的编号需要一个字节编码，包括标识号与字段类型。在16到2047范围的标签用两个字节。这对于数据储存大小有很大的联系，因此频繁出现的标签成员应该使编号尽可能小。其原因在于一种Varint的编码。其标签范围是1到$2^{29}-1$（536,870,911），同时除了中间的19000到19999，这是为协议缓冲区（Protocol Buffers）实现保留的。</p>
<p><strong>Reserved Fields</strong></p>
<p>保留字段，是对于被删除或者注释的字段进行保留， 如果以后加载相同.proto的旧版本，这可能会导致严重的问题。确保不会发生这种情况的一种方法是指定保留已删除字段的字段标签。协议缓冲区编译器将报告任何未来的用户是否尝试使用这些字段标识符。</p>
<pre><code>message Foo {
  reserved 2, 15, 9 to 11;
  reserved &quot;foo&quot;, &quot;bar&quot;;
}
</code></pre><p>不能在同一保留语句中混合字段名和标识号。</p>
<hr>
<h3 id="编译-proto文件"><a href="#编译-proto文件" class="headerlink" title="编译.proto文件"></a>编译.proto文件</h3><p>使用写好的proto就可以用编译器将文件编译为目标语言了。在<a href="https://github.com/google/protobuf/releases/tag/v3.0.0" target="_blank" rel="noopener">protobuf V3.0</a>网站上可以下载 Protobuf V3.0的源代码，<a href="https://github.com/google/protobuf/releases?after=v3.0.0-alpha-3" target="_blank" rel="noopener">V2.6版本</a>在网页上比较靠后，更新到2.6.1。然后解压编译安装便可以使用它了。从caffe文件上看用的还是2的语法。2和3的区别可以从网上搜到，如<a href="http://www.tuicool.com/articles/YNni6rv" target="_blank" rel="noopener">Google Protobuf 3版本介绍</a>。其实变化也是不少，比如只保留repeated标记数组类型，optional和required都被去掉了；字段default标记不能使用了。</p>
<p>坑爹的是服务器没有权限装不了，于是只好在自家电脑上用windows版的。主要是用于验证，</p>
<pre><code> package lm; 
 message helloworld 
 { 
    required int32     id = 1;   // ID 
    required string    str = 2;  // str 
    optional int32     opt = 3;  //optional field 
 }
</code></pre><p>用命令行执行</p>
<pre><code>protoc -I=. --cpp_out=. lm.helloworld.proto
</code></pre><p>得到了两个文件：lm.helloworld.pb.h ， 定义了 C++ 类的头文件<br>lm.helloworld.pb.cc ， C++ 类的实现文件。有了这两个文件，之后我们想读写都可以用类操作实现了。</p>
<hr>
<h3 id="读写数据"><a href="#读写数据" class="headerlink" title="读写数据"></a>读写数据</h3><p><strong>数据写到磁盘代码</strong><br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"lm.helloworld.pb.h"</span></span></span><br><span class="line">…</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span> </span></span><br><span class="line"><span class="function"> </span>&#123; </span><br><span class="line">  </span><br><span class="line">  lm::helloworld msg1; </span><br><span class="line">  msg1.set_id(<span class="number">101</span>); </span><br><span class="line">  msg1.set_str(“hello”); </span><br><span class="line">    </span><br><span class="line">  <span class="comment">// Write the new address book back to disk. </span></span><br><span class="line">  <span class="function">fstream <span class="title">output</span><span class="params">(<span class="string">"./log"</span>, ios::out | ios::trunc | ios::binary)</span></span>; </span><br><span class="line">        </span><br><span class="line">  <span class="keyword">if</span> (!msg1.SerializeToOstream(&amp;output)) &#123; </span><br><span class="line">      <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Failed to write msg."</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">      <span class="keyword">return</span> <span class="number">-1</span>; </span><br><span class="line">  &#125;         </span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>; </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></p>
<p>在代码中，其实重要的只是前三行，定义了helloworld类的对象，设置id的值，设置str的值。最后用SerializeToOstream输出到文件流。</p>
<p><strong>读取数据代码</strong></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"> <span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"lm.helloworld.pb.h"</span> </span></span><br><span class="line">…</span><br><span class="line"> <span class="function"><span class="keyword">void</span> <span class="title">ListMsg</span><span class="params">(<span class="keyword">const</span> lm::helloworld &amp; msg)</span> </span>&#123; </span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; msg.id() &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">  <span class="built_in">cout</span> &lt;&lt; msg.str() &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line"> &#125; </span><br><span class="line"> </span><br><span class="line"> <span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123; </span><br><span class="line"></span><br><span class="line">  lm::helloworld msg1; </span><br><span class="line"> </span><br><span class="line">  &#123; </span><br><span class="line">    <span class="function">fstream <span class="title">input</span><span class="params">(<span class="string">"./log"</span>, ios::in | ios::binary)</span></span>; </span><br><span class="line">    <span class="keyword">if</span> (!msg1.ParseFromIstream(&amp;input)) &#123; </span><br><span class="line">      <span class="built_in">cerr</span> &lt;&lt; <span class="string">"Failed to parse address book."</span> &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">      <span class="keyword">return</span> <span class="number">-1</span>; </span><br><span class="line">    &#125; </span><br><span class="line">  &#125; </span><br><span class="line"> </span><br><span class="line">  ListMsg(msg1); </span><br><span class="line">  … </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
<p>在读取代码中，声明类 helloworld 的对象 msg1，然后利用 ParseFromIstream 从一个 fstream 流中读取信息并反序列化。此后，ListMsg 中采用 get 方法读取消息的内部信息，并进行打印输出操作。</p>
<p>分别运行后得到如下结果：</p>
<pre><code> &gt;writer 
 &gt;reader 
 101 
 Hello
</code></pre><p>验证了程序。</p>
<h3 id="Peotocol-Buffer-编码"><a href="#Peotocol-Buffer-编码" class="headerlink" title="Peotocol Buffer 编码"></a>Peotocol Buffer 编码</h3><p>在标签中说到了Varint，现在再结合编码讲一下，主要是参考了引用2的网页。Varint 中的每个 byte 的最高位 bit 有特殊的含义，如果该位为 1，表示后续的 byte 也是该数字的一部分，如果该位为 0，则结束。其他的 7 个 bit 都用来表示数字。因此小于 128 的数字都可以用一个 byte 表示。大于 128 的数字，比如 300，会用两个字节来表示：1010 1100 0000 0010。下图演示了 Google Protocol Buffer 如何解析两个 bytes。注意到最终计算前将两个 byte 的位置相互交换过一次，这是因为 Google Protocol Buffer 字节序采用 little-endian(小端在前) 的方式。<br><img src="http://img.blog.csdn.net/20161107214645660" alt="Varint编码"><br>消息经过序列化后会成为一个二进制数据流，该流中的数据为一系列的 Key-Value 对。如下图所示：<br><img src="http://img.blog.csdn.net/20161107214939796" alt="message buffer"><br>采用这种 Key-Pair 结构无需使用分隔符来分割不同的 Field。对于可选的 Field，如果消息中不存在该 field，那么在最终的 Message Buffer 中就没有该 field，这些特性都有助于节约消息本身的大小。<br>假如生成如下的消息：</p>
<pre><code>Test1.id = 10; 
Test1.str = “hello”；
</code></pre><p>则最终的 Message Buffer 中有两个 Key-Value 对，一个对应消息中的 id；另一个对应 str。Key 用来标识具体的 field，在解包的时候，Protocol Buffer 根据 Key 就可以知道相应的 Value 应该对应于消息中的哪一个 field。Key 的定义如下：</p>
<pre><code> (field_number &lt;&lt; 3) | wire_type   //&lt;&lt;是左移运算
</code></pre><p>可以看到 Key 由两部分组成。第一部分是 field_number，比如消息 lm.helloworld 中 field id 的 field_number 为 1。第二部分为 wire_type。表示 Value 的传输类型。其中wire_type有如下几种：<br><img src="http://img.blog.csdn.net/20161107215439475" alt="wire type"><br>在我们的例子当中，field id 所采用的数据类型为 int32，因此对应的 wire type 为 0。细心的读者或许会看到在 Type 0 所能表示的数据类型中有 int32 和 sint32 这两个非常类似的数据类型。Google Protocol Buffer 区别它们的主要意图也是为了减少 encoding 后的字节数。<br>在计算机内，一个负数一般会被表示为一个很大的整数，因为计算机定义负数的符号位为数字的最高位。如果采用 Varint 表示一个负数，那么一定需要 5 个 byte。为此 Google Protocol Buffer 定义了 sint32 这种类型，采用 zigzag 编码。Zigzag 编码用无符号数来表示有符号数字，正数和负数交错，这就是 zigzag 这个词的含义了。具体编码如图所示：<br><img src="http://img.blog.csdn.net/20161107215628757" alt="zigza"><br>使用 zigzag 编码，绝对值小的数字，无论正负都可以采用较少的 byte 来表示，充分利用了 Varint 这种技术。<br>其他的数据类型，比如字符串等则采用类似数据库中的 varchar 的表示方法，即用一个 varint 表示长度，然后将其余部分紧跟在这个长度部分之后即可。<br>总之，Protocol Buffer的编码确费尽心机，效果当然也不错，特别是与常用的XML相比，包括解包的速度。</p>
<hr>
<p>了解了一下Google Protocol Buffer，算是一些课外知识了。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/07/caffe学习（7）损失层、通用层/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/07/caffe学习（7）损失层、通用层/" class="post-title-link" itemprop="url">caffe学习（7）损失层、通用层</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-07 16:19:00" itemprop="dateCreated datePublished" datetime="2016-11-07T16:19:00+08:00">2016-11-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:28:38" itemprop="dateModified" datetime="2016-11-18T15:28:38+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/07/caffe学习（7）损失层、通用层/" class="leancloud_visitors" data-flag-title="caffe学习（7）损失层、通用层">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p><a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="noopener">Caffe Layers</a><br><a href="http://www.cnblogs.com/denny402/p/5072746.html" target="_blank" rel="noopener">Caffe学习系列(5)：其它常用层及参数，denny402</a></p>
</blockquote>
<h2 id="损失层Loss-Layers"><a href="#损失层Loss-Layers" class="headerlink" title="损失层Loss Layers"></a>损失层Loss Layers</h2><hr>
<p>损失通过将输出与目标进行比较，并不断优化减小loss。</p>
<p><strong>Softmax（with loss）</strong></p>
<ul>
<li>层类型：SoftmaxWithLoss</li>
<li><p>示例：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">	layer &#123;</span><br><span class="line">	  name: <span class="string">"loss"</span></span><br><span class="line">	  type: <span class="string">"SoftmaxWithLoss"</span></span><br><span class="line">	  bottom: <span class="string">"ip1"</span></span><br><span class="line">	  bottom: <span class="string">"label"</span></span><br><span class="line">	  top: <span class="string">"loss"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>在概念上等同于softmax layer+多项对数损失层（multinomial logistic loss layer），但提供了更稳定的梯度。softmax只是输出每一类的概率，并没有与label做比较。</p>
<p><strong>Sum-of-Squares / Euclidean</strong></p>
<ul>
<li>层类型：EuclideanLoss<br>这是比较传统的求偏差的方法，$\frac 1 {2N} \sum_{i=1}^N | x^1_i - x^2_i |_2^2$，直接计算欧氏距离。</li>
</ul>
<p><strong>Hinge / Margin</strong></p>
<ul>
<li>层类型：HingeLoss</li>
<li><p>参数(HingeLossParameter hinge_loss_param)：</p>
<ul>
<li>可选<ul>
<li>norm [default L1]:应该是正则化方法，目前只有L1、L2。</li>
</ul>
</li>
<li>输入： <ul>
<li>n <em> c </em> h * w Predictions预测值</li>
<li>n <em> 1 </em> 1 * 1 Labels标签</li>
</ul>
</li>
<li>输出：1 <em> 1 </em> 1 * 1 Computed Loss</li>
</ul>
</li>
<li><p>示例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># L1 Norm L1正则</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  type: <span class="string">"HingeLoss"</span></span><br><span class="line">  bottom: <span class="string">"pred"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># L2 Norm L2正则</span></span><br><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"loss"</span></span><br><span class="line">  type: <span class="string">"HingeLoss"</span></span><br><span class="line">  bottom: <span class="string">"pred"</span></span><br><span class="line">  bottom: <span class="string">"label"</span></span><br><span class="line">  top: <span class="string">"loss"</span></span><br><span class="line">  hinge_loss_param &#123;</span><br><span class="line">    norm: L2</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Hinge_loss" target="_blank" rel="noopener">Hinge loss</a>主要用于SVM。</p>
<p><strong>Accuracy</strong></p>
<ul>
<li>层类型：Accuracy</li>
<li><p>示例</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"accuracy"</span></span><br><span class="line">	  type: <span class="string">"Accuracy"</span></span><br><span class="line">	  bottom: <span class="string">"ip2"</span></span><br><span class="line">	  bottom: <span class="string">"label"</span></span><br><span class="line">	  top: <span class="string">"accuracy"</span></span><br><span class="line">	  include &#123;</span><br><span class="line">		    phase: TEST</span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>只有test阶段才有，因此需要加入include参数。它实际上不是损失并且没有后退步骤。</p>
<h2 id="通用层Common-Layers"><a href="#通用层Common-Layers" class="headerlink" title="通用层Common Layers"></a>通用层Common Layers</h2><hr>
<p><strong>Inner Product</strong></p>
<ul>
<li>层类型：InnerProduct</li>
<li>参数 (InnerProductParameter inner_product_param)：<ul>
<li>必须参数<ul>
<li>num_output (c_o):滤波器数量。</li>
</ul>
</li>
<li>推荐参数<ul>
<li>weight_filler [default type: ‘constant’ value: 0]：全职初始化方式、值。还可以选择”xavier”算法来进行初始化，也可以设置为”gaussian”。</li>
</ul>
</li>
<li>可选参数<ul>
<li>bias_filler [default type: ‘constant’ value: 0]：偏置初始化。</li>
<li>bias_term [default true]: 是否启用偏置项。</li>
</ul>
</li>
</ul>
</li>
<li>输入：n <em> c_i </em> h_i * w_i</li>
<li>输出：n <em> c_o </em> 1 * 1</li>
<li><p>示例：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"fc8"</span></span><br><span class="line">	  type: <span class="string">"InnerProduct"</span></span><br><span class="line">	  <span class="comment"># learning rate and decay multipliers for the weights</span></span><br><span class="line">	  param &#123; lr_mult: <span class="number">1</span> decay_mult: <span class="number">1</span> &#125;</span><br><span class="line">  <span class="comment"># learning rate and decay multipliers for the biases</span></span><br><span class="line">	  param &#123; lr_mult: <span class="number">2</span> decay_mult: <span class="number">0</span> &#125;</span><br><span class="line">	  inner_product_param &#123;</span><br><span class="line">		    num_output: <span class="number">1000</span></span><br><span class="line">		    weight_filler &#123;</span><br><span class="line">			      type: <span class="string">"gaussian"</span></span><br><span class="line">			      std: <span class="number">0.01</span></span><br><span class="line">			    &#125;</span><br><span class="line">		    bias_filler &#123;</span><br><span class="line">			      type: <span class="string">"constant"</span></span><br><span class="line">			      value: <span class="number">0</span></span><br><span class="line">			    &#125;</span><br><span class="line">		  &#125;</span><br><span class="line">	  bottom: <span class="string">"fc7"</span></span><br><span class="line">	  top: <span class="string">"fc8"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p><strong>Reshape</strong></p>
<ul>
<li>层类型：Reshape</li>
<li><p>参数 (ReshapeParameter reshape_param)：</p>
<ul>
<li>可选参数：<ul>
<li>shape</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：单独的blob</p>
</li>
<li>输出：变形后的blob</li>
<li><p>示例：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	    name: <span class="string">"reshape"</span></span><br><span class="line">	    type: <span class="string">"Reshape"</span></span><br><span class="line">	    bottom: <span class="string">"input"</span></span><br><span class="line">	    top: <span class="string">"output"</span></span><br><span class="line">	    reshape_param &#123;</span><br><span class="line">		      shape &#123;</span><br><span class="line">		        dim: <span class="number">0</span>  <span class="comment"># copy the dimension from below</span></span><br><span class="line">		        dim: <span class="number">2</span></span><br><span class="line">		        dim: <span class="number">3</span></span><br><span class="line">		        dim: <span class="number">-1</span> <span class="comment"># infer it from the other dimensions</span></span><br><span class="line">		      &#125;</span><br><span class="line">	    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这一操作不改变数据，只改变维度，也没有在过程中拷贝数据。输出的尺寸有shape参数的值规定，正数是对应的维度，除此外还有两个特殊值：</p>
<ul>
<li><strong>0</strong>表示复制底层对应的维度。bottom第一维度值为2，top第一维度也是2。</li>
<li><strong>-1</strong>表示从其他维度推断。为了保证数据总数不变，可以根据其他维数值计算。</li>
</ul>
<p>特别的，当时用参数：<code>reshape_param { shape { dim: 0 dim: -1 } }</code>时，reshape层相当于flatten层，将n <em> c </em> h <em> w的数据变为n </em> (c<em>h</em>w)。</p>
<p><strong>Concatenation</strong></p>
<ul>
<li>层类型： Concat</li>
<li>参数 (ConcatParameter concat_param)：<ul>
<li>可选参数<ul>
<li>axis [default 1]: 0表示沿着数量（n），1表示沿着通道（C）。</li>
</ul>
</li>
<li>输入：n_i <em> c_i </em> h * w 对于每个blob输入，i= 1 到 K。</li>
<li>输出：<ul>
<li>当 axis = 0: (n_1 + n_2 + … + n_K) <em> c_1 </em> h * w, 所有的c_i应该相同。</li>
<li>当 axis = 1: n_1 <em> (c_1 + c_2 + … + c_K) </em> h * w, 所有的n_i 应该相同。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>这个层把多个blob连接为一个blob。</p>
<hr>
<p>层的学习暂时到这里。。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/07/caffe学习（6）激活层/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/07/caffe学习（6）激活层/" class="post-title-link" itemprop="url">caffe学习（6）激活层</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-07 11:22:00" itemprop="dateCreated datePublished" datetime="2016-11-07T11:22:00+08:00">2016-11-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:27:48" itemprop="dateModified" datetime="2016-11-18T15:27:48+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/07/caffe学习（6）激活层/" class="leancloud_visitors" data-flag-title="caffe学习（6）激活层">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>激活（Activation）层又叫神经元（Neuron）层，最主要的是激活函数的设置。</p>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="noopener">Activation / Neuron Layers</a><br><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/5200850.html" target="_blank" rel="noopener">Caffe源码解析6：Neuron_Layer，楼燚航的blog</a></p>
</blockquote>
<hr>
<p>一般来说，这一层是元素级的运算符，从底部blob作为输入并产生一个相同大小的顶部blob：</p>
<ul>
<li>输入：n <em> c </em> h * w</li>
<li>输出：n <em> c </em> h * w</li>
</ul>
<p><strong>ReLU / Rectified-Linear and Leaky-ReLU</strong></p>
<ul>
<li>层类型：ReLU</li>
<li>参数(ReLUParameter relu_param)：<ul>
<li>可选参数<ul>
<li>negative_slope [default 0]: 用来指定负斜率部分的因子$\nu$。完整的函数表达式为：$y = \max(0, x) + \nu \min(0, x)$。反向传播的公式为<script type="math/tex; mode=display">\frac{\partial E}{\partial x} = \left\{
\begin{array}{lr}
\nu \frac{\partial E}{\partial y} & \mathrm{if} \; x \le 0 \\
\frac{\partial E}{\partial y} & \mathrm{if} \; x > 0
\end{array} \right.</script></li>
</ul>
</li>
</ul>
</li>
<li><p>示例（./models/bvlc_reference_caffenet/train_val.prototxt）：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"relu1"</span></span><br><span class="line">  type: <span class="string">"ReLU"</span></span><br><span class="line">  bottom: <span class="string">"conv1"</span></span><br><span class="line">  top: <span class="string">"conv1"</span></span><br><span class="line">&#125;</span><br><span class="line">	```  </span><br><span class="line">	支持<span class="keyword">in</span>-place计算，bottom输入和top输出可以相同避免内存消耗。</span><br><span class="line">	</span><br><span class="line">**Sigmoid**</span><br><span class="line"></span><br><span class="line"> - 层类型：Sigmoid</span><br><span class="line"> - 示例( ./models/bvlc_reference_caffenet/train_val.prototxt)：</span><br><span class="line"></span><br><span class="line">	```python</span><br><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"relu1"</span></span><br><span class="line">	  type: <span class="string">"ReLU"</span></span><br><span class="line">	  bottom: <span class="string">"conv1"</span></span><br><span class="line">	  top: <span class="string">"conv1"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>激活函数表达式为$y = (1 + \exp(-x))^{-1}$，由于收敛速度问题现在用的不多了。</p>
<p><strong>TanH、AbsVal、BNLL</strong></p>
<ul>
<li>层类型：TanH、AbsVal、BNLL</li>
<li><p>示例：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">	layer &#123;</span><br><span class="line">	  name: <span class="string">"layer"</span></span><br><span class="line">	  bottom: <span class="string">"in"</span></span><br><span class="line">	  top: <span class="string">"out"</span></span><br><span class="line">	  type: <span class="string">"TanH"</span><span class="comment">#"AbsVal"、“BNLL”官网上BNLL没有加双引号，应该是有误</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>分别是双曲正切函数、绝对值、binomial normal log likelihood（$f(x)=log(1 + e^x)$）的简称。</p>
<p><strong>Power</strong></p>
<ul>
<li>层类型：Power</li>
<li>参数 (PowerParameter power_param)：<ul>
<li>可选<ul>
<li>power [default 1]</li>
<li>scale [default 1]</li>
<li>shift [default 0]</li>
</ul>
</li>
</ul>
</li>
<li><p>示例：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">	layer &#123;</span><br><span class="line">	  name: <span class="string">"layer"</span></span><br><span class="line">	  bottom: <span class="string">"in"</span></span><br><span class="line">	  top: <span class="string">"out"</span></span><br><span class="line">	  type: <span class="string">"Power"</span></span><br><span class="line">	  power_param &#123;</span><br><span class="line">		    power: <span class="number">2</span></span><br><span class="line">		    scale: <span class="number">1</span></span><br><span class="line">		    shift: <span class="number">0</span></span><br><span class="line">	  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>幂运算函数为$f(x)= (shift + scale * x) ^ p$。</p>
<hr>
<p>Caffe中的激活层还有很多，也有一些是加速的层。比如DropoutLayer现在是非常常用的一种网络层，只用在训练阶段，一般用在网络的全连接层中，可以减少网络的过拟合问题。<br>具体的使用再具体看./src/caffe/layers/下的文件吧。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/06/caffe学习（5）视觉层/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/06/caffe学习（5）视觉层/" class="post-title-link" itemprop="url">caffe学习（5）视觉层</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-06 22:52:00" itemprop="dateCreated datePublished" datetime="2016-11-06T22:52:00+08:00">2016-11-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:34:56" itemprop="dateModified" datetime="2016-11-18T15:34:56+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/06/caffe学习（5）视觉层/" class="leancloud_visitors" data-flag-title="caffe学习（5）视觉层">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>上一篇是数据层，这一篇是视觉层（Vision Layers）。参考官网和网友博客。</p>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/tutorial/layers.html" target="_blank" rel="noopener">Vision Layers</a><br><a href="http://www.cnblogs.com/denny402/p/5071126.html" target="_blank" rel="noopener">Caffe学习系列(3)：视觉层（Vision Layers)及参数，denny402</a><br><a href="http://www.cnblogs.com/louyihang-loves-baiyan/p/5154337.html" target="_blank" rel="noopener">Caffe源码解析5：Conv_Layer，楼燚航的blog</a></p>
</blockquote>
<hr>
<p>视觉层通常将图像作为输入，产生其他图像作为输出。图像输入可以是灰度图（通道C=1），RGB图（通道C=3）。同样图像也具有二维的空间结构，其高度$h&gt;1$宽度$w&gt;1$。大多数视觉层通过对输入区域应用特定操作产生输出的相应区域，这里就有点像传统的数字图像处理的工作了。相比之下其他层常常忽略输入的空间结构，视其为具有$chw$维度的大向量。</p>
<h2 id="卷积层Convolution"><a href="#卷积层Convolution" class="headerlink" title="卷积层Convolution"></a>卷积层Convolution</h2><hr>
<ul>
<li>层类型：Convolution</li>
<li>CPU实现：./src/caffe/layers/convolution_layer.cpp</li>
<li>CUDA GPU实现： ./src/caffe/layers/convolution_layer.cu</li>
<li><p>参数 (ConvolutionParameter convolution_param)：</p>
<ul>
<li>必须参数<ul>
<li>num_output (c_o):卷积核（filter）的个数。</li>
<li>kernel_size (or kernel_h and kernel_w): 卷积核大小，非方阵用_h _w。</li>
</ul>
</li>
<li>推荐参数<ul>
<li>weight_filler [default type: ‘constant’ value: 0]：卷积核的初始化，默认为全0，可以用”xavier”算法来进行初始化，也可以设置为”gaussian”。</li>
</ul>
</li>
<li><p>可选参数</p>
<ul>
<li>bias_term [default true]:是否开启偏置项，默认为true, 开启。</li>
<li>pad (or pad_h and pad_w) [default 0]: 填零操作，默认为0，不填零。是对原图进行填零，使卷积核在图像边缘能够进行卷积操作，运算后和原图的尺寸相同。扩充的时候是左右、上下对称的，比如卷积核的大小为5*5，那么pad设置为2，则四个边缘都扩充2个像素，即宽度和高度都扩充了4个像素。</li>
<li>stride (or stride_h and stride_w) [default 1]: 卷积核的移动步长，默认为1。</li>
<li><p>group (g) [default 1]: 分组，默认为1组。如果大于1，我们限制卷积的连接操作在一个子集内。如果我们根据图像的通道来分组，那么第i个输出分组只能与第i个输入分组进行连接。groups是代表filter 组的个数。引入gruop主要是为了选择性的连接卷基层的输入端和输出端的channels，否则参数会太多。</p>
<blockquote>
<p>It was there to implement the grouped convolution in Alex Krizhevsky’s paper: when group=2, the first half of the filters are only connected to the first half of the input channels, and the second half only connected to the second half.</p>
</blockquote>
<p> 当group=2时，前半部分filter与输入的前半部分通道连接，后半部分filter与后半部分输入通道连接。</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>输入：n <em> c_i </em> h_i * w_i</p>
</li>
<li>输出：n <em> c_o </em> h_o <em> w_o, where h_o = (h_i + 2 </em> pad_h - kernel_h) / stride_h + 1 and w_o likewise。</li>
<li><p>示例 (./models/bvlc_reference_caffenet/train_val.prototxt)</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"conv1"</span></span><br><span class="line">	  type: <span class="string">"Convolution"</span></span><br><span class="line">	  bottom: <span class="string">"data"</span></span><br><span class="line">	  top: <span class="string">"conv1"</span></span><br><span class="line">	  <span class="comment"># learning rate and decay multipliers for the </span></span><br><span class="line">	  <span class="comment"># filters</span></span><br><span class="line">	  param &#123; lr_mult: <span class="number">1</span> decay_mult: <span class="number">1</span> &#125;</span><br><span class="line">	  <span class="comment"># learning rate and decay multipliers for the biases</span></span><br><span class="line">	  param &#123; lr_mult: <span class="number">2</span> decay_mult: <span class="number">0</span> &#125;</span><br><span class="line">	  convolution_param &#123;</span><br><span class="line">		    num_output: <span class="number">96</span>     </span><br><span class="line">		    <span class="comment"># learn 96 filters</span></span><br><span class="line">		    kernel_size: <span class="number">11</span>    </span><br><span class="line">		    <span class="comment"># each filter is 11x11</span></span><br><span class="line">			stride: <span class="number">4</span>      </span><br><span class="line">			<span class="comment"># step 4 pixels between each filter </span></span><br><span class="line">			<span class="comment"># application</span></span><br><span class="line">		    weight_filler &#123;</span><br><span class="line">			    type: <span class="string">"gaussian"</span> </span><br><span class="line">			    <span class="comment"># initialize the filters from a Gaussian</span></span><br><span class="line">			    std: <span class="number">0.01</span>        </span><br><span class="line">			    <span class="comment"># distribution with stdev 0.01 (default </span></span><br><span class="line">			    <span class="comment"># mean: 0)</span></span><br><span class="line">		    &#125;</span><br><span class="line">		    bias_filler &#123;</span><br><span class="line">			    type: <span class="string">"constant"</span> </span><br><span class="line">			    <span class="comment"># initialize the biases to zero (0)</span></span><br><span class="line">			    value: <span class="number">0</span></span><br><span class="line">			&#125;</span><br><span class="line">		 &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>卷积层将输入图像与一组可学习的滤波器进行卷积，每个在输出图像中产生一个特征图。</p>
</li>
</ul>
<h2 id="池化层Pooling"><a href="#池化层Pooling" class="headerlink" title="池化层Pooling"></a>池化层Pooling</h2><hr>
<p>Pooling 层一般在网络中是跟在Conv卷积层之后，做采样操作，其实是为了进一步缩小feature map，同时也能增大神经元的视野。</p>
<ul>
<li>层类型：Pooling</li>
<li>CPU实现：./src/caffe/layers/pooling_layer.cpp</li>
<li>CUDA GPU实现：./src/caffe/layers/pooling_layer.cu</li>
<li>参数(PoolingParameter pooling_param)：<ul>
<li>必须<ul>
<li>kernel_size (or kernel_h and kernel_w)：池化核大小。</li>
</ul>
</li>
<li>可选参数<ul>
<li>pool [default MAX]: 池化方法，默认为MAX，还有 AVE, or STOCHASTIC。</li>
<li>pad (or pad_h and pad_w) [default 0]:填零。</li>
<li>stride (or stride_h and stride_w) [default 1]:步长。</li>
</ul>
</li>
<li>输入：n <em> c </em> h_i * w_i</li>
<li>输出：n <em> c </em> h_o * w_o，h_o and w_o 与卷积层计算方法相同。</li>
</ul>
</li>
<li><p>示例 ( ./models/bvlc_reference_caffenet/train_val.prototxt)</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"pool1"</span></span><br><span class="line">	  type: <span class="string">"Pooling"</span></span><br><span class="line">	  bottom: <span class="string">"conv1"</span></span><br><span class="line">	  top: <span class="string">"pool1"</span></span><br><span class="line">	  pooling_param &#123;</span><br><span class="line">		    pool: MAX</span><br><span class="line">		    kernel_size: <span class="number">3</span> <span class="comment"># pool over a 3x3 region</span></span><br><span class="line">		    stride: <span class="number">2</span>      <span class="comment"># step two pixels (in the </span></span><br><span class="line">						   <span class="comment"># bottom blob) between pooling </span></span><br><span class="line">						   <span class="comment"># regions</span></span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="局部响应归一化层RNL"><a href="#局部响应归一化层RNL" class="headerlink" title="局部响应归一化层RNL"></a>局部响应归一化层RNL</h2><hr>
<p>局部响应归一化层通过对局部输入区域进行归一化来执行一种“横向抑制”。具体作用感觉和特征缩放有点像，使梯度下降在所有方向上具有相同的曲率。而RNL这种方法的计算相比对每个神经元输入归一化要简单。</p>
<ul>
<li>层类型：LRN</li>
<li>CPU实现： ./src/caffe/layers/lrn_layer.cpp</li>
<li>CUDA GPU实现：./src/caffe/layers/lrn_layer.cu</li>
<li>参数 (LRNParameter lrn_param)：<ul>
<li>可选参数<ul>
<li>local_size [default 5]:需要求和的通道数数目（对于跨通道LRN），或者是方形区域求和的变长（对于通道内LRN）。</li>
<li>alpha [default 1]: 比例参数。</li>
<li>beta [default 5]: 指数参数。</li>
<li>norm_region [default ACROSS_CHANNELS]:ACROSS_CHANNELS表示在相邻的通道间求和归一化，但没有空间延伸，即大小为local_size x 1 x 1；WITHIN_CHANNEL表示在一个通道内部特定的区域内进行求和归一化，其大小为：1 x local_size x local_size。每个输入值被除以$(1 + (\alpha/n) \sum_i x_i^2)^\beta$，$n$是每个局部区域的大小。</li>
</ul>
</li>
</ul>
</li>
<li><p>示例：</p>
 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">layers &#123;</span><br><span class="line">	  name: <span class="string">"norm1"</span></span><br><span class="line">	  type: LRN</span><br><span class="line">	  bottom: <span class="string">"pool1"</span></span><br><span class="line">	  top: <span class="string">"norm1"</span></span><br><span class="line">	  lrn_param &#123;</span><br><span class="line">		    local_size: <span class="number">5</span></span><br><span class="line">		    alpha: <span class="number">0.0001</span></span><br><span class="line">		    beta: <span class="number">0.75</span></span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="Im2col"><a href="#Im2col" class="headerlink" title="Im2col"></a>Im2col</h2><hr>
<p>Caffe中卷积操作需要先对数据进行im2col，再进行内积运算，如下图所示：<br><img src="http://images2015.cnblogs.com/blog/140867/201512/140867-20151224103937640-1245969956.jpg" alt="卷积计算"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/06/caffe学习（4）数据层/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/06/caffe学习（4）数据层/" class="post-title-link" itemprop="url">caffe学习（4）数据层</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-06 21:10:00" itemprop="dateCreated datePublished" datetime="2016-11-06T21:10:00+08:00">2016-11-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:34:10" itemprop="dateModified" datetime="2016-11-18T15:34:10+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/06/caffe学习（4）数据层/" class="leancloud_visitors" data-flag-title="caffe学习（4）数据层">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>数据是学习的原料，参考官网和网友的资料，来看一下数据与数据层。</p>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/tutorial/data.html" target="_blank" rel="noopener">Data：Ins and Outs</a><br><a href="http://www.cnblogs.com/denny402/p/5070928.html" target="_blank" rel="noopener">Caffe学习系列(2)：数据层及参数，denny402</a></p>
</blockquote>
<h2 id="数据：输入与输出"><a href="#数据：输入与输出" class="headerlink" title="数据：输入与输出"></a>数据：输入与输出</h2><hr>
<p>在Caffe中，数据是以Blobs流动的（见<a href="http://blog.csdn.net/yan_joy/article/details/53048154" target="_blank" rel="noopener">caffe学习（1）caffe模型三种结构</a>）。数据层的输入输出便需要由其他格式与Blobs进行相互转换。一些常见的变换如平均减法（mean-subtraction）、特征缩放是通过data layer配置完成。新的输入类型需要开发新的数据层，网络的其余部分遵循Caffe层目录的模块化。下段加载了MNIST数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: <span class="string">"mnist"</span></span><br><span class="line">  <span class="comment"># Data layer loads leveldb or lmdb storage DBs for high-throughput.加载leveldb 或 lmdb类型的数据实现高吞吐量</span></span><br><span class="line">  type: <span class="string">"Data"</span></span><br><span class="line">  <span class="comment"># the 1st top is the data itself: the name is only convention</span></span><br><span class="line">  top: <span class="string">"data"</span></span><br><span class="line">  <span class="comment"># the 2nd top is the ground truth: the name is only convention</span></span><br><span class="line">  top: <span class="string">"label"</span></span><br><span class="line">  <span class="comment"># the Data layer configuration</span></span><br><span class="line">  data_param &#123;</span><br><span class="line">    <span class="comment"># path to the DB</span></span><br><span class="line">    source: <span class="string">"examples/mnist/mnist_train_lmdb"</span></span><br><span class="line">    <span class="comment"># type of DB: LEVELDB or LMDB (LMDB supports concurrent reads)</span></span><br><span class="line">    backend: LMDB</span><br><span class="line">    <span class="comment"># batch processing improves efficiency.</span></span><br><span class="line">    batch_size: <span class="number">64</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment"># common data transformations</span></span><br><span class="line">  transform_param &#123;</span><br><span class="line">    <span class="comment"># feature scaling coefficient: this maps the [0, 255] MNIST data to [0, 1]</span></span><br><span class="line">    scale: <span class="number">0.00390625</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>name: 表示该层的名称，可随意取，本层为”mnist”。</li>
<li>type: 层类型，如果是Data，表示数据来源于LevelDB或LMDB。根据数据的来源不同，数据层的类型也不同（后面会详细阐述）。一般在练习的时候，我们都是采用的LevelDB或LMDB数据，因此层类型设置为Data。</li>
<li>top或bottom: 每一层用bottom来输入数据，用top来输出数据。如果只有top没有bottom，则此层只有输出，没有输入。反之亦然。如果有多个 top或多个bottom，表示有多个blobs数据的输入和输出。</li>
<li>data 与 label: 在数据层中，至少有一个命名为data的top。如果有第二个top，一般命名为label。 这种(data,label)配对是分类模型所必需的。本例中第一个top是数据本身，第二个top是label（ground truth）（这些名字只是约定的）。</li>
<li><p>include: 一般训练的时候和测试的时候，模型的层是不一样的。该层（layer）是属于训练阶段的层，还是属于测试阶段的层，需要用include来指定。如果没有include参数，则表示该层既在训练模型中，又在测试模型中。（上例中没有出现）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">include &#123;</span><br><span class="line">    phase: TRAIN <span class="comment">#仅在训练中出现</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Transformations: 数据的预处理，可以将数据变换到定义的范围内。如设置scale为0.00390625，实际上就是1/255, 即将输入数据由0-255归一化到0-1之间。除了缩放，还有其他的一些预处理操作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">transform_param &#123;</span><br><span class="line">    scale: <span class="number">0.00390625</span></span><br><span class="line">    mean_file_size: <span class="string">"examples/cifar10/mean.binaryproto"</span></span><br><span class="line">    <span class="comment"># 用一个配置文件来进行均值操作</span></span><br><span class="line">    mirror: <span class="number">1</span>  <span class="comment"># 1表示开启镜像，0表示关闭，也可用ture和false来表示</span></span><br><span class="line">    <span class="comment"># 剪裁一个 227*227的图块，在训练阶段随机剪裁（random cropping），在测试阶段从中间裁剪</span></span><br><span class="line">    crop_size: <span class="number">227</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>prefetching：预取，对于吞吐量数据层获取下一批数据，并在Net计算当前批处理时在后台准备。</p>
</li>
</ul>
<p>具体的还需要分析<a href="http://caffe.berkeleyvision.org/tutorial/layers.html#data-layers" target="_blank" rel="noopener">data_param</a>，data_param部分，就是根据数据的来源不同，来进行不同的设置。</p>
<ol>
<li>数据来自于数据库（如LevelDB和LMDB）<br>层类型（layer type）:Data</li>
</ol>
<ul>
<li>必须设置的参数：<br>source: 包含数据库的目录名称<br>batch_size: 每次处理的数据个数，如64</li>
<li><p>可选的参数：<br>rand_skip: 在开始的时候，跳过一定数量的数据输入，通常对异步的SGD很有用（useful for asynchronous sgd）。<br>backend: 选择是采用LevelDB还是LMDB, 默认是LevelDB.<br>示例：</p>
<pre><code> <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">data_param</span> &#123;</span><br><span class="line">	<span class="attribute">source</span>: <span class="string">"examples/mnist/mnist_train_lmdb"</span></span><br><span class="line">    batch_size: <span class="number">64</span>&#125;</span><br></pre></td></tr></table></figure>
</code></pre></li>
</ul>
<ol>
<li>数据来自于内存<br>层类型：MemoryData</li>
</ol>
<ul>
<li>必须设置的参数：<br>batch_size：每一次处理的数据个数，比如2<br>channels：通道数<br>height：高度<br>width: 宽度<br>即指定要从内存中读取的输入块的大小。存储器数据层直接从存储器读取数据，而不复制它。为了使用它，必须调用<code>MemoryDataLayer :: Reset</code>（C ++）或<code>Net.set_input_arrays</code>（Python），以便指定一个连续数据源（<a href="http://blog.csdn.net/yan_joy/article/details/53048154" target="_blank" rel="noopener">作为4D行主数组</a>），一次读取一个批处理大小的块。示例： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">  layer &#123;</span><br><span class="line">	  top: <span class="string">"data"</span></span><br><span class="line">	  top: <span class="string">"label"</span></span><br><span class="line">	  name: <span class="string">"memory_data"</span></span><br><span class="line">	  type: <span class="string">"MemoryData"</span></span><br><span class="line">	  memory_data_param&#123;</span><br><span class="line">		    batch_size: <span class="number">2</span></span><br><span class="line">		    height: <span class="number">100</span></span><br><span class="line">		    width: <span class="number">100</span></span><br><span class="line">		    channels: <span class="number">1</span></span><br><span class="line">		  &#125;</span><br><span class="line">	  transform_param &#123;</span><br><span class="line">		    scale: <span class="number">0.0078125</span></span><br><span class="line">		    mean_file: <span class="string">"mean.proto"</span></span><br><span class="line">		    mirror: false</span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li>数据来自于HDF5(Input)<br>层类型：HDF5Data</li>
</ol>
<ul>
<li>必须设置的参数：<br>source: 读取的文件名称<br>batch_size: 每一次处理的数据个数<br>示例： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"data"</span></span><br><span class="line">	  type: <span class="string">"HDF5Data"</span></span><br><span class="line">	  top: <span class="string">"data"</span></span><br><span class="line">	  top: <span class="string">"label"</span></span><br><span class="line">	  hdf5_data_param &#123;</span><br><span class="line">		    source: <span class="string">"examples/hdf5_classification/data/train.txt"</span></span><br><span class="line">		    batch_size: <span class="number">10</span></span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ol>
<li>数据输出到HDF5(Output)<br>层类型：HDF5Data</li>
</ol>
<ul>
<li>必须设置的参数：<br>file_name: 输出到的文件名称<br>HDF5输出层执行与本节中其他层相反的功能：它将其输出blob写入磁盘。</li>
</ul>
<ol>
<li><p>数据来自于图片<br>层类型：ImageData</p>
<ul>
<li>必须设置的参数：<br>source: 一个文本文件的名字，每一行给定一个图片文件的名称和标签（label）<br>batch_size: 每一次处理的数据个数，即图片数</li>
<li>可选参数：<br>rand_skip: 在开始的时候，跳过一定的数据输入。通常对异步的SGD很有用。<br>shuffle: 随机打乱顺序，默认值为false<br>new_height,new_width: 如果设置，则将图片进行resize<br>示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	  name: <span class="string">"data"</span></span><br><span class="line">	  type: <span class="string">"ImageData"</span></span><br><span class="line">	  top: <span class="string">"data"</span></span><br><span class="line">	  top: <span class="string">"label"</span></span><br><span class="line">	  transform_param &#123;</span><br><span class="line">		    mirror: false</span><br><span class="line">		    crop_size: <span class="number">227</span></span><br><span class="line">		    mean_file: <span class="string">"data/ilsvrc12/imagenet_mean.binaryproto"</span></span><br><span class="line">		  &#125;</span><br><span class="line">	  image_data_param &#123;</span><br><span class="line">		    source: <span class="string">"examples/_temp/file_list.txt"</span></span><br><span class="line">		    batch_size: <span class="number">50</span></span><br><span class="line">		    new_height: <span class="number">256</span></span><br><span class="line">		    new_width: <span class="number">256</span></span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>数据来源于Windows<br>层类型：WindowData</p>
<ul>
<li>必须设置的参数：<br>source: 一个文本文件的名字<br>batch_size: 每一次处理的数据个数，即图片数<br>示例：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">  layer &#123;</span><br><span class="line">	  name: <span class="string">"data"</span></span><br><span class="line">	  type: <span class="string">"WindowData"</span></span><br><span class="line">	  top: <span class="string">"data"</span></span><br><span class="line">	  top: <span class="string">"label"</span></span><br><span class="line">	  include &#123;</span><br><span class="line">		    phase: TRAIN</span><br><span class="line">		  &#125;</span><br><span class="line">	  transform_param &#123;</span><br><span class="line">		    mirror: true</span><br><span class="line">		    crop_size: <span class="number">227</span></span><br><span class="line">		    mean_file: <span class="string">"data/ilsvrc12/imagenet_mean.binaryproto"</span></span><br><span class="line">		  &#125;</span><br><span class="line">  window_data_param &#123;</span><br><span class="line">		    source: <span class="string">"examples/finetune_pascal_detection/window_file_2007_trainval.txt"</span></span><br><span class="line">		    batch_size: <span class="number">128</span></span><br><span class="line">		    fg_threshold: <span class="number">0.5</span></span><br><span class="line">		    bg_threshold: <span class="number">0.5</span></span><br><span class="line">		    fg_fraction: <span class="number">0.25</span></span><br><span class="line">		    context_pad: <span class="number">16</span></span><br><span class="line">		    crop_mode: <span class="string">"warp"</span></span><br><span class="line">		  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Dummy<br>DummyData用于调试，详见DummyDataParameter。</p>
</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://joyfyan.github.io/2016/11/06/caffe学习（3）接口/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yan Joy">
      <meta itemprop="description" content="Default~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小一一的小站">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2016/11/06/caffe学习（3）接口/" class="post-title-link" itemprop="url">caffe学习（3）接口</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2016-11-06 14:23:00" itemprop="dateCreated datePublished" datetime="2016-11-06T14:23:00+08:00">2016-11-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2016-11-18 15:33:34" itemprop="dateModified" datetime="2016-11-18T15:33:34+08:00">2016-11-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ML/" itemprop="url" rel="index"><span itemprop="name">ML</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          
            <span id="/2016/11/06/caffe学习（3）接口/" class="leancloud_visitors" data-flag-title="caffe学习（3）接口">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="接口Interfaces"><a href="#接口Interfaces" class="headerlink" title="接口Interfaces"></a>接口Interfaces</h1><hr>
<blockquote>
<p><a href="http://caffe.berkeleyvision.org/tutorial/interfaces.html" target="_blank" rel="noopener">Interfaces</a> </p>
</blockquote>
<p>Caffe提供丰富的接口，比如命令行，python，matlab。先说一下命令行</p>
<h2 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h2><hr>
<blockquote>
<p><a href="http://www.cnblogs.com/CQUTWH/p/5932570.html" target="_blank" rel="noopener">caffe命令及其参数解析，Single、Dog</a></p>
</blockquote>
<p>Caffe的程序位于<code>caffe / build / tools</code>，运行时可以在根目录执行<code>./build/tools/caffe &lt;command&gt;&lt;args&gt;</code>。<br>其中<code>&lt;command&gt;</code>有四种：</p>
<ul>
<li>train：训练或finetune模型(model)</li>
<li>test：测试模型</li>
<li>device_query：显示gpu信息</li>
<li>time：显示程序执行时间</li>
</ul>
<p>其中的<code>&lt;args&gt;</code>参数有：</p>
<ul>
<li>-solver</li>
<li>-gpu</li>
<li>-snapshot</li>
<li>-weights</li>
<li>-model</li>
<li>-sighup_effect</li>
<li>-sigint_effect</li>
</ul>
<h3 id="训练train"><a href="#训练train" class="headerlink" title="训练train"></a>训练train</h3><p><code>caffe train</code>可以从头开始学习模型、从已保存的快照中恢复学习或添加新数据进行fine-tunes。具体来说，所有训练需要通过<code>-solver solver.prototxt</code>参数进行求解器配置；恢复需要使用<code>-snapshot model_iter_1000.solverstate</code>参数来加载求解程序快照；fine-tunes微调需要模型初始化的<code>-weights model.caffemodel</code>参数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train LeNet 训练LeNet</span></span><br><span class="line">caffe train -solver examples/mnist/lenet_solver.prototxt</span><br><span class="line"><span class="comment"># train on GPU 2 在特定的GPU上</span></span><br><span class="line">caffe train -solver examples/mnist/lenet_solver.prototxt -gpu <span class="number">2</span></span><br><span class="line"><span class="comment"># resume training from the half-way point snapshot 从快照恢复</span></span><br><span class="line">caffe train -solver examples/mnist/lenet_solver.prototxt -snapshot examples/mnist/lenet_iter_5000.solverstate</span><br><span class="line"><span class="comment"># fine-tune CaffeNet model weights for style recognition 完整例子参阅examples/finetuning_on_flickr_style，仅调用可使用：</span></span><br><span class="line">caffe train -solver examples/finetuning_on_flickr_style/solver.prototxt -weights models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel</span><br></pre></td></tr></table></figure>
<p>对于train的参数，功能为：</p>
<ul>
<li><code>-solver</code>：必选，后跟一个protocol buffer类型(.prototxt)的文件，即模型的配置文件。</li>
<li><p><code>-gpu</code>：可选，指定某一块GPU运行，<code>-gpu all</code>是所有运行：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train on GPUs 0 &amp; 1 (doubling the batch size)</span></span><br><span class="line">caffe train -solver examples/mnist/lenet_solver.prototxt -gpu <span class="number">0</span>,<span class="number">1</span></span><br><span class="line"><span class="comment"># train on all GPUs (multiplying batch size by number of devices)</span></span><br><span class="line">caffe train -solver examples/mnist/lenet_solver.prototxt -gpu all</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>-snapshot</code>：可选，从快照中恢复，设置快照可从solver配置中进行，保存为solverstate。</p>
</li>
<li><code>-weights</code>：可选参数。用预先训练好的权重来fine-tuning模型，需要一个caffemodel，不能和-snapshot同时使用。</li>
<li><code>-iterations</code>： 可选参数，迭代次数，默认为50。 如果在配置文件文件中没有设定迭代次数，则默认迭代50次。</li>
<li><code>-model</code>：可选参数，定义在protocol buffer文件中的模型。也可以在solver配置文件中指定。</li>
<li><code>-sighup_effect</code>：可选参数。用来设定当程序发生挂起事件时，执行的操作，可以设置为snapshot, stop或none, 默认为snapshot。</li>
<li><code>-sigint_effect</code>： 可选参数。用来设定当程序发生键盘中止事件时（ctrl+c), 执行的操作，可以设置为snapshot, stop或none, 默认为stop。<h3 id="测试test"><a href="#测试test" class="headerlink" title="测试test"></a>测试test</h3>测试时输出每个batch得分，最后返回平均值。test参数用在测试阶段，用于最终结果的输出，要模型配置文件中我们可以设定需要输入accuracy还是loss. 假设我们要在验证集中验证已经训练好的模型，就可以这样写<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># score the learned LeNet model on the validation set as defined in the</span></span><br><span class="line"><span class="comment"># model architeture lenet_train_test.prototxt</span></span><br><span class="line">caffe test -model examples/mnist/lenet_train_test.prototxt -weights examples/mnist/lenet_iter_10000.caffemodel -gpu <span class="number">0</span> -iterations <span class="number">100</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>意思是利用训练好了的权重（-weight)，输入到测试模型中(-model)，用编号为0的gpu(-gpu)测试100次(-iteration)。</p>
<h3 id="时间time"><a href="#时间time" class="headerlink" title="时间time"></a>时间time</h3><p>time参数用来在屏幕上显示程序运行时间。如：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (These example calls require you complete the LeNet / MNIST example first.)</span></span><br><span class="line"><span class="comment"># time LeNet training on CPU for 10 iterations</span></span><br><span class="line">caffe time -model examples/mnist/lenet_train_test.prototxt -iterations <span class="number">10</span></span><br></pre></td></tr></table></figure></p>
<p>这个例子用来在屏幕上显示lenet模型迭代10次所使用的时间。包括每次迭代的forward和backward所用的时间，也包括每层forward和backward所用的平均时间。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/10/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/12/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Yan Joy">
            
              <p class="site-author-name" itemprop="name">Yan Joy</p>
              <div class="site-description motion-element" itemprop="description">Default~</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">104</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">60</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="http://weibo.com/yanjoy" title="Weibo &rarr; http://weibo.com/yanjoy" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>Weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="http://blog.csdn.net/yan_joy" title="CSDN &rarr; http://blog.csdn.net/yan_joy" rel="noopener" target="_blank"><i class="fa fa-fw fa-linux"></i>CSDN</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.xieqiang.site/" title="http://www.xieqiang.site/" rel="noopener" target="_blank">Johnny</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://judasdie.github.io/" title="https://judasdie.github.io/" rel="noopener" target="_blank">ZP Zhang</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://pages.coding.me/" title="https://pages.coding.me/" rel="noopener" target="_blank">Hosted by Coding Pages</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2016 – <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Joy</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.1.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
      <div>
        <div class="addthis_inline_share_toolbox">
  <script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5b692d42870e12d9" async="async"></script>
</div>

      </div>
    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>




  

  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  



  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  
  <script>
    
    function showTime(Counter) {
      var entries = [];
      var $visitors = $('.leancloud_visitors');

      $visitors.each(function() {
        entries.push( $(this).attr('id').trim() );
      });

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url: { '$in': entries } }) })
        .done(function({ results }) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if (countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function ({ responseJSON }) {
          console.log('LeanCloud Counter Error: ' + responseJSON.code + ' ' + responseJSON.error);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'CETsF97cGhnDDoeuypbWtlvs-gzGzoHsz')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'CETsF97cGhnDDoeuypbWtlvs-gzGzoHsz',
                'X-LC-Key': 'owWjRRJ2oBD7Dekc8l1RRctK',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            if ($('.post-title-link').length >= 1) {
              showTime(Counter);
            }
          
        });
    });
  </script>



  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
